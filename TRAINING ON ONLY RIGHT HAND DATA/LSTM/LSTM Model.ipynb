{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "589cce50-b090-4a53-9d2d-46883f900823",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69888c4f-f8b6-435f-85de-50c50d6d7038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af2f7c",
   "metadata": {},
   "source": [
    "## Dataset And Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8437d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 - Inputs shape: torch.Size([32, 30, 42]), Labels shape: torch.Size([32])\n",
      "Sample 1: tensor([[ 1.6648e+01,  1.5082e-01, -1.5210e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 1.6678e+01,  4.0196e-01,  3.4363e-01,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 1.6718e+01,  2.4442e-01,  2.5817e-01,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 1.7447e+01, -1.6774e+00,  1.3619e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 1.7479e+01, -1.6774e+00, -9.0036e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 1.7501e+01, -1.6774e+00, -9.0036e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), Label: 3\n",
      "Sample 2: tensor([[23.7017,  0.7714,  0.2793,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [23.7309,  0.6564,  0.4872,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [23.7608,  0.7688,  0.5276,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [24.5020,  0.6502,  0.1624,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [24.5319,  0.9439, -0.1683,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [24.5700,  0.6180, -0.1676,  ...,  0.0000,  0.0000,  0.0000]]), Label: 4\n",
      "Sample 3: tensor([[ 4.0056e+01,  2.4902e-01,  2.4015e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 4.0074e+01,  3.4053e-01, -5.9037e-03,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 4.0100e+01,  3.4053e-01, -5.9037e-03,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 4.0742e+01, -1.5980e+00, -6.9446e-01,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 4.0774e+01, -8.6544e-01, -4.1592e-01,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 4.0803e+01, -6.2433e-01, -3.1778e-01,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), Label: 3\n",
      "Sample 4: tensor([[ 3.3415e+01,  1.2407e-01, -9.8529e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 3.3445e+01,  7.3930e-02,  2.5130e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 3.3471e+01,  7.3930e-02,  2.5130e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 3.4225e+01, -1.0944e+00, -6.5135e-01,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 3.4276e+01, -6.9286e-01, -4.7538e-01,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 3.4336e+01, -4.2793e-01, -2.9042e-01,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), Label: 5\n",
      "Sample 5: tensor([[ 2.0749e+01,  3.4722e-01, -6.7770e-01,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 2.0767e+01,  3.4722e-01, -6.7770e-01,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 2.0805e+01,  8.0771e-01, -1.1880e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 2.5249e+01,  5.5544e-02,  4.4134e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 2.5268e+01,  4.5640e-03,  8.9533e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 2.5281e+01,  4.5640e-03,  8.9533e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), Label: 3\n"
     ]
    }
   ],
   "source": [
    "class TemporalDataset(Dataset):\n",
    "    def __init__(self, features, labels, sequence_length):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.sequence_length = sequence_length\n",
    "        self.max_length = len(features)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_end = idx + self.sequence_length\n",
    "\n",
    "        # Extract the sequence of features\n",
    "        sequence_features = self.features[idx:idx_end]\n",
    "\n",
    "        # Handle NaN values: replace NaNs with zeros or any other strategy\n",
    "        sequence_features = torch.tensor(sequence_features, dtype=torch.float32)\n",
    "        sequence_features[torch.isnan(sequence_features)] = 0.0\n",
    "\n",
    "        # Extract the label\n",
    "        label = torch.tensor(self.labels[idx_end - 1], dtype=torch.long)\n",
    "\n",
    "        return sequence_features, label\n",
    "\n",
    "\n",
    "# File paths for training, validation, and testing\n",
    "train_file = \"D:/iMobie/Final Project/final test of LEGO CAR assambly/DATA OF ONLY RIGHT HAND DATA/concatenated_data_one_hot_train.csv\"\n",
    "test_file  = \"D:/iMobie/Final Project/final test of LEGO CAR assambly/DATA OF ONLY RIGHT HAND DATA/concatenated_data_one_hot_test.csv\"\n",
    "val_file = \"D:/iMobie/Final Project/final test of LEGO CAR assambly/DATA OF ONLY RIGHT HAND DATA/concatenated_data_one_hot_val.csv\"\n",
    "\n",
    "# Sequence length for creating sequences\n",
    "sequence_length = 30 # You can adjust this based on your preference\n",
    "\n",
    "# Read CSV files\n",
    "train_data = pd.read_csv(train_file)\n",
    "val_data = pd.read_csv(val_file)\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "# Specify columns to normalize\n",
    "columns_to_normalize = ['RotationRate',\t'HorizontalRate',\t'VerticalRate'\t,'DuctionRate'\t,'FlexionRate',\t'StrengthAmplitude']  # Replace with your actual column names\n",
    "\n",
    "# Normalize selected columns using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_data[columns_to_normalize] = scaler.fit_transform(train_data[columns_to_normalize])\n",
    "val_data[columns_to_normalize] = scaler.transform(val_data[columns_to_normalize])\n",
    "test_data[columns_to_normalize] = scaler.transform(test_data[columns_to_normalize])\n",
    "\n",
    "# Extract features and labels\n",
    "train_features = train_data.drop(['temporal_order', 'action'], axis=1).values\n",
    "train_labels = train_data['action'].values\n",
    "\n",
    "val_features = val_data.drop(['temporal_order', 'action'], axis=1).values\n",
    "val_labels = val_data['action'].values\n",
    "\n",
    "test_features = test_data.drop(['temporal_order', 'action'], axis=1).values\n",
    "test_labels = test_data['action'].values\n",
    "\n",
    "# Use LabelEncoder to convert string labels to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)\n",
    "val_labels = label_encoder.transform(val_labels)\n",
    "test_labels = label_encoder.transform(test_labels)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TemporalDataset(train_features, train_labels, sequence_length)\n",
    "val_dataset = TemporalDataset(val_features, val_labels, sequence_length)\n",
    "test_dataset = TemporalDataset(test_features, test_labels, sequence_length)\n",
    "\n",
    "# Batch size for the data loader\n",
    "batch_size = 32\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print a sample from the training DataLoader\n",
    "for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "    # Print the content of the first batch\n",
    "    if batch_idx == 0:\n",
    "        print(f\"Batch {batch_idx + 1} - Inputs shape: {inputs.shape}, Labels shape: {labels.shape}\")\n",
    "        for i in range(min(5, batch_size)):  # Print only the first 5 samples\n",
    "            print(f\"Sample {i + 1}: {inputs[i]}, Label: {labels[i]}\")\n",
    "\n",
    "    # Break out of the loop after printing the first batch\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e02650e",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9f5eebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400, Batch 1/4457, Loss: 1.9431930780410767\n",
      "Epoch 1/400, Batch 101/4457, Loss: 1.748540997505188\n",
      "Epoch 1/400, Batch 201/4457, Loss: 1.5659160614013672\n",
      "Epoch 1/400, Batch 301/4457, Loss: 1.6819970607757568\n",
      "Epoch 1/400, Batch 401/4457, Loss: 1.3945823907852173\n",
      "Epoch 1/400, Batch 501/4457, Loss: 1.500264286994934\n",
      "Epoch 1/400, Batch 601/4457, Loss: 1.4466806650161743\n",
      "Epoch 1/400, Batch 701/4457, Loss: 1.4145846366882324\n",
      "Epoch 1/400, Batch 801/4457, Loss: 1.3798861503601074\n",
      "Epoch 1/400, Batch 901/4457, Loss: 1.5079249143600464\n",
      "Epoch 1/400, Batch 1001/4457, Loss: 1.429541826248169\n",
      "Epoch 1/400, Batch 1101/4457, Loss: 1.5117307901382446\n",
      "Epoch 1/400, Batch 1201/4457, Loss: 1.4456894397735596\n",
      "Epoch 1/400, Batch 1301/4457, Loss: 1.4755032062530518\n",
      "Epoch 1/400, Batch 1401/4457, Loss: 1.5555425882339478\n",
      "Epoch 1/400, Batch 1501/4457, Loss: 1.467800259590149\n",
      "Epoch 1/400, Batch 1601/4457, Loss: 1.388073444366455\n",
      "Epoch 1/400, Batch 1701/4457, Loss: 1.5655490159988403\n",
      "Epoch 1/400, Batch 1801/4457, Loss: 1.504568099975586\n",
      "Epoch 1/400, Batch 1901/4457, Loss: 1.335923433303833\n",
      "Epoch 1/400, Batch 2001/4457, Loss: 1.4141114950180054\n",
      "Epoch 1/400, Batch 2101/4457, Loss: 1.6991543769836426\n",
      "Epoch 1/400, Batch 2201/4457, Loss: 1.66464364528656\n",
      "Epoch 1/400, Batch 2301/4457, Loss: 1.4296128749847412\n",
      "Epoch 1/400, Batch 2401/4457, Loss: 1.6289570331573486\n",
      "Epoch 1/400, Batch 2501/4457, Loss: 1.6347551345825195\n",
      "Epoch 1/400, Batch 2601/4457, Loss: 1.3527204990386963\n",
      "Epoch 1/400, Batch 2701/4457, Loss: 1.3650585412979126\n",
      "Epoch 1/400, Batch 2801/4457, Loss: 1.4190317392349243\n",
      "Epoch 1/400, Batch 2901/4457, Loss: 1.446791410446167\n",
      "Epoch 1/400, Batch 3001/4457, Loss: 1.3688524961471558\n",
      "Epoch 1/400, Batch 3101/4457, Loss: 1.3596234321594238\n",
      "Epoch 1/400, Batch 3201/4457, Loss: 1.417214035987854\n",
      "Epoch 1/400, Batch 3301/4457, Loss: 1.3506790399551392\n",
      "Epoch 1/400, Batch 3401/4457, Loss: 1.2635886669158936\n",
      "Epoch 1/400, Batch 3501/4457, Loss: 1.634070634841919\n",
      "Epoch 1/400, Batch 3601/4457, Loss: 1.4434396028518677\n",
      "Epoch 1/400, Batch 3701/4457, Loss: 1.2946343421936035\n",
      "Epoch 1/400, Batch 3801/4457, Loss: 1.338496208190918\n",
      "Epoch 1/400, Batch 3901/4457, Loss: 1.2969236373901367\n",
      "Epoch 1/400, Batch 4001/4457, Loss: 1.2285892963409424\n",
      "Epoch 1/400, Batch 4101/4457, Loss: 1.360504388809204\n",
      "Epoch 1/400, Batch 4201/4457, Loss: 1.355398416519165\n",
      "Epoch 1/400, Batch 4301/4457, Loss: 1.572492241859436\n",
      "Epoch 1/400, Batch 4401/4457, Loss: 1.5507944822311401\n",
      "Epoch 1/400, Validation Loss: 1.579881709601198\n",
      "Epoch 2/400, Batch 1/4457, Loss: 1.5405467748641968\n",
      "Epoch 2/400, Batch 101/4457, Loss: 1.3843499422073364\n",
      "Epoch 2/400, Batch 201/4457, Loss: 1.4986926317214966\n",
      "Epoch 2/400, Batch 301/4457, Loss: 1.40498948097229\n",
      "Epoch 2/400, Batch 401/4457, Loss: 1.4138084650039673\n",
      "Epoch 2/400, Batch 501/4457, Loss: 1.3190728425979614\n",
      "Epoch 2/400, Batch 601/4457, Loss: 1.4169366359710693\n",
      "Epoch 2/400, Batch 701/4457, Loss: 1.384708285331726\n",
      "Epoch 2/400, Batch 801/4457, Loss: 1.4716570377349854\n",
      "Epoch 2/400, Batch 901/4457, Loss: 1.383825421333313\n",
      "Epoch 2/400, Batch 1001/4457, Loss: 1.4727108478546143\n",
      "Epoch 2/400, Batch 1101/4457, Loss: 1.2285714149475098\n",
      "Epoch 2/400, Batch 1201/4457, Loss: 1.4472448825836182\n",
      "Epoch 2/400, Batch 1301/4457, Loss: 1.2591755390167236\n",
      "Epoch 2/400, Batch 1401/4457, Loss: 1.327800989151001\n",
      "Epoch 2/400, Batch 1501/4457, Loss: 1.3223119974136353\n",
      "Epoch 2/400, Batch 1601/4457, Loss: 1.3470515012741089\n",
      "Epoch 2/400, Batch 1701/4457, Loss: 1.4158967733383179\n",
      "Epoch 2/400, Batch 1801/4457, Loss: 1.4352929592132568\n",
      "Epoch 2/400, Batch 1901/4457, Loss: 1.2279475927352905\n",
      "Epoch 2/400, Batch 2001/4457, Loss: 1.3494912385940552\n",
      "Epoch 2/400, Batch 2101/4457, Loss: 1.264761209487915\n",
      "Epoch 2/400, Batch 2201/4457, Loss: 1.3579859733581543\n",
      "Epoch 2/400, Batch 2301/4457, Loss: 1.3212130069732666\n",
      "Epoch 2/400, Batch 2401/4457, Loss: 1.3529268503189087\n",
      "Epoch 2/400, Batch 2501/4457, Loss: 1.2269599437713623\n",
      "Epoch 2/400, Batch 2601/4457, Loss: 1.3872722387313843\n",
      "Epoch 2/400, Batch 2701/4457, Loss: 1.3204387426376343\n",
      "Epoch 2/400, Batch 2801/4457, Loss: 1.2572932243347168\n",
      "Epoch 2/400, Batch 2901/4457, Loss: 1.2904340028762817\n",
      "Epoch 2/400, Batch 3001/4457, Loss: 1.2647205591201782\n",
      "Epoch 2/400, Batch 3101/4457, Loss: 1.446563959121704\n",
      "Epoch 2/400, Batch 3201/4457, Loss: 1.2914015054702759\n",
      "Epoch 2/400, Batch 3301/4457, Loss: 1.2311198711395264\n",
      "Epoch 2/400, Batch 3401/4457, Loss: 1.2593213319778442\n",
      "Epoch 2/400, Batch 3501/4457, Loss: 1.3841123580932617\n",
      "Epoch 2/400, Batch 3601/4457, Loss: 1.3331997394561768\n",
      "Epoch 2/400, Batch 3701/4457, Loss: 1.3533028364181519\n",
      "Epoch 2/400, Batch 3801/4457, Loss: 1.6027206182479858\n",
      "Epoch 2/400, Batch 3901/4457, Loss: 1.4791176319122314\n",
      "Epoch 2/400, Batch 4001/4457, Loss: 1.5862066745758057\n",
      "Epoch 2/400, Batch 4101/4457, Loss: 1.550274133682251\n",
      "Epoch 2/400, Batch 4201/4457, Loss: 1.6193265914916992\n",
      "Epoch 2/400, Batch 4301/4457, Loss: 1.4153852462768555\n",
      "Epoch 2/400, Batch 4401/4457, Loss: 1.4359411001205444\n",
      "Epoch 2/400, Validation Loss: 1.6042683255814372\n",
      "Epoch 3/400, Batch 1/4457, Loss: 1.5028626918792725\n",
      "Epoch 3/400, Batch 101/4457, Loss: 1.4727038145065308\n",
      "Epoch 3/400, Batch 201/4457, Loss: 1.4934602975845337\n",
      "Epoch 3/400, Batch 301/4457, Loss: 1.5058199167251587\n",
      "Epoch 3/400, Batch 401/4457, Loss: 1.4982914924621582\n",
      "Epoch 3/400, Batch 501/4457, Loss: 1.5093543529510498\n",
      "Epoch 3/400, Batch 601/4457, Loss: 1.491774320602417\n",
      "Epoch 3/400, Batch 701/4457, Loss: 1.4239288568496704\n",
      "Epoch 3/400, Batch 801/4457, Loss: 1.3843392133712769\n",
      "Epoch 3/400, Batch 901/4457, Loss: 1.329868197441101\n",
      "Epoch 3/400, Batch 1001/4457, Loss: 1.4779380559921265\n",
      "Epoch 3/400, Batch 1101/4457, Loss: 1.573221206665039\n",
      "Epoch 3/400, Batch 1201/4457, Loss: 1.6030666828155518\n",
      "Epoch 3/400, Batch 1301/4457, Loss: 1.4466087818145752\n",
      "Epoch 3/400, Batch 1401/4457, Loss: 1.5733160972595215\n",
      "Epoch 3/400, Batch 1501/4457, Loss: 1.3755161762237549\n",
      "Epoch 3/400, Batch 1601/4457, Loss: 1.3841958045959473\n",
      "Epoch 3/400, Batch 1701/4457, Loss: 1.509238362312317\n",
      "Epoch 3/400, Batch 1801/4457, Loss: 1.420470118522644\n",
      "Epoch 3/400, Batch 1901/4457, Loss: 1.4779363870620728\n",
      "Epoch 3/400, Batch 2001/4457, Loss: 1.4477628469467163\n",
      "Epoch 3/400, Batch 2101/4457, Loss: 1.5124658346176147\n",
      "Epoch 3/400, Batch 2201/4457, Loss: 1.5717076063156128\n",
      "Epoch 3/400, Batch 2301/4457, Loss: 1.3541737794876099\n",
      "Epoch 3/400, Batch 2401/4457, Loss: 1.4320589303970337\n",
      "Epoch 3/400, Batch 2501/4457, Loss: 1.615939736366272\n",
      "Epoch 3/400, Batch 2601/4457, Loss: 1.5430105924606323\n",
      "Epoch 3/400, Batch 2701/4457, Loss: 1.3849308490753174\n",
      "Epoch 3/400, Batch 2801/4457, Loss: 1.2904424667358398\n",
      "Epoch 3/400, Batch 2901/4457, Loss: 1.3234797716140747\n",
      "Epoch 3/400, Batch 3001/4457, Loss: 1.3218920230865479\n",
      "Epoch 3/400, Batch 3101/4457, Loss: 1.259748101234436\n",
      "Epoch 3/400, Batch 3201/4457, Loss: 1.2940338850021362\n",
      "Epoch 3/400, Batch 3301/4457, Loss: 1.3305435180664062\n",
      "Epoch 3/400, Batch 3401/4457, Loss: 1.1967318058013916\n",
      "Epoch 3/400, Batch 3501/4457, Loss: 1.2598469257354736\n",
      "Epoch 3/400, Batch 3601/4457, Loss: 1.3508630990982056\n",
      "Epoch 3/400, Batch 3701/4457, Loss: 1.260238766670227\n",
      "Epoch 3/400, Batch 3801/4457, Loss: 1.3078515529632568\n",
      "Epoch 3/400, Batch 3901/4457, Loss: 1.2804279327392578\n",
      "Epoch 3/400, Batch 4001/4457, Loss: 1.2551562786102295\n",
      "Epoch 3/400, Batch 4101/4457, Loss: 1.376875400543213\n",
      "Epoch 3/400, Batch 4201/4457, Loss: 1.2904715538024902\n",
      "Epoch 3/400, Batch 4301/4457, Loss: 1.2905974388122559\n",
      "Epoch 3/400, Batch 4401/4457, Loss: 1.2591931819915771\n",
      "Epoch 3/400, Validation Loss: 1.5075750652523268\n",
      "Epoch 4/400, Batch 1/4457, Loss: 1.489243507385254\n",
      "Epoch 4/400, Batch 101/4457, Loss: 1.2022300958633423\n",
      "Epoch 4/400, Batch 201/4457, Loss: 1.2904053926467896\n",
      "Epoch 4/400, Batch 301/4457, Loss: 1.335158348083496\n",
      "Epoch 4/400, Batch 401/4457, Loss: 1.2592066526412964\n",
      "Epoch 4/400, Batch 501/4457, Loss: 1.3221015930175781\n",
      "Epoch 4/400, Batch 601/4457, Loss: 1.4147461652755737\n",
      "Epoch 4/400, Batch 701/4457, Loss: 1.2900352478027344\n",
      "Epoch 4/400, Batch 801/4457, Loss: 1.2600187063217163\n",
      "Epoch 4/400, Batch 901/4457, Loss: 1.3406195640563965\n",
      "Epoch 4/400, Batch 1001/4457, Loss: 1.2714784145355225\n",
      "Epoch 4/400, Batch 1101/4457, Loss: 1.289706826210022\n",
      "Epoch 4/400, Batch 1201/4457, Loss: 1.259603500366211\n",
      "Epoch 4/400, Batch 1301/4457, Loss: 1.2319308519363403\n",
      "Epoch 4/400, Batch 1401/4457, Loss: 1.317928671836853\n",
      "Epoch 4/400, Batch 1501/4457, Loss: 1.335627555847168\n",
      "Epoch 4/400, Batch 1601/4457, Loss: 1.3824702501296997\n",
      "Epoch 4/400, Batch 1701/4457, Loss: 1.2073649168014526\n",
      "Epoch 4/400, Batch 1801/4457, Loss: 1.353024959564209\n",
      "Epoch 4/400, Batch 1901/4457, Loss: 1.344771385192871\n",
      "Epoch 4/400, Batch 2001/4457, Loss: 1.3958929777145386\n",
      "Epoch 4/400, Batch 2101/4457, Loss: 1.2378777265548706\n",
      "Epoch 4/400, Batch 2201/4457, Loss: 1.2794691324234009\n",
      "Epoch 4/400, Batch 2301/4457, Loss: 1.197105884552002\n",
      "Epoch 4/400, Batch 2401/4457, Loss: 1.2893459796905518\n",
      "Epoch 4/400, Batch 2501/4457, Loss: 1.2526034116744995\n",
      "Epoch 4/400, Batch 2601/4457, Loss: 1.1967333555221558\n",
      "Epoch 4/400, Batch 2701/4457, Loss: 1.2699353694915771\n",
      "Epoch 4/400, Batch 2801/4457, Loss: 1.3183834552764893\n",
      "Epoch 4/400, Batch 2901/4457, Loss: 1.231186032295227\n",
      "Epoch 4/400, Batch 3001/4457, Loss: 1.2483738660812378\n",
      "Epoch 4/400, Batch 3101/4457, Loss: 1.3528742790222168\n",
      "Epoch 4/400, Batch 3201/4457, Loss: 1.4158525466918945\n",
      "Epoch 4/400, Batch 3301/4457, Loss: 1.3762876987457275\n",
      "Epoch 4/400, Batch 3401/4457, Loss: 1.3130159378051758\n",
      "Epoch 4/400, Batch 3501/4457, Loss: 1.1664637327194214\n",
      "Epoch 4/400, Batch 3601/4457, Loss: 1.3115912675857544\n",
      "Epoch 4/400, Batch 3701/4457, Loss: 1.2545534372329712\n",
      "Epoch 4/400, Batch 3801/4457, Loss: 1.2649086713790894\n",
      "Epoch 4/400, Batch 3901/4457, Loss: 1.2029062509536743\n",
      "Epoch 4/400, Batch 4001/4457, Loss: 1.4188923835754395\n",
      "Epoch 4/400, Batch 4101/4457, Loss: 1.2898139953613281\n",
      "Epoch 4/400, Batch 4201/4457, Loss: 1.5363706350326538\n",
      "Epoch 4/400, Batch 4301/4457, Loss: 1.4469677209854126\n",
      "Epoch 4/400, Batch 4401/4457, Loss: 1.3762943744659424\n",
      "Epoch 4/400, Validation Loss: 1.5308984523964306\n",
      "Epoch 5/400, Batch 1/4457, Loss: 1.480001449584961\n",
      "Epoch 5/400, Batch 101/4457, Loss: 1.4131423234939575\n",
      "Epoch 5/400, Batch 201/4457, Loss: 1.5108338594436646\n",
      "Epoch 5/400, Batch 301/4457, Loss: 1.353276252746582\n",
      "Epoch 5/400, Batch 401/4457, Loss: 1.1984918117523193\n",
      "Epoch 5/400, Batch 501/4457, Loss: 1.1657062768936157\n",
      "Epoch 5/400, Batch 601/4457, Loss: 1.3312832117080688\n",
      "Epoch 5/400, Batch 701/4457, Loss: 1.2599003314971924\n",
      "Epoch 5/400, Batch 801/4457, Loss: 1.2680182456970215\n",
      "Epoch 5/400, Batch 901/4457, Loss: 1.4516688585281372\n",
      "Epoch 5/400, Batch 1001/4457, Loss: 1.2138373851776123\n",
      "Epoch 5/400, Batch 1101/4457, Loss: 1.274498701095581\n",
      "Epoch 5/400, Batch 1201/4457, Loss: 1.2339396476745605\n",
      "Epoch 5/400, Batch 1301/4457, Loss: 1.2317346334457397\n",
      "Epoch 5/400, Batch 1401/4457, Loss: 1.2588728666305542\n",
      "Epoch 5/400, Batch 1501/4457, Loss: 1.2592813968658447\n",
      "Epoch 5/400, Batch 1601/4457, Loss: 1.216627836227417\n",
      "Epoch 5/400, Batch 1701/4457, Loss: 1.279060959815979\n",
      "Epoch 5/400, Batch 1801/4457, Loss: 1.2578537464141846\n",
      "Epoch 5/400, Batch 1901/4457, Loss: 1.2593626976013184\n",
      "Epoch 5/400, Batch 2001/4457, Loss: 1.1970038414001465\n",
      "Epoch 5/400, Batch 2101/4457, Loss: 1.2239949703216553\n",
      "Epoch 5/400, Batch 2201/4457, Loss: 1.1750766038894653\n",
      "Epoch 5/400, Batch 2301/4457, Loss: 1.2282594442367554\n",
      "Epoch 5/400, Batch 2401/4457, Loss: 1.3731142282485962\n",
      "Epoch 5/400, Batch 2501/4457, Loss: 1.228055715560913\n",
      "Epoch 5/400, Batch 2601/4457, Loss: 1.1966421604156494\n",
      "Epoch 5/400, Batch 2701/4457, Loss: 1.2303402423858643\n",
      "Epoch 5/400, Batch 2801/4457, Loss: 1.2596065998077393\n",
      "Epoch 5/400, Batch 2901/4457, Loss: 1.1656008958816528\n",
      "Epoch 5/400, Batch 3001/4457, Loss: 1.235472321510315\n",
      "Epoch 5/400, Batch 3101/4457, Loss: 1.2762634754180908\n",
      "Epoch 5/400, Batch 3201/4457, Loss: 1.259760856628418\n",
      "Epoch 5/400, Batch 3301/4457, Loss: 1.1763488054275513\n",
      "Epoch 5/400, Batch 3401/4457, Loss: 1.197738766670227\n",
      "Epoch 5/400, Batch 3501/4457, Loss: 1.1684266328811646\n",
      "Epoch 5/400, Batch 3601/4457, Loss: 1.2037632465362549\n",
      "Epoch 5/400, Batch 3701/4457, Loss: 1.225379228591919\n",
      "Epoch 5/400, Batch 3801/4457, Loss: 1.1672236919403076\n",
      "Epoch 5/400, Batch 3901/4457, Loss: 1.2659920454025269\n",
      "Epoch 5/400, Batch 4001/4457, Loss: 1.2109694480895996\n",
      "Epoch 5/400, Batch 4101/4457, Loss: 1.1925784349441528\n",
      "Epoch 5/400, Batch 4201/4457, Loss: 1.3015713691711426\n",
      "Epoch 5/400, Batch 4301/4457, Loss: 1.1657087802886963\n",
      "Epoch 5/400, Batch 4401/4457, Loss: 1.1758424043655396\n",
      "Epoch 5/400, Validation Loss: 1.322443085766974\n",
      "Epoch 6/400, Batch 1/4457, Loss: 1.172879934310913\n",
      "Epoch 6/400, Batch 101/4457, Loss: 1.2837817668914795\n",
      "Epoch 6/400, Batch 201/4457, Loss: 1.196319580078125\n",
      "Epoch 6/400, Batch 301/4457, Loss: 1.1978477239608765\n",
      "Epoch 6/400, Batch 401/4457, Loss: 1.1974135637283325\n",
      "Epoch 6/400, Batch 501/4457, Loss: 1.2282321453094482\n",
      "Epoch 6/400, Batch 601/4457, Loss: 1.1677058935165405\n",
      "Epoch 6/400, Batch 701/4457, Loss: 1.165472388267517\n",
      "Epoch 6/400, Batch 801/4457, Loss: 1.1654300689697266\n",
      "Epoch 6/400, Batch 901/4457, Loss: 1.213666319847107\n",
      "Epoch 6/400, Batch 1001/4457, Loss: 1.1669431924819946\n",
      "Epoch 6/400, Batch 1101/4457, Loss: 1.2531228065490723\n",
      "Epoch 6/400, Batch 1201/4457, Loss: 1.2553653717041016\n",
      "Epoch 6/400, Batch 1301/4457, Loss: 1.2540597915649414\n",
      "Epoch 6/400, Batch 1401/4457, Loss: 1.2981936931610107\n",
      "Epoch 6/400, Batch 1501/4457, Loss: 1.2196763753890991\n",
      "Epoch 6/400, Batch 1601/4457, Loss: 1.2307208776474\n",
      "Epoch 6/400, Batch 1701/4457, Loss: 1.1661434173583984\n",
      "Epoch 6/400, Batch 1801/4457, Loss: 1.284279227256775\n",
      "Epoch 6/400, Batch 1901/4457, Loss: 1.2620824575424194\n",
      "Epoch 6/400, Batch 2001/4457, Loss: 1.2897913455963135\n",
      "Epoch 6/400, Batch 2101/4457, Loss: 1.3003257513046265\n",
      "Epoch 6/400, Batch 2201/4457, Loss: 1.2153228521347046\n",
      "Epoch 6/400, Batch 2301/4457, Loss: 1.2360904216766357\n",
      "Epoch 6/400, Batch 2401/4457, Loss: 1.1971923112869263\n",
      "Epoch 6/400, Batch 2501/4457, Loss: 1.1981483697891235\n",
      "Epoch 6/400, Batch 2601/4457, Loss: 1.165486454963684\n",
      "Epoch 6/400, Batch 2701/4457, Loss: 1.1655182838439941\n",
      "Epoch 6/400, Batch 2801/4457, Loss: 1.2608447074890137\n",
      "Epoch 6/400, Batch 2901/4457, Loss: 1.1851632595062256\n",
      "Epoch 6/400, Batch 3001/4457, Loss: 1.1969910860061646\n",
      "Epoch 6/400, Batch 3101/4457, Loss: 1.2576020956039429\n",
      "Epoch 6/400, Batch 3201/4457, Loss: 1.2002931833267212\n",
      "Epoch 6/400, Batch 3301/4457, Loss: 1.2597033977508545\n",
      "Epoch 6/400, Batch 3401/4457, Loss: 1.259190559387207\n",
      "Epoch 6/400, Batch 3501/4457, Loss: 1.1864889860153198\n",
      "Epoch 6/400, Batch 3601/4457, Loss: 1.196684718132019\n",
      "Epoch 6/400, Batch 3701/4457, Loss: 1.1654926538467407\n",
      "Epoch 6/400, Batch 3801/4457, Loss: 1.1967076063156128\n",
      "Epoch 6/400, Batch 3901/4457, Loss: 1.196697473526001\n",
      "Epoch 6/400, Batch 4001/4457, Loss: 1.2564131021499634\n",
      "Epoch 6/400, Batch 4101/4457, Loss: 1.1955417394638062\n",
      "Epoch 6/400, Batch 4201/4457, Loss: 1.1654314994812012\n",
      "Epoch 6/400, Batch 4301/4457, Loss: 1.2301074266433716\n",
      "Epoch 6/400, Batch 4401/4457, Loss: 1.2551335096359253\n",
      "Epoch 6/400, Validation Loss: 1.3082638109723728\n",
      "Epoch 7/400, Batch 1/4457, Loss: 1.2424942255020142\n",
      "Epoch 7/400, Batch 101/4457, Loss: 1.2410657405853271\n",
      "Epoch 7/400, Batch 201/4457, Loss: 1.22793447971344\n",
      "Epoch 7/400, Batch 301/4457, Loss: 1.259181022644043\n",
      "Epoch 7/400, Batch 401/4457, Loss: 1.2591699361801147\n",
      "Epoch 7/400, Batch 501/4457, Loss: 1.2264190912246704\n",
      "Epoch 7/400, Batch 601/4457, Loss: 1.1654269695281982\n",
      "Epoch 7/400, Batch 701/4457, Loss: 1.1999375820159912\n",
      "Epoch 7/400, Batch 801/4457, Loss: 1.165454626083374\n",
      "Epoch 7/400, Batch 901/4457, Loss: 1.1868518590927124\n",
      "Epoch 7/400, Batch 1001/4457, Loss: 1.1654287576675415\n",
      "Epoch 7/400, Batch 1101/4457, Loss: 1.2591941356658936\n",
      "Epoch 7/400, Batch 1201/4457, Loss: 1.166506052017212\n",
      "Epoch 7/400, Batch 1301/4457, Loss: 1.1654592752456665\n",
      "Epoch 7/400, Batch 1401/4457, Loss: 1.165485143661499\n",
      "Epoch 7/400, Batch 1501/4457, Loss: 1.2541687488555908\n",
      "Epoch 7/400, Batch 1601/4457, Loss: 1.2666149139404297\n",
      "Epoch 7/400, Batch 1701/4457, Loss: 1.165521264076233\n",
      "Epoch 7/400, Batch 1801/4457, Loss: 1.2070834636688232\n",
      "Epoch 7/400, Batch 1901/4457, Loss: 1.1996960639953613\n",
      "Epoch 7/400, Batch 2001/4457, Loss: 1.1962895393371582\n",
      "Epoch 7/400, Batch 2101/4457, Loss: 1.259232521057129\n",
      "Epoch 7/400, Batch 2201/4457, Loss: 1.2602181434631348\n",
      "Epoch 7/400, Batch 2301/4457, Loss: 1.2449854612350464\n",
      "Epoch 7/400, Batch 2401/4457, Loss: 1.218484878540039\n",
      "Epoch 7/400, Batch 2501/4457, Loss: 1.2278741598129272\n",
      "Epoch 7/400, Batch 2601/4457, Loss: 1.1657977104187012\n",
      "Epoch 7/400, Batch 2701/4457, Loss: 1.1978881359100342\n",
      "Epoch 7/400, Batch 2801/4457, Loss: 1.2602795362472534\n",
      "Epoch 7/400, Batch 2901/4457, Loss: 1.2134780883789062\n",
      "Epoch 7/400, Batch 3001/4457, Loss: 1.1655404567718506\n",
      "Epoch 7/400, Batch 3101/4457, Loss: 1.227928638458252\n",
      "Epoch 7/400, Batch 3201/4457, Loss: 1.1664698123931885\n",
      "Epoch 7/400, Batch 3301/4457, Loss: 1.1967371702194214\n",
      "Epoch 7/400, Batch 3401/4457, Loss: 1.1966811418533325\n",
      "Epoch 7/400, Batch 3501/4457, Loss: 1.2302091121673584\n",
      "Epoch 7/400, Batch 3601/4457, Loss: 1.1991748809814453\n",
      "Epoch 7/400, Batch 3701/4457, Loss: 1.1993401050567627\n",
      "Epoch 7/400, Batch 3801/4457, Loss: 1.2118533849716187\n",
      "Epoch 7/400, Batch 3901/4457, Loss: 1.1656439304351807\n",
      "Epoch 7/400, Batch 4001/4457, Loss: 1.2284555435180664\n",
      "Epoch 7/400, Batch 4101/4457, Loss: 1.1976847648620605\n",
      "Epoch 7/400, Batch 4201/4457, Loss: 1.2279942035675049\n",
      "Epoch 7/400, Batch 4301/4457, Loss: 1.2985990047454834\n",
      "Epoch 7/400, Batch 4401/4457, Loss: 1.19669771194458\n",
      "Epoch 7/400, Validation Loss: 1.3014835608857018\n",
      "Epoch 8/400, Batch 1/4457, Loss: 1.1954174041748047\n",
      "Epoch 8/400, Batch 101/4457, Loss: 1.3381205797195435\n",
      "Epoch 8/400, Batch 201/4457, Loss: 1.2111166715621948\n",
      "Epoch 8/400, Batch 301/4457, Loss: 1.1845803260803223\n",
      "Epoch 8/400, Batch 401/4457, Loss: 1.257832646369934\n",
      "Epoch 8/400, Batch 501/4457, Loss: 1.1654506921768188\n",
      "Epoch 8/400, Batch 601/4457, Loss: 1.2351410388946533\n",
      "Epoch 8/400, Batch 701/4457, Loss: 1.2323096990585327\n",
      "Epoch 8/400, Batch 801/4457, Loss: 1.2281070947647095\n",
      "Epoch 8/400, Batch 901/4457, Loss: 1.196656346321106\n",
      "Epoch 8/400, Batch 1001/4457, Loss: 1.1968332529067993\n",
      "Epoch 8/400, Batch 1101/4457, Loss: 1.1966971158981323\n",
      "Epoch 8/400, Batch 1201/4457, Loss: 1.1966888904571533\n",
      "Epoch 8/400, Batch 1301/4457, Loss: 1.1966811418533325\n",
      "Epoch 8/400, Batch 1401/4457, Loss: 1.3253517150878906\n",
      "Epoch 8/400, Batch 1501/4457, Loss: 1.1654579639434814\n",
      "Epoch 8/400, Batch 1601/4457, Loss: 1.287119746208191\n",
      "Epoch 8/400, Batch 1701/4457, Loss: 1.25917387008667\n",
      "Epoch 8/400, Batch 1801/4457, Loss: 1.2275478839874268\n",
      "Epoch 8/400, Batch 1901/4457, Loss: 1.229626178741455\n",
      "Epoch 8/400, Batch 2001/4457, Loss: 1.2279343605041504\n",
      "Epoch 8/400, Batch 2101/4457, Loss: 1.1966869831085205\n",
      "Epoch 8/400, Batch 2201/4457, Loss: 1.2279236316680908\n",
      "Epoch 8/400, Batch 2301/4457, Loss: 1.2098796367645264\n",
      "Epoch 8/400, Batch 2401/4457, Loss: 1.1992123126983643\n",
      "Epoch 8/400, Batch 2501/4457, Loss: 1.1654303073883057\n",
      "Epoch 8/400, Batch 2601/4457, Loss: 1.1973741054534912\n",
      "Epoch 8/400, Batch 2701/4457, Loss: 1.172666072845459\n",
      "Epoch 8/400, Batch 2801/4457, Loss: 1.2279586791992188\n",
      "Epoch 8/400, Batch 2901/4457, Loss: 1.2419543266296387\n",
      "Epoch 8/400, Batch 3001/4457, Loss: 1.1655113697052002\n",
      "Epoch 8/400, Batch 3101/4457, Loss: 1.2279618978500366\n",
      "Epoch 8/400, Batch 3201/4457, Loss: 1.2054502964019775\n",
      "Epoch 8/400, Batch 3301/4457, Loss: 1.1966780424118042\n",
      "Epoch 8/400, Batch 3401/4457, Loss: 1.1668325662612915\n",
      "Epoch 8/400, Batch 3501/4457, Loss: 1.196812391281128\n",
      "Epoch 8/400, Batch 3601/4457, Loss: 1.166858196258545\n",
      "Epoch 8/400, Batch 3701/4457, Loss: 1.1967170238494873\n",
      "Epoch 8/400, Batch 3801/4457, Loss: 1.22792387008667\n",
      "Epoch 8/400, Batch 3901/4457, Loss: 1.2342792749404907\n",
      "Epoch 8/400, Batch 4001/4457, Loss: 1.3164581060409546\n",
      "Epoch 8/400, Batch 4101/4457, Loss: 1.2110470533370972\n",
      "Epoch 8/400, Batch 4201/4457, Loss: 1.1655101776123047\n",
      "Epoch 8/400, Batch 4301/4457, Loss: 1.165649175643921\n",
      "Epoch 8/400, Batch 4401/4457, Loss: 1.2288713455200195\n",
      "Epoch 8/400, Validation Loss: 1.3016501104547864\n",
      "Epoch 9/400, Batch 1/4457, Loss: 1.2289981842041016\n",
      "Epoch 9/400, Batch 101/4457, Loss: 1.1967722177505493\n",
      "Epoch 9/400, Batch 201/4457, Loss: 1.1966828107833862\n",
      "Epoch 9/400, Batch 301/4457, Loss: 1.2074133157730103\n",
      "Epoch 9/400, Batch 401/4457, Loss: 1.1974905729293823\n",
      "Epoch 9/400, Batch 501/4457, Loss: 1.1657428741455078\n",
      "Epoch 9/400, Batch 601/4457, Loss: 1.2222683429718018\n",
      "Epoch 9/400, Batch 701/4457, Loss: 1.1966949701309204\n",
      "Epoch 9/400, Batch 801/4457, Loss: 1.2515840530395508\n",
      "Epoch 9/400, Batch 901/4457, Loss: 1.165485143661499\n",
      "Epoch 9/400, Batch 1001/4457, Loss: 1.1654331684112549\n",
      "Epoch 9/400, Batch 1101/4457, Loss: 1.2227498292922974\n",
      "Epoch 9/400, Batch 1201/4457, Loss: 1.165854573249817\n",
      "Epoch 9/400, Batch 1301/4457, Loss: 1.1654318571090698\n",
      "Epoch 9/400, Batch 1401/4457, Loss: 1.1967263221740723\n",
      "Epoch 9/400, Batch 1501/4457, Loss: 1.1966793537139893\n",
      "Epoch 9/400, Batch 1601/4457, Loss: 1.2279322147369385\n",
      "Epoch 9/400, Batch 1701/4457, Loss: 1.1686903238296509\n",
      "Epoch 9/400, Batch 1801/4457, Loss: 1.1711952686309814\n",
      "Epoch 9/400, Batch 1901/4457, Loss: 1.248732328414917\n",
      "Epoch 9/400, Batch 2001/4457, Loss: 1.1968507766723633\n",
      "Epoch 9/400, Batch 2101/4457, Loss: 1.1965999603271484\n",
      "Epoch 9/400, Batch 2201/4457, Loss: 1.1915847063064575\n",
      "Epoch 9/400, Batch 2301/4457, Loss: 1.1654233932495117\n",
      "Epoch 9/400, Batch 2401/4457, Loss: 1.228757619857788\n",
      "Epoch 9/400, Batch 2501/4457, Loss: 1.2274702787399292\n",
      "Epoch 9/400, Batch 2601/4457, Loss: 1.2202084064483643\n",
      "Epoch 9/400, Batch 2701/4457, Loss: 1.2279242277145386\n",
      "Epoch 9/400, Batch 2801/4457, Loss: 1.1954419612884521\n",
      "Epoch 9/400, Batch 2901/4457, Loss: 1.2820324897766113\n",
      "Epoch 9/400, Batch 3001/4457, Loss: 1.196689248085022\n",
      "Epoch 9/400, Batch 3101/4457, Loss: 1.1659773588180542\n",
      "Epoch 9/400, Batch 3201/4457, Loss: 1.264320731163025\n",
      "Epoch 9/400, Batch 3301/4457, Loss: 1.2892602682113647\n",
      "Epoch 9/400, Batch 3401/4457, Loss: 1.1668213605880737\n",
      "Epoch 9/400, Batch 3501/4457, Loss: 1.1947447061538696\n",
      "Epoch 9/400, Batch 3601/4457, Loss: 1.1966859102249146\n",
      "Epoch 9/400, Batch 3701/4457, Loss: 1.1987006664276123\n",
      "Epoch 9/400, Batch 3801/4457, Loss: 1.196948528289795\n",
      "Epoch 9/400, Batch 3901/4457, Loss: 1.165496587753296\n",
      "Epoch 9/400, Batch 4001/4457, Loss: 1.223900556564331\n",
      "Epoch 9/400, Batch 4101/4457, Loss: 1.2283921241760254\n",
      "Epoch 9/400, Batch 4201/4457, Loss: 1.2304128408432007\n",
      "Epoch 9/400, Batch 4301/4457, Loss: 1.2279261350631714\n",
      "Epoch 9/400, Batch 4401/4457, Loss: 1.2640674114227295\n",
      "Epoch 9/400, Validation Loss: 1.2897164218482517\n",
      "Epoch 10/400, Batch 1/4457, Loss: 1.1966900825500488\n",
      "Epoch 10/400, Batch 101/4457, Loss: 1.2302515506744385\n",
      "Epoch 10/400, Batch 201/4457, Loss: 1.1654248237609863\n",
      "Epoch 10/400, Batch 301/4457, Loss: 1.198225498199463\n",
      "Epoch 10/400, Batch 401/4457, Loss: 1.1966800689697266\n",
      "Epoch 10/400, Batch 501/4457, Loss: 1.2595088481903076\n",
      "Epoch 10/400, Batch 601/4457, Loss: 1.2498092651367188\n",
      "Epoch 10/400, Batch 701/4457, Loss: 1.2279250621795654\n",
      "Epoch 10/400, Batch 801/4457, Loss: 1.2021011114120483\n",
      "Epoch 10/400, Batch 901/4457, Loss: 1.1968728303909302\n",
      "Epoch 10/400, Batch 1001/4457, Loss: 1.165424108505249\n",
      "Epoch 10/400, Batch 1101/4457, Loss: 1.1654245853424072\n",
      "Epoch 10/400, Batch 1201/4457, Loss: 1.2279428243637085\n",
      "Epoch 10/400, Batch 1301/4457, Loss: 1.174045443534851\n",
      "Epoch 10/400, Batch 1401/4457, Loss: 1.1654505729675293\n",
      "Epoch 10/400, Batch 1501/4457, Loss: 1.2279208898544312\n",
      "Epoch 10/400, Batch 1601/4457, Loss: 1.1969225406646729\n",
      "Epoch 10/400, Batch 1701/4457, Loss: 1.2279328107833862\n",
      "Epoch 10/400, Batch 1801/4457, Loss: 1.2593333721160889\n",
      "Epoch 10/400, Batch 1901/4457, Loss: 1.2091097831726074\n",
      "Epoch 10/400, Batch 2001/4457, Loss: 1.2407352924346924\n",
      "Epoch 10/400, Batch 2101/4457, Loss: 1.245980143547058\n",
      "Epoch 10/400, Batch 2201/4457, Loss: 1.2811853885650635\n",
      "Epoch 10/400, Batch 2301/4457, Loss: 1.1654266119003296\n",
      "Epoch 10/400, Batch 2401/4457, Loss: 1.2260133028030396\n",
      "Epoch 10/400, Batch 2501/4457, Loss: 1.2524774074554443\n",
      "Epoch 10/400, Batch 2601/4457, Loss: 1.1966735124588013\n",
      "Epoch 10/400, Batch 2701/4457, Loss: 1.1971379518508911\n",
      "Epoch 10/400, Batch 2801/4457, Loss: 1.22201669216156\n",
      "Epoch 10/400, Batch 2901/4457, Loss: 1.1656328439712524\n",
      "Epoch 10/400, Batch 3001/4457, Loss: 1.2348707914352417\n",
      "Epoch 10/400, Batch 3101/4457, Loss: 1.2594504356384277\n",
      "Epoch 10/400, Batch 3201/4457, Loss: 1.2591856718063354\n",
      "Epoch 10/400, Batch 3301/4457, Loss: 1.1966781616210938\n",
      "Epoch 10/400, Batch 3401/4457, Loss: 1.1966941356658936\n",
      "Epoch 10/400, Batch 3501/4457, Loss: 1.1967432498931885\n",
      "Epoch 10/400, Batch 3601/4457, Loss: 1.1654316186904907\n",
      "Epoch 10/400, Batch 3701/4457, Loss: 1.2279332876205444\n",
      "Epoch 10/400, Batch 3801/4457, Loss: 1.196683406829834\n",
      "Epoch 10/400, Batch 3901/4457, Loss: 1.1966767311096191\n",
      "Epoch 10/400, Batch 4001/4457, Loss: 1.1654300689697266\n",
      "Epoch 10/400, Batch 4101/4457, Loss: 1.1966743469238281\n",
      "Epoch 10/400, Batch 4201/4457, Loss: 1.2279363870620728\n",
      "Epoch 10/400, Batch 4301/4457, Loss: 1.1931266784667969\n",
      "Epoch 10/400, Batch 4401/4457, Loss: 1.2273787260055542\n",
      "Epoch 10/400, Validation Loss: 1.2946243532120236\n",
      "Epoch 11/400, Batch 1/4457, Loss: 1.1655055284500122\n",
      "Epoch 11/400, Batch 101/4457, Loss: 1.1966654062271118\n",
      "Epoch 11/400, Batch 201/4457, Loss: 1.225048542022705\n",
      "Epoch 11/400, Batch 301/4457, Loss: 1.1966750621795654\n",
      "Epoch 11/400, Batch 401/4457, Loss: 1.259220838546753\n",
      "Epoch 11/400, Batch 501/4457, Loss: 1.211432933807373\n",
      "Epoch 11/400, Batch 601/4457, Loss: 1.1655101776123047\n",
      "Epoch 11/400, Batch 701/4457, Loss: 1.258316159248352\n",
      "Epoch 11/400, Batch 801/4457, Loss: 1.1966787576675415\n",
      "Epoch 11/400, Batch 901/4457, Loss: 1.165423035621643\n",
      "Epoch 11/400, Batch 1001/4457, Loss: 1.2415647506713867\n",
      "Epoch 11/400, Batch 1101/4457, Loss: 1.167060375213623\n",
      "Epoch 11/400, Batch 1201/4457, Loss: 1.227922797203064\n",
      "Epoch 11/400, Batch 1301/4457, Loss: 1.1654229164123535\n",
      "Epoch 11/400, Batch 1401/4457, Loss: 1.1966781616210938\n",
      "Epoch 11/400, Batch 1501/4457, Loss: 1.1986322402954102\n",
      "Epoch 11/400, Batch 1601/4457, Loss: 1.1967148780822754\n",
      "Epoch 11/400, Batch 1701/4457, Loss: 1.1654616594314575\n",
      "Epoch 11/400, Batch 1801/4457, Loss: 1.2571033239364624\n",
      "Epoch 11/400, Batch 1901/4457, Loss: 1.196666955947876\n",
      "Epoch 11/400, Batch 2001/4457, Loss: 1.1655299663543701\n",
      "Epoch 11/400, Batch 2101/4457, Loss: 1.1966829299926758\n",
      "Epoch 11/400, Batch 2201/4457, Loss: 1.196846842765808\n",
      "Epoch 11/400, Batch 2301/4457, Loss: 1.2901856899261475\n",
      "Epoch 11/400, Batch 2401/4457, Loss: 1.1966736316680908\n",
      "Epoch 11/400, Batch 2501/4457, Loss: 1.2272753715515137\n",
      "Epoch 11/400, Batch 2601/4457, Loss: 1.1967219114303589\n",
      "Epoch 11/400, Batch 2701/4457, Loss: 1.1966735124588013\n",
      "Epoch 11/400, Batch 2801/4457, Loss: 1.1654235124588013\n",
      "Epoch 11/400, Batch 2901/4457, Loss: 1.1966781616210938\n",
      "Epoch 11/400, Batch 3001/4457, Loss: 1.1659293174743652\n",
      "Epoch 11/400, Batch 3101/4457, Loss: 1.1969497203826904\n",
      "Epoch 11/400, Batch 3201/4457, Loss: 1.2280011177062988\n",
      "Epoch 11/400, Batch 3301/4457, Loss: 1.196685791015625\n",
      "Epoch 11/400, Batch 3401/4457, Loss: 1.196135401725769\n",
      "Epoch 11/400, Batch 3501/4457, Loss: 1.1654248237609863\n",
      "Epoch 11/400, Batch 3601/4457, Loss: 1.2045462131500244\n",
      "Epoch 11/400, Batch 3701/4457, Loss: 1.1966476440429688\n",
      "Epoch 11/400, Batch 3801/4457, Loss: 1.228407621383667\n",
      "Epoch 11/400, Batch 3901/4457, Loss: 1.2279555797576904\n",
      "Epoch 11/400, Batch 4001/4457, Loss: 1.183525800704956\n",
      "Epoch 11/400, Batch 4101/4457, Loss: 1.1966809034347534\n",
      "Epoch 11/400, Batch 4201/4457, Loss: 1.2594752311706543\n",
      "Epoch 11/400, Batch 4301/4457, Loss: 1.2498736381530762\n",
      "Epoch 11/400, Batch 4401/4457, Loss: 1.1654859781265259\n",
      "Epoch 11/400, Validation Loss: 1.2944327436742329\n",
      "Epoch 12/400, Batch 1/4457, Loss: 1.1966756582260132\n",
      "Epoch 12/400, Batch 101/4457, Loss: 1.2282475233078003\n",
      "Epoch 12/400, Batch 201/4457, Loss: 1.2271137237548828\n",
      "Epoch 12/400, Batch 301/4457, Loss: 1.2288053035736084\n",
      "Epoch 12/400, Batch 401/4457, Loss: 1.196672797203064\n",
      "Epoch 12/400, Batch 501/4457, Loss: 1.1966818571090698\n",
      "Epoch 12/400, Batch 601/4457, Loss: 1.2296382188796997\n",
      "Epoch 12/400, Batch 701/4457, Loss: 1.1969411373138428\n",
      "Epoch 12/400, Batch 801/4457, Loss: 1.1966791152954102\n",
      "Epoch 12/400, Batch 901/4457, Loss: 1.1967657804489136\n",
      "Epoch 12/400, Batch 1001/4457, Loss: 1.2035417556762695\n",
      "Epoch 12/400, Batch 1101/4457, Loss: 1.2265764474868774\n",
      "Epoch 12/400, Batch 1201/4457, Loss: 1.2100616693496704\n",
      "Epoch 12/400, Batch 1301/4457, Loss: 1.1940133571624756\n",
      "Epoch 12/400, Batch 1401/4457, Loss: 1.1966981887817383\n",
      "Epoch 12/400, Batch 1501/4457, Loss: 1.1967118978500366\n",
      "Epoch 12/400, Batch 1601/4457, Loss: 1.2421561479568481\n",
      "Epoch 12/400, Batch 1701/4457, Loss: 1.1966739892959595\n",
      "Epoch 12/400, Batch 1801/4457, Loss: 1.1698638200759888\n",
      "Epoch 12/400, Batch 1901/4457, Loss: 1.1966766119003296\n",
      "Epoch 12/400, Batch 2001/4457, Loss: 1.1966853141784668\n",
      "Epoch 12/400, Batch 2101/4457, Loss: 1.2241827249526978\n",
      "Epoch 12/400, Batch 2201/4457, Loss: 1.1968601942062378\n",
      "Epoch 12/400, Batch 2301/4457, Loss: 1.2896196842193604\n",
      "Epoch 12/400, Batch 2401/4457, Loss: 1.2725826501846313\n",
      "Epoch 12/400, Batch 2501/4457, Loss: 1.1654253005981445\n",
      "Epoch 12/400, Batch 2601/4457, Loss: 1.196678876876831\n",
      "Epoch 12/400, Batch 2701/4457, Loss: 1.198095679283142\n",
      "Epoch 12/400, Batch 2801/4457, Loss: 1.1666700839996338\n",
      "Epoch 12/400, Batch 2901/4457, Loss: 1.1654226779937744\n",
      "Epoch 12/400, Batch 3001/4457, Loss: 1.1969717741012573\n",
      "Epoch 12/400, Batch 3101/4457, Loss: 1.1654224395751953\n",
      "Epoch 12/400, Batch 3201/4457, Loss: 1.185042381286621\n",
      "Epoch 12/400, Batch 3301/4457, Loss: 1.1924251317977905\n",
      "Epoch 12/400, Batch 3401/4457, Loss: 1.1891034841537476\n",
      "Epoch 12/400, Batch 3501/4457, Loss: 1.1663131713867188\n",
      "Epoch 12/400, Batch 3601/4457, Loss: 1.228096842765808\n",
      "Epoch 12/400, Batch 3701/4457, Loss: 1.186784029006958\n",
      "Epoch 12/400, Batch 3801/4457, Loss: 1.1654257774353027\n",
      "Epoch 12/400, Batch 3901/4457, Loss: 1.1918866634368896\n",
      "Epoch 12/400, Batch 4001/4457, Loss: 1.1966736316680908\n",
      "Epoch 12/400, Batch 4101/4457, Loss: 1.1980599164962769\n",
      "Epoch 12/400, Batch 4201/4457, Loss: 1.16659677028656\n",
      "Epoch 12/400, Batch 4301/4457, Loss: 1.1966760158538818\n",
      "Epoch 12/400, Batch 4401/4457, Loss: 1.227926254272461\n",
      "Epoch 12/400, Validation Loss: 1.3062030919014462\n",
      "Epoch 13/400, Batch 1/4457, Loss: 1.2576003074645996\n",
      "Epoch 13/400, Batch 101/4457, Loss: 1.2007092237472534\n",
      "Epoch 13/400, Batch 201/4457, Loss: 1.1966767311096191\n",
      "Epoch 13/400, Batch 301/4457, Loss: 1.228009581565857\n",
      "Epoch 13/400, Batch 401/4457, Loss: 1.1654226779937744\n",
      "Epoch 13/400, Batch 501/4457, Loss: 1.2594470977783203\n",
      "Epoch 13/400, Batch 601/4457, Loss: 1.1654248237609863\n",
      "Epoch 13/400, Batch 701/4457, Loss: 1.1966748237609863\n",
      "Epoch 13/400, Batch 801/4457, Loss: 1.1688815355300903\n",
      "Epoch 13/400, Batch 901/4457, Loss: 1.196675181388855\n",
      "Epoch 13/400, Batch 1001/4457, Loss: 1.259183645248413\n",
      "Epoch 13/400, Batch 1101/4457, Loss: 1.2541989088058472\n",
      "Epoch 13/400, Batch 1201/4457, Loss: 1.1966934204101562\n",
      "Epoch 13/400, Batch 1301/4457, Loss: 1.2530364990234375\n",
      "Epoch 13/400, Batch 1401/4457, Loss: 1.1966657638549805\n",
      "Epoch 13/400, Batch 1501/4457, Loss: 1.2902443408966064\n",
      "Epoch 13/400, Batch 1601/4457, Loss: 1.225432276725769\n",
      "Epoch 13/400, Batch 1701/4457, Loss: 1.249417781829834\n",
      "Epoch 13/400, Batch 1801/4457, Loss: 1.1968247890472412\n",
      "Epoch 13/400, Batch 1901/4457, Loss: 1.16543710231781\n",
      "Epoch 13/400, Batch 2001/4457, Loss: 1.1940593719482422\n",
      "Epoch 13/400, Batch 2101/4457, Loss: 1.2280489206314087\n",
      "Epoch 13/400, Batch 2201/4457, Loss: 1.1986968517303467\n",
      "Epoch 13/400, Batch 2301/4457, Loss: 1.2182040214538574\n",
      "Epoch 13/400, Batch 2401/4457, Loss: 1.1966718435287476\n",
      "Epoch 13/400, Batch 2501/4457, Loss: 1.2016921043395996\n",
      "Epoch 13/400, Batch 2601/4457, Loss: 1.1964855194091797\n",
      "Epoch 13/400, Batch 2701/4457, Loss: 1.1966724395751953\n",
      "Epoch 13/400, Batch 2801/4457, Loss: 1.2275800704956055\n",
      "Epoch 13/400, Batch 2901/4457, Loss: 1.1949741840362549\n",
      "Epoch 13/400, Batch 3001/4457, Loss: 1.1967623233795166\n",
      "Epoch 13/400, Batch 3101/4457, Loss: 1.2279229164123535\n",
      "Epoch 13/400, Batch 3201/4457, Loss: 1.2014116048812866\n",
      "Epoch 13/400, Batch 3301/4457, Loss: 1.1963374614715576\n",
      "Epoch 13/400, Batch 3401/4457, Loss: 1.1983261108398438\n",
      "Epoch 13/400, Batch 3501/4457, Loss: 1.1966729164123535\n",
      "Epoch 13/400, Batch 3601/4457, Loss: 1.1966731548309326\n",
      "Epoch 13/400, Batch 3701/4457, Loss: 1.1978223323822021\n",
      "Epoch 13/400, Batch 3801/4457, Loss: 1.1654298305511475\n",
      "Epoch 13/400, Batch 3901/4457, Loss: 1.1967284679412842\n",
      "Epoch 13/400, Batch 4001/4457, Loss: 1.2666735649108887\n",
      "Epoch 13/400, Batch 4101/4457, Loss: 1.1654877662658691\n",
      "Epoch 13/400, Batch 4201/4457, Loss: 1.1966800689697266\n",
      "Epoch 13/400, Batch 4301/4457, Loss: 1.1965571641921997\n",
      "Epoch 13/400, Batch 4401/4457, Loss: 1.228464126586914\n",
      "Epoch 13/400, Validation Loss: 1.285714562804926\n",
      "Epoch 14/400, Batch 1/4457, Loss: 1.1967768669128418\n",
      "Epoch 14/400, Batch 101/4457, Loss: 1.242444396018982\n",
      "Epoch 14/400, Batch 201/4457, Loss: 1.227931022644043\n",
      "Epoch 14/400, Batch 301/4457, Loss: 1.16610586643219\n",
      "Epoch 14/400, Batch 401/4457, Loss: 1.2209396362304688\n",
      "Epoch 14/400, Batch 501/4457, Loss: 1.165487289428711\n",
      "Epoch 14/400, Batch 601/4457, Loss: 1.165432333946228\n",
      "Epoch 14/400, Batch 701/4457, Loss: 1.1978744268417358\n",
      "Epoch 14/400, Batch 801/4457, Loss: 1.22792387008667\n",
      "Epoch 14/400, Batch 901/4457, Loss: 1.1966862678527832\n",
      "Epoch 14/400, Batch 1001/4457, Loss: 1.1654274463653564\n",
      "Epoch 14/400, Batch 1101/4457, Loss: 1.165429711341858\n",
      "Epoch 14/400, Batch 1201/4457, Loss: 1.1834461688995361\n",
      "Epoch 14/400, Batch 1301/4457, Loss: 1.1657787561416626\n",
      "Epoch 14/400, Batch 1401/4457, Loss: 1.2599594593048096\n",
      "Epoch 14/400, Batch 1501/4457, Loss: 1.1718039512634277\n",
      "Epoch 14/400, Batch 1601/4457, Loss: 1.2621084451675415\n",
      "Epoch 14/400, Batch 1701/4457, Loss: 1.1654233932495117\n",
      "Epoch 14/400, Batch 1801/4457, Loss: 1.1654266119003296\n",
      "Epoch 14/400, Batch 1901/4457, Loss: 1.1654231548309326\n",
      "Epoch 14/400, Batch 2001/4457, Loss: 1.2280125617980957\n",
      "Epoch 14/400, Batch 2101/4457, Loss: 1.1966733932495117\n",
      "Epoch 14/400, Batch 2201/4457, Loss: 1.2281289100646973\n",
      "Epoch 14/400, Batch 2301/4457, Loss: 1.2279281616210938\n",
      "Epoch 14/400, Batch 2401/4457, Loss: 1.165444016456604\n",
      "Epoch 14/400, Batch 2501/4457, Loss: 1.1968302726745605\n",
      "Epoch 14/400, Batch 2601/4457, Loss: 1.1654324531555176\n",
      "Epoch 14/400, Batch 2701/4457, Loss: 1.2289063930511475\n",
      "Epoch 14/400, Batch 2801/4457, Loss: 1.1966737508773804\n",
      "Epoch 14/400, Batch 2901/4457, Loss: 1.1654225587844849\n",
      "Epoch 14/400, Batch 3001/4457, Loss: 1.165715217590332\n",
      "Epoch 14/400, Batch 3101/4457, Loss: 1.196674108505249\n",
      "Epoch 14/400, Batch 3201/4457, Loss: 1.212929368019104\n",
      "Epoch 14/400, Batch 3301/4457, Loss: 1.1966744661331177\n",
      "Epoch 14/400, Batch 3401/4457, Loss: 1.165492057800293\n",
      "Epoch 14/400, Batch 3501/4457, Loss: 1.1654260158538818\n",
      "Epoch 14/400, Batch 3601/4457, Loss: 1.1999361515045166\n",
      "Epoch 14/400, Batch 3701/4457, Loss: 1.1961658000946045\n",
      "Epoch 14/400, Batch 3801/4457, Loss: 1.2279274463653564\n",
      "Epoch 14/400, Batch 3901/4457, Loss: 1.1654419898986816\n",
      "Epoch 14/400, Batch 4001/4457, Loss: 1.2279587984085083\n",
      "Epoch 14/400, Batch 4101/4457, Loss: 1.1975016593933105\n",
      "Epoch 14/400, Batch 4201/4457, Loss: 1.1654250621795654\n",
      "Epoch 14/400, Batch 4301/4457, Loss: 1.1967155933380127\n",
      "Epoch 14/400, Batch 4401/4457, Loss: 1.2591757774353027\n",
      "Epoch 14/400, Validation Loss: 1.2928971055717695\n",
      "Epoch 15/400, Batch 1/4457, Loss: 1.1655534505844116\n",
      "Epoch 15/400, Batch 101/4457, Loss: 1.22796630859375\n",
      "Epoch 15/400, Batch 201/4457, Loss: 1.1654295921325684\n",
      "Epoch 15/400, Batch 301/4457, Loss: 1.1654324531555176\n",
      "Epoch 15/400, Batch 401/4457, Loss: 1.2279186248779297\n",
      "Epoch 15/400, Batch 501/4457, Loss: 1.2279285192489624\n",
      "Epoch 15/400, Batch 601/4457, Loss: 1.1702216863632202\n",
      "Epoch 15/400, Batch 701/4457, Loss: 1.2165591716766357\n",
      "Epoch 15/400, Batch 801/4457, Loss: 1.2261455059051514\n",
      "Epoch 15/400, Batch 901/4457, Loss: 1.1966720819473267\n",
      "Epoch 15/400, Batch 1001/4457, Loss: 1.165424108505249\n",
      "Epoch 15/400, Batch 1101/4457, Loss: 1.1654237508773804\n",
      "Epoch 15/400, Batch 1201/4457, Loss: 1.2051281929016113\n",
      "Epoch 15/400, Batch 1301/4457, Loss: 1.196674108505249\n",
      "Epoch 15/400, Batch 1401/4457, Loss: 1.165743350982666\n",
      "Epoch 15/400, Batch 1501/4457, Loss: 1.1654715538024902\n",
      "Epoch 15/400, Batch 1601/4457, Loss: 1.1692276000976562\n",
      "Epoch 15/400, Batch 1701/4457, Loss: 1.1654225587844849\n",
      "Epoch 15/400, Batch 1801/4457, Loss: 1.1966724395751953\n",
      "Epoch 15/400, Batch 1901/4457, Loss: 1.1967017650604248\n",
      "Epoch 15/400, Batch 2001/4457, Loss: 1.171592116355896\n",
      "Epoch 15/400, Batch 2101/4457, Loss: 1.196675181388855\n",
      "Epoch 15/400, Batch 2201/4457, Loss: 1.1654236316680908\n",
      "Epoch 15/400, Batch 2301/4457, Loss: 1.1654231548309326\n",
      "Epoch 15/400, Batch 2401/4457, Loss: 1.1966755390167236\n",
      "Epoch 15/400, Batch 2501/4457, Loss: 1.1966779232025146\n",
      "Epoch 15/400, Batch 2601/4457, Loss: 1.1654229164123535\n",
      "Epoch 15/400, Batch 2701/4457, Loss: 1.1966733932495117\n",
      "Epoch 15/400, Batch 2801/4457, Loss: 1.196869969367981\n",
      "Epoch 15/400, Batch 2901/4457, Loss: 1.2564830780029297\n",
      "Epoch 15/400, Batch 3001/4457, Loss: 1.1976208686828613\n",
      "Epoch 15/400, Batch 3101/4457, Loss: 1.1985106468200684\n",
      "Epoch 15/400, Batch 3201/4457, Loss: 1.1966856718063354\n",
      "Epoch 15/400, Batch 3301/4457, Loss: 1.2591724395751953\n",
      "Epoch 15/400, Batch 3401/4457, Loss: 1.1654233932495117\n",
      "Epoch 15/400, Batch 3501/4457, Loss: 1.1657679080963135\n",
      "Epoch 15/400, Batch 3601/4457, Loss: 1.1655093431472778\n",
      "Epoch 15/400, Batch 3701/4457, Loss: 1.1655871868133545\n",
      "Epoch 15/400, Batch 3801/4457, Loss: 1.196674108505249\n",
      "Epoch 15/400, Batch 3901/4457, Loss: 1.2276408672332764\n",
      "Epoch 15/400, Batch 4001/4457, Loss: 1.1654229164123535\n",
      "Epoch 15/400, Batch 4101/4457, Loss: 1.1654287576675415\n",
      "Epoch 15/400, Batch 4201/4457, Loss: 1.2279235124588013\n",
      "Epoch 15/400, Batch 4301/4457, Loss: 1.1967124938964844\n",
      "Epoch 15/400, Batch 4401/4457, Loss: 1.186862587928772\n",
      "Epoch 15/400, Validation Loss: 1.2916440876230362\n",
      "Epoch 16/400, Batch 1/4457, Loss: 1.227854609489441\n",
      "Epoch 16/400, Batch 101/4457, Loss: 1.2239227294921875\n",
      "Epoch 16/400, Batch 201/4457, Loss: 1.1667442321777344\n",
      "Epoch 16/400, Batch 301/4457, Loss: 1.2911322116851807\n",
      "Epoch 16/400, Batch 401/4457, Loss: 1.2279179096221924\n",
      "Epoch 16/400, Batch 501/4457, Loss: 1.2592111825942993\n",
      "Epoch 16/400, Batch 601/4457, Loss: 1.196673035621643\n",
      "Epoch 16/400, Batch 701/4457, Loss: 1.1654243469238281\n",
      "Epoch 16/400, Batch 801/4457, Loss: 1.196672797203064\n",
      "Epoch 16/400, Batch 901/4457, Loss: 1.1654243469238281\n",
      "Epoch 16/400, Batch 1001/4457, Loss: 1.1754218339920044\n",
      "Epoch 16/400, Batch 1101/4457, Loss: 1.16542387008667\n",
      "Epoch 16/400, Batch 1201/4457, Loss: 1.1973589658737183\n",
      "Epoch 16/400, Batch 1301/4457, Loss: 1.2279223203659058\n",
      "Epoch 16/400, Batch 1401/4457, Loss: 1.1654276847839355\n",
      "Epoch 16/400, Batch 1501/4457, Loss: 1.210975170135498\n",
      "Epoch 16/400, Batch 1601/4457, Loss: 1.1967413425445557\n",
      "Epoch 16/400, Batch 1701/4457, Loss: 1.2792203426361084\n",
      "Epoch 16/400, Batch 1801/4457, Loss: 1.1968599557876587\n",
      "Epoch 16/400, Batch 1901/4457, Loss: 1.16542387008667\n",
      "Epoch 16/400, Batch 2001/4457, Loss: 1.1966725587844849\n",
      "Epoch 16/400, Batch 2101/4457, Loss: 1.167331576347351\n",
      "Epoch 16/400, Batch 2201/4457, Loss: 1.228010654449463\n",
      "Epoch 16/400, Batch 2301/4457, Loss: 1.1663093566894531\n",
      "Epoch 16/400, Batch 2401/4457, Loss: 1.1654235124588013\n",
      "Epoch 16/400, Batch 2501/4457, Loss: 1.1655340194702148\n",
      "Epoch 16/400, Batch 2601/4457, Loss: 1.1654515266418457\n",
      "Epoch 16/400, Batch 2701/4457, Loss: 1.1922118663787842\n",
      "Epoch 16/400, Batch 2801/4457, Loss: 1.19667387008667\n",
      "Epoch 16/400, Batch 2901/4457, Loss: 1.1968121528625488\n",
      "Epoch 16/400, Batch 3001/4457, Loss: 1.1967318058013916\n",
      "Epoch 16/400, Batch 3101/4457, Loss: 1.1964871883392334\n",
      "Epoch 16/400, Batch 3201/4457, Loss: 1.16542387008667\n",
      "Epoch 16/400, Batch 3301/4457, Loss: 1.2247580289840698\n",
      "Epoch 16/400, Batch 3401/4457, Loss: 1.2279449701309204\n",
      "Epoch 16/400, Batch 3501/4457, Loss: 1.1657263040542603\n",
      "Epoch 16/400, Batch 3601/4457, Loss: 1.2590510845184326\n",
      "Epoch 16/400, Batch 3701/4457, Loss: 1.1967096328735352\n",
      "Epoch 16/400, Batch 3801/4457, Loss: 1.2279248237609863\n",
      "Epoch 16/400, Batch 3901/4457, Loss: 1.1661345958709717\n",
      "Epoch 16/400, Batch 4001/4457, Loss: 1.1654253005981445\n",
      "Epoch 16/400, Batch 4101/4457, Loss: 1.1654298305511475\n",
      "Epoch 16/400, Batch 4201/4457, Loss: 1.165498971939087\n",
      "Epoch 16/400, Batch 4301/4457, Loss: 1.1654338836669922\n",
      "Epoch 16/400, Batch 4401/4457, Loss: 1.3216183185577393\n",
      "Epoch 16/400, Validation Loss: 1.3033264263991327\n",
      "Epoch 17/400, Batch 1/4457, Loss: 1.196697473526001\n",
      "Epoch 17/400, Batch 101/4457, Loss: 1.2017874717712402\n",
      "Epoch 17/400, Batch 201/4457, Loss: 1.1975865364074707\n",
      "Epoch 17/400, Batch 301/4457, Loss: 1.1966724395751953\n",
      "Epoch 17/400, Batch 401/4457, Loss: 1.197493553161621\n",
      "Epoch 17/400, Batch 501/4457, Loss: 1.1654367446899414\n",
      "Epoch 17/400, Batch 601/4457, Loss: 1.1654233932495117\n",
      "Epoch 17/400, Batch 701/4457, Loss: 1.1654229164123535\n",
      "Epoch 17/400, Batch 801/4457, Loss: 1.2279225587844849\n",
      "Epoch 17/400, Batch 901/4457, Loss: 1.1966736316680908\n",
      "Epoch 17/400, Batch 1001/4457, Loss: 1.1654884815216064\n",
      "Epoch 17/400, Batch 1101/4457, Loss: 1.1966893672943115\n",
      "Epoch 17/400, Batch 1201/4457, Loss: 1.166380763053894\n",
      "Epoch 17/400, Batch 1301/4457, Loss: 1.2591726779937744\n",
      "Epoch 17/400, Batch 1401/4457, Loss: 1.2057552337646484\n",
      "Epoch 17/400, Batch 1501/4457, Loss: 1.227922797203064\n",
      "Epoch 17/400, Batch 1601/4457, Loss: 1.2591910362243652\n",
      "Epoch 17/400, Batch 1701/4457, Loss: 1.2217048406600952\n",
      "Epoch 17/400, Batch 1801/4457, Loss: 1.1654225587844849\n",
      "Epoch 17/400, Batch 1901/4457, Loss: 1.196638822555542\n",
      "Epoch 17/400, Batch 2001/4457, Loss: 1.1654300689697266\n",
      "Epoch 17/400, Batch 2101/4457, Loss: 1.2279224395751953\n",
      "Epoch 17/400, Batch 2201/4457, Loss: 1.1654226779937744\n",
      "Epoch 17/400, Batch 2301/4457, Loss: 1.1730738878250122\n",
      "Epoch 17/400, Batch 2401/4457, Loss: 1.2590081691741943\n",
      "Epoch 17/400, Batch 2501/4457, Loss: 1.1654226779937744\n",
      "Epoch 17/400, Batch 2601/4457, Loss: 1.165504813194275\n",
      "Epoch 17/400, Batch 2701/4457, Loss: 1.1654226779937744\n",
      "Epoch 17/400, Batch 2801/4457, Loss: 1.1654499769210815\n",
      "Epoch 17/400, Batch 2901/4457, Loss: 1.2279237508773804\n",
      "Epoch 17/400, Batch 3001/4457, Loss: 1.1654374599456787\n",
      "Epoch 17/400, Batch 3101/4457, Loss: 1.1654223203659058\n",
      "Epoch 17/400, Batch 3201/4457, Loss: 1.228685975074768\n",
      "Epoch 17/400, Batch 3301/4457, Loss: 1.1736533641815186\n",
      "Epoch 17/400, Batch 3401/4457, Loss: 1.1969552040100098\n",
      "Epoch 17/400, Batch 3501/4457, Loss: 1.16594398021698\n",
      "Epoch 17/400, Batch 3601/4457, Loss: 1.290744662284851\n",
      "Epoch 17/400, Batch 3701/4457, Loss: 1.1990182399749756\n",
      "Epoch 17/400, Batch 3801/4457, Loss: 1.1654384136199951\n",
      "Epoch 17/400, Batch 3901/4457, Loss: 1.1751751899719238\n",
      "Epoch 17/400, Batch 4001/4457, Loss: 1.165426254272461\n",
      "Epoch 17/400, Batch 4101/4457, Loss: 1.1654269695281982\n",
      "Epoch 17/400, Batch 4201/4457, Loss: 1.1654269695281982\n",
      "Epoch 17/400, Batch 4301/4457, Loss: 1.2592096328735352\n",
      "Epoch 17/400, Batch 4401/4457, Loss: 1.196673035621643\n",
      "Epoch 17/400, Validation Loss: 1.2983586691201678\n",
      "Epoch 18/400, Batch 1/4457, Loss: 1.1966742277145386\n",
      "Epoch 18/400, Batch 101/4457, Loss: 1.1966853141784668\n",
      "Epoch 18/400, Batch 201/4457, Loss: 1.2591811418533325\n",
      "Epoch 18/400, Batch 301/4457, Loss: 1.196676254272461\n",
      "Epoch 18/400, Batch 401/4457, Loss: 1.2430577278137207\n",
      "Epoch 18/400, Batch 501/4457, Loss: 1.1970036029815674\n",
      "Epoch 18/400, Batch 601/4457, Loss: 1.1911990642547607\n",
      "Epoch 18/400, Batch 701/4457, Loss: 1.1654373407363892\n",
      "Epoch 18/400, Batch 801/4457, Loss: 1.1966725587844849\n",
      "Epoch 18/400, Batch 901/4457, Loss: 1.228383183479309\n",
      "Epoch 18/400, Batch 1001/4457, Loss: 1.1654226779937744\n",
      "Epoch 18/400, Batch 1101/4457, Loss: 1.1654257774353027\n",
      "Epoch 18/400, Batch 1201/4457, Loss: 1.258752465248108\n",
      "Epoch 18/400, Batch 1301/4457, Loss: 1.1654229164123535\n",
      "Epoch 18/400, Batch 1401/4457, Loss: 1.1654293537139893\n",
      "Epoch 18/400, Batch 1501/4457, Loss: 1.1654345989227295\n",
      "Epoch 18/400, Batch 1601/4457, Loss: 1.16736900806427\n",
      "Epoch 18/400, Batch 1701/4457, Loss: 1.198379397392273\n",
      "Epoch 18/400, Batch 1801/4457, Loss: 1.1966731548309326\n",
      "Epoch 18/400, Batch 1901/4457, Loss: 1.2279331684112549\n",
      "Epoch 18/400, Batch 2001/4457, Loss: 1.2279211282730103\n",
      "Epoch 18/400, Batch 2101/4457, Loss: 1.1654973030090332\n",
      "Epoch 18/400, Batch 2201/4457, Loss: 1.227925181388855\n",
      "Epoch 18/400, Batch 2301/4457, Loss: 1.1654229164123535\n",
      "Epoch 18/400, Batch 2401/4457, Loss: 1.196681261062622\n",
      "Epoch 18/400, Batch 2501/4457, Loss: 1.1966722011566162\n",
      "Epoch 18/400, Batch 2601/4457, Loss: 1.1654223203659058\n",
      "Epoch 18/400, Batch 2701/4457, Loss: 1.1654233932495117\n",
      "Epoch 18/400, Batch 2801/4457, Loss: 1.1966726779937744\n",
      "Epoch 18/400, Batch 2901/4457, Loss: 1.1654225587844849\n",
      "Epoch 18/400, Batch 3001/4457, Loss: 1.2279224395751953\n",
      "Epoch 18/400, Batch 3101/4457, Loss: 1.196676254272461\n",
      "Epoch 18/400, Batch 3201/4457, Loss: 1.2537562847137451\n",
      "Epoch 18/400, Batch 3301/4457, Loss: 1.1966884136199951\n",
      "Epoch 18/400, Batch 3401/4457, Loss: 1.165424108505249\n",
      "Epoch 18/400, Batch 3501/4457, Loss: 1.165616750717163\n",
      "Epoch 18/400, Batch 3601/4457, Loss: 1.1966726779937744\n",
      "Epoch 18/400, Batch 3701/4457, Loss: 1.1966724395751953\n",
      "Epoch 18/400, Batch 3801/4457, Loss: 1.1966760158538818\n",
      "Epoch 18/400, Batch 3901/4457, Loss: 1.1967538595199585\n",
      "Epoch 18/400, Batch 4001/4457, Loss: 1.1966724395751953\n",
      "Epoch 18/400, Batch 4101/4457, Loss: 1.2904211282730103\n",
      "Epoch 18/400, Batch 4201/4457, Loss: 1.165428876876831\n",
      "Epoch 18/400, Batch 4301/4457, Loss: 1.1966936588287354\n",
      "Epoch 18/400, Batch 4401/4457, Loss: 1.228175401687622\n",
      "Epoch 18/400, Validation Loss: 1.291495653845015\n",
      "Epoch 19/400, Batch 1/4457, Loss: 1.1654224395751953\n",
      "Epoch 19/400, Batch 101/4457, Loss: 1.1654224395751953\n",
      "Epoch 19/400, Batch 201/4457, Loss: 1.22820246219635\n",
      "Epoch 19/400, Batch 301/4457, Loss: 1.2276840209960938\n",
      "Epoch 19/400, Batch 401/4457, Loss: 1.1983786821365356\n",
      "Epoch 19/400, Batch 501/4457, Loss: 1.1966724395751953\n",
      "Epoch 19/400, Batch 601/4457, Loss: 1.1953125\n",
      "Epoch 19/400, Batch 701/4457, Loss: 1.1654236316680908\n",
      "Epoch 19/400, Batch 801/4457, Loss: 1.1654988527297974\n",
      "Epoch 19/400, Batch 901/4457, Loss: 1.227908968925476\n",
      "Epoch 19/400, Batch 1001/4457, Loss: 1.1966747045516968\n",
      "Epoch 19/400, Batch 1101/4457, Loss: 1.1654236316680908\n",
      "Epoch 19/400, Batch 1201/4457, Loss: 1.1967270374298096\n",
      "Epoch 19/400, Batch 1301/4457, Loss: 1.1966900825500488\n",
      "Epoch 19/400, Batch 1401/4457, Loss: 1.2710016965866089\n",
      "Epoch 19/400, Batch 1501/4457, Loss: 1.1966946125030518\n",
      "Epoch 19/400, Batch 1601/4457, Loss: 1.1654620170593262\n",
      "Epoch 19/400, Batch 1701/4457, Loss: 1.1654345989227295\n",
      "Epoch 19/400, Batch 1801/4457, Loss: 1.1966748237609863\n",
      "Epoch 19/400, Batch 1901/4457, Loss: 1.196772813796997\n",
      "Epoch 19/400, Batch 2001/4457, Loss: 1.2591736316680908\n",
      "Epoch 19/400, Batch 2101/4457, Loss: 1.1654245853424072\n",
      "Epoch 19/400, Batch 2201/4457, Loss: 1.2904229164123535\n",
      "Epoch 19/400, Batch 2301/4457, Loss: 1.1654248237609863\n",
      "Epoch 19/400, Batch 2401/4457, Loss: 1.1654257774353027\n",
      "Epoch 19/400, Batch 2501/4457, Loss: 1.1654260158538818\n",
      "Epoch 19/400, Batch 2601/4457, Loss: 1.2279239892959595\n",
      "Epoch 19/400, Batch 2701/4457, Loss: 1.1654233932495117\n",
      "Epoch 19/400, Batch 2801/4457, Loss: 1.165422797203064\n",
      "Epoch 19/400, Batch 2901/4457, Loss: 1.1966726779937744\n",
      "Epoch 19/400, Batch 3001/4457, Loss: 1.1654739379882812\n",
      "Epoch 19/400, Batch 3101/4457, Loss: 1.1656081676483154\n",
      "Epoch 19/400, Batch 3201/4457, Loss: 1.1966745853424072\n",
      "Epoch 19/400, Batch 3301/4457, Loss: 1.2279222011566162\n",
      "Epoch 19/400, Batch 3401/4457, Loss: 1.1966724395751953\n",
      "Epoch 19/400, Batch 3501/4457, Loss: 1.1654285192489624\n",
      "Epoch 19/400, Batch 3601/4457, Loss: 1.1654231548309326\n",
      "Epoch 19/400, Batch 3701/4457, Loss: 1.1655231714248657\n",
      "Epoch 19/400, Batch 3801/4457, Loss: 1.1750590801239014\n",
      "Epoch 19/400, Batch 3901/4457, Loss: 1.2279226779937744\n",
      "Epoch 19/400, Batch 4001/4457, Loss: 1.2256653308868408\n",
      "Epoch 19/400, Batch 4101/4457, Loss: 1.165440559387207\n",
      "Epoch 19/400, Batch 4201/4457, Loss: 1.2593342065811157\n",
      "Epoch 19/400, Batch 4301/4457, Loss: 1.1720741987228394\n",
      "Epoch 19/400, Batch 4401/4457, Loss: 1.2279274463653564\n",
      "Epoch 19/400, Validation Loss: 1.2827578136135662\n",
      "Epoch 20/400, Batch 1/4457, Loss: 1.1966911554336548\n",
      "Epoch 20/400, Batch 101/4457, Loss: 1.227923035621643\n",
      "Epoch 20/400, Batch 201/4457, Loss: 1.2279257774353027\n",
      "Epoch 20/400, Batch 301/4457, Loss: 1.196852207183838\n",
      "Epoch 20/400, Batch 401/4457, Loss: 1.196690559387207\n",
      "Epoch 20/400, Batch 501/4457, Loss: 1.1654242277145386\n",
      "Epoch 20/400, Batch 601/4457, Loss: 1.1964771747589111\n",
      "Epoch 20/400, Batch 701/4457, Loss: 1.1654229164123535\n",
      "Epoch 20/400, Batch 801/4457, Loss: 1.1654224395751953\n",
      "Epoch 20/400, Batch 901/4457, Loss: 1.1654224395751953\n",
      "Epoch 20/400, Batch 1001/4457, Loss: 1.1966723203659058\n",
      "Epoch 20/400, Batch 1101/4457, Loss: 1.1834626197814941\n",
      "Epoch 20/400, Batch 1201/4457, Loss: 1.1654224395751953\n",
      "Epoch 20/400, Batch 1301/4457, Loss: 1.1966732740402222\n",
      "Epoch 20/400, Batch 1401/4457, Loss: 1.1654225587844849\n",
      "Epoch 20/400, Batch 1501/4457, Loss: 1.1966745853424072\n",
      "Epoch 20/400, Batch 1601/4457, Loss: 1.2279226779937744\n",
      "Epoch 20/400, Batch 1701/4457, Loss: 1.1966729164123535\n",
      "Epoch 20/400, Batch 1801/4457, Loss: 1.1654226779937744\n",
      "Epoch 20/400, Batch 1901/4457, Loss: 1.1966862678527832\n",
      "Epoch 20/400, Batch 2001/4457, Loss: 1.1654267311096191\n",
      "Epoch 20/400, Batch 2101/4457, Loss: 1.1966722011566162\n",
      "Epoch 20/400, Batch 2201/4457, Loss: 1.1654225587844849\n",
      "Epoch 20/400, Batch 2301/4457, Loss: 1.1967856884002686\n",
      "Epoch 20/400, Batch 2401/4457, Loss: 1.1654224395751953\n",
      "Epoch 20/400, Batch 2501/4457, Loss: 1.1654261350631714\n",
      "Epoch 20/400, Batch 2601/4457, Loss: 1.1654231548309326\n",
      "Epoch 20/400, Batch 2701/4457, Loss: 1.2279399633407593\n",
      "Epoch 20/400, Batch 2801/4457, Loss: 1.1654282808303833\n",
      "Epoch 20/400, Batch 2901/4457, Loss: 1.1654287576675415\n",
      "Epoch 20/400, Batch 3001/4457, Loss: 1.2183940410614014\n",
      "Epoch 20/400, Batch 3101/4457, Loss: 1.1654242277145386\n",
      "Epoch 20/400, Batch 3201/4457, Loss: 1.1654226779937744\n",
      "Epoch 20/400, Batch 3301/4457, Loss: 1.1654506921768188\n",
      "Epoch 20/400, Batch 3401/4457, Loss: 1.1966724395751953\n",
      "Epoch 20/400, Batch 3501/4457, Loss: 1.1966743469238281\n",
      "Epoch 20/400, Batch 3601/4457, Loss: 1.1812649965286255\n",
      "Epoch 20/400, Batch 3701/4457, Loss: 1.1970750093460083\n",
      "Epoch 20/400, Batch 3801/4457, Loss: 1.1654223203659058\n",
      "Epoch 20/400, Batch 3901/4457, Loss: 1.1654818058013916\n",
      "Epoch 20/400, Batch 4001/4457, Loss: 1.16542387008667\n",
      "Epoch 20/400, Batch 4101/4457, Loss: 1.196672797203064\n",
      "Epoch 20/400, Batch 4201/4457, Loss: 1.1966760158538818\n",
      "Epoch 20/400, Batch 4301/4457, Loss: 1.1966811418533325\n",
      "Epoch 20/400, Batch 4401/4457, Loss: 1.2314058542251587\n",
      "Epoch 20/400, Validation Loss: 1.2918613639379304\n",
      "Epoch 21/400, Batch 1/4457, Loss: 1.165569543838501\n",
      "Epoch 21/400, Batch 101/4457, Loss: 1.16542387008667\n",
      "Epoch 21/400, Batch 201/4457, Loss: 1.227787733078003\n",
      "Epoch 21/400, Batch 301/4457, Loss: 1.1953487396240234\n",
      "Epoch 21/400, Batch 401/4457, Loss: 1.1654527187347412\n",
      "Epoch 21/400, Batch 501/4457, Loss: 1.2329622507095337\n",
      "Epoch 21/400, Batch 601/4457, Loss: 1.1654224395751953\n",
      "Epoch 21/400, Batch 701/4457, Loss: 1.165431022644043\n",
      "Epoch 21/400, Batch 801/4457, Loss: 1.165435791015625\n",
      "Epoch 21/400, Batch 901/4457, Loss: 1.1654268503189087\n",
      "Epoch 21/400, Batch 1001/4457, Loss: 1.1966782808303833\n",
      "Epoch 21/400, Batch 1101/4457, Loss: 1.16542649269104\n",
      "Epoch 21/400, Batch 1201/4457, Loss: 1.281806230545044\n",
      "Epoch 21/400, Batch 1301/4457, Loss: 1.2279325723648071\n",
      "Epoch 21/400, Batch 1401/4457, Loss: 1.1654236316680908\n",
      "Epoch 21/400, Batch 1501/4457, Loss: 1.1966733932495117\n",
      "Epoch 21/400, Batch 1601/4457, Loss: 1.1654250621795654\n",
      "Epoch 21/400, Batch 1701/4457, Loss: 1.1966533660888672\n",
      "Epoch 21/400, Batch 1801/4457, Loss: 1.1958740949630737\n",
      "Epoch 21/400, Batch 1901/4457, Loss: 1.1966724395751953\n",
      "Epoch 21/400, Batch 2001/4457, Loss: 1.1966731548309326\n",
      "Epoch 21/400, Batch 2101/4457, Loss: 1.1654233932495117\n",
      "Epoch 21/400, Batch 2201/4457, Loss: 1.1654285192489624\n",
      "Epoch 21/400, Batch 2301/4457, Loss: 1.1654226779937744\n",
      "Epoch 21/400, Batch 2401/4457, Loss: 1.1969501972198486\n",
      "Epoch 21/400, Batch 2501/4457, Loss: 1.1970188617706299\n",
      "Epoch 21/400, Batch 2601/4457, Loss: 1.2591722011566162\n",
      "Epoch 21/400, Batch 2701/4457, Loss: 1.1967008113861084\n",
      "Epoch 21/400, Batch 2801/4457, Loss: 1.2279338836669922\n",
      "Epoch 21/400, Batch 2901/4457, Loss: 1.196671485900879\n",
      "Epoch 21/400, Batch 3001/4457, Loss: 1.2279226779937744\n",
      "Epoch 21/400, Batch 3101/4457, Loss: 1.1966756582260132\n",
      "Epoch 21/400, Batch 3201/4457, Loss: 1.1654232740402222\n",
      "Epoch 21/400, Batch 3301/4457, Loss: 1.1966755390167236\n",
      "Epoch 21/400, Batch 3401/4457, Loss: 1.1654536724090576\n",
      "Epoch 21/400, Batch 3501/4457, Loss: 1.2291412353515625\n",
      "Epoch 21/400, Batch 3601/4457, Loss: 1.3216722011566162\n",
      "Epoch 21/400, Batch 3701/4457, Loss: 1.2059130668640137\n",
      "Epoch 21/400, Batch 3801/4457, Loss: 1.227918267250061\n",
      "Epoch 21/400, Batch 3901/4457, Loss: 1.1966729164123535\n",
      "Epoch 21/400, Batch 4001/4457, Loss: 1.196784496307373\n",
      "Epoch 21/400, Batch 4101/4457, Loss: 1.2541086673736572\n",
      "Epoch 21/400, Batch 4201/4457, Loss: 1.1654239892959595\n",
      "Epoch 21/400, Batch 4301/4457, Loss: 1.1966726779937744\n",
      "Epoch 21/400, Batch 4401/4457, Loss: 1.1983251571655273\n",
      "Epoch 21/400, Validation Loss: 1.2908850121829245\n",
      "Epoch 22/400, Batch 1/4457, Loss: 1.1966500282287598\n",
      "Epoch 22/400, Batch 101/4457, Loss: 1.165431022644043\n",
      "Epoch 22/400, Batch 201/4457, Loss: 1.196678638458252\n",
      "Epoch 22/400, Batch 301/4457, Loss: 1.1966698169708252\n",
      "Epoch 22/400, Batch 401/4457, Loss: 1.165450096130371\n",
      "Epoch 22/400, Batch 501/4457, Loss: 1.196286916732788\n",
      "Epoch 22/400, Batch 601/4457, Loss: 1.1966732740402222\n",
      "Epoch 22/400, Batch 701/4457, Loss: 1.1654248237609863\n",
      "Epoch 22/400, Batch 801/4457, Loss: 1.1966755390167236\n",
      "Epoch 22/400, Batch 901/4457, Loss: 1.1662424802780151\n",
      "Epoch 22/400, Batch 1001/4457, Loss: 1.1963552236557007\n",
      "Epoch 22/400, Batch 1101/4457, Loss: 1.1654279232025146\n",
      "Epoch 22/400, Batch 1201/4457, Loss: 1.1966595649719238\n",
      "Epoch 22/400, Batch 1301/4457, Loss: 1.165435791015625\n",
      "Epoch 22/400, Batch 1401/4457, Loss: 1.196677803993225\n",
      "Epoch 22/400, Batch 1501/4457, Loss: 1.1654620170593262\n",
      "Epoch 22/400, Batch 1601/4457, Loss: 1.196672797203064\n",
      "Epoch 22/400, Batch 1701/4457, Loss: 1.1966723203659058\n",
      "Epoch 22/400, Batch 1801/4457, Loss: 1.196672797203064\n",
      "Epoch 22/400, Batch 1901/4457, Loss: 1.2279244661331177\n",
      "Epoch 22/400, Batch 2001/4457, Loss: 1.1966735124588013\n",
      "Epoch 22/400, Batch 2101/4457, Loss: 1.1654382944107056\n",
      "Epoch 22/400, Batch 2201/4457, Loss: 1.1654233932495117\n",
      "Epoch 22/400, Batch 2301/4457, Loss: 1.1654245853424072\n",
      "Epoch 22/400, Batch 2401/4457, Loss: 1.2279229164123535\n",
      "Epoch 22/400, Batch 2501/4457, Loss: 1.1654255390167236\n",
      "Epoch 22/400, Batch 2601/4457, Loss: 1.1654233932495117\n",
      "Epoch 22/400, Batch 2701/4457, Loss: 1.1654582023620605\n",
      "Epoch 22/400, Batch 2801/4457, Loss: 1.1655864715576172\n",
      "Epoch 22/400, Batch 2901/4457, Loss: 1.2010672092437744\n",
      "Epoch 22/400, Batch 3001/4457, Loss: 1.1684287786483765\n",
      "Epoch 22/400, Batch 3101/4457, Loss: 1.1654232740402222\n",
      "Epoch 22/400, Batch 3201/4457, Loss: 1.196758508682251\n",
      "Epoch 22/400, Batch 3301/4457, Loss: 1.228145956993103\n",
      "Epoch 22/400, Batch 3401/4457, Loss: 1.2591700553894043\n",
      "Epoch 22/400, Batch 3501/4457, Loss: 1.1654224395751953\n",
      "Epoch 22/400, Batch 3601/4457, Loss: 1.1972206830978394\n",
      "Epoch 22/400, Batch 3701/4457, Loss: 1.1654409170150757\n",
      "Epoch 22/400, Batch 3801/4457, Loss: 1.196674108505249\n",
      "Epoch 22/400, Batch 3901/4457, Loss: 1.2002859115600586\n",
      "Epoch 22/400, Batch 4001/4457, Loss: 1.1654225587844849\n",
      "Epoch 22/400, Batch 4101/4457, Loss: 1.1966757774353027\n",
      "Epoch 22/400, Batch 4201/4457, Loss: 1.1966761350631714\n",
      "Epoch 22/400, Batch 4301/4457, Loss: 1.1966729164123535\n",
      "Epoch 22/400, Batch 4401/4457, Loss: 1.195560097694397\n",
      "Epoch 22/400, Validation Loss: 1.2872488543627754\n",
      "Epoch 23/400, Batch 1/4457, Loss: 1.2591346502304077\n",
      "Epoch 23/400, Batch 101/4457, Loss: 1.1654354333877563\n",
      "Epoch 23/400, Batch 201/4457, Loss: 1.1981384754180908\n",
      "Epoch 23/400, Batch 301/4457, Loss: 1.1654624938964844\n",
      "Epoch 23/400, Batch 401/4457, Loss: 1.1966722011566162\n",
      "Epoch 23/400, Batch 501/4457, Loss: 1.1654222011566162\n",
      "Epoch 23/400, Batch 601/4457, Loss: 1.1966729164123535\n",
      "Epoch 23/400, Batch 701/4457, Loss: 1.2068243026733398\n",
      "Epoch 23/400, Batch 801/4457, Loss: 1.1654225587844849\n",
      "Epoch 23/400, Batch 901/4457, Loss: 1.1966743469238281\n",
      "Epoch 23/400, Batch 1001/4457, Loss: 1.1654237508773804\n",
      "Epoch 23/400, Batch 1101/4457, Loss: 1.1654223203659058\n",
      "Epoch 23/400, Batch 1201/4457, Loss: 1.1964805126190186\n",
      "Epoch 23/400, Batch 1301/4457, Loss: 1.1966742277145386\n",
      "Epoch 23/400, Batch 1401/4457, Loss: 1.1951119899749756\n",
      "Epoch 23/400, Batch 1501/4457, Loss: 1.1654226779937744\n",
      "Epoch 23/400, Batch 1601/4457, Loss: 1.1988170146942139\n",
      "Epoch 23/400, Batch 1701/4457, Loss: 1.1966729164123535\n",
      "Epoch 23/400, Batch 1801/4457, Loss: 1.1966722011566162\n",
      "Epoch 23/400, Batch 1901/4457, Loss: 1.1654378175735474\n",
      "Epoch 23/400, Batch 2001/4457, Loss: 1.197504997253418\n",
      "Epoch 23/400, Batch 2101/4457, Loss: 1.1654233932495117\n",
      "Epoch 23/400, Batch 2201/4457, Loss: 1.2382123470306396\n",
      "Epoch 23/400, Batch 2301/4457, Loss: 1.165605068206787\n",
      "Epoch 23/400, Batch 2401/4457, Loss: 1.1654305458068848\n",
      "Epoch 23/400, Batch 2501/4457, Loss: 1.1655559539794922\n",
      "Epoch 23/400, Batch 2601/4457, Loss: 1.1654692888259888\n",
      "Epoch 23/400, Batch 2701/4457, Loss: 1.1706764698028564\n",
      "Epoch 23/400, Batch 2801/4457, Loss: 1.1966725587844849\n",
      "Epoch 23/400, Batch 2901/4457, Loss: 1.1654229164123535\n",
      "Epoch 23/400, Batch 3001/4457, Loss: 1.1654226779937744\n",
      "Epoch 23/400, Batch 3101/4457, Loss: 1.1666643619537354\n",
      "Epoch 23/400, Batch 3201/4457, Loss: 1.1654222011566162\n",
      "Epoch 23/400, Batch 3301/4457, Loss: 1.1966724395751953\n",
      "Epoch 23/400, Batch 3401/4457, Loss: 1.1654224395751953\n",
      "Epoch 23/400, Batch 3501/4457, Loss: 1.1967062950134277\n",
      "Epoch 23/400, Batch 3601/4457, Loss: 1.1654224395751953\n",
      "Epoch 23/400, Batch 3701/4457, Loss: 1.1966750621795654\n",
      "Epoch 23/400, Batch 3801/4457, Loss: 1.1966724395751953\n",
      "Epoch 23/400, Batch 3901/4457, Loss: 1.2279224395751953\n",
      "Epoch 23/400, Batch 4001/4457, Loss: 1.1966853141784668\n",
      "Epoch 23/400, Batch 4101/4457, Loss: 1.196677803993225\n",
      "Epoch 23/400, Batch 4201/4457, Loss: 1.2279243469238281\n",
      "Epoch 23/400, Batch 4301/4457, Loss: 1.1654274463653564\n",
      "Epoch 23/400, Batch 4401/4457, Loss: 1.2280504703521729\n",
      "Epoch 23/400, Validation Loss: 1.2864920728736453\n",
      "Epoch 24/400, Batch 1/4457, Loss: 1.1968541145324707\n",
      "Epoch 24/400, Batch 101/4457, Loss: 1.1655107736587524\n",
      "Epoch 24/400, Batch 201/4457, Loss: 1.1966845989227295\n",
      "Epoch 24/400, Batch 301/4457, Loss: 1.1966726779937744\n",
      "Epoch 24/400, Batch 401/4457, Loss: 1.1655309200286865\n",
      "Epoch 24/400, Batch 501/4457, Loss: 1.165468454360962\n",
      "Epoch 24/400, Batch 601/4457, Loss: 1.1768637895584106\n",
      "Epoch 24/400, Batch 701/4457, Loss: 1.1966722011566162\n",
      "Epoch 24/400, Batch 801/4457, Loss: 1.1966722011566162\n",
      "Epoch 24/400, Batch 901/4457, Loss: 1.165422797203064\n",
      "Epoch 24/400, Batch 1001/4457, Loss: 1.1654224395751953\n",
      "Epoch 24/400, Batch 1101/4457, Loss: 1.1654231548309326\n",
      "Epoch 24/400, Batch 1201/4457, Loss: 1.1654274463653564\n",
      "Epoch 24/400, Batch 1301/4457, Loss: 1.1966793537139893\n",
      "Epoch 24/400, Batch 1401/4457, Loss: 1.1966723203659058\n",
      "Epoch 24/400, Batch 1501/4457, Loss: 1.1967244148254395\n",
      "Epoch 24/400, Batch 1601/4457, Loss: 1.1654269695281982\n",
      "Epoch 24/400, Batch 1701/4457, Loss: 1.1654229164123535\n",
      "Epoch 24/400, Batch 1801/4457, Loss: 1.1966724395751953\n",
      "Epoch 24/400, Batch 1901/4457, Loss: 1.1966726779937744\n",
      "Epoch 24/400, Batch 2001/4457, Loss: 1.2278800010681152\n",
      "Epoch 24/400, Batch 2101/4457, Loss: 1.1966724395751953\n",
      "Epoch 24/400, Batch 2201/4457, Loss: 1.2279701232910156\n",
      "Epoch 24/400, Batch 2301/4457, Loss: 1.196699619293213\n",
      "Epoch 24/400, Batch 2401/4457, Loss: 1.1654226779937744\n",
      "Epoch 24/400, Batch 2501/4457, Loss: 1.1878340244293213\n",
      "Epoch 24/400, Batch 2601/4457, Loss: 1.165422797203064\n",
      "Epoch 24/400, Batch 2701/4457, Loss: 1.165422797203064\n",
      "Epoch 24/400, Batch 2801/4457, Loss: 1.1654292345046997\n",
      "Epoch 24/400, Batch 2901/4457, Loss: 1.1952908039093018\n",
      "Epoch 24/400, Batch 3001/4457, Loss: 1.1657741069793701\n",
      "Epoch 24/400, Batch 3101/4457, Loss: 1.1654455661773682\n",
      "Epoch 24/400, Batch 3201/4457, Loss: 1.1697908639907837\n",
      "Epoch 24/400, Batch 3301/4457, Loss: 1.1967518329620361\n",
      "Epoch 24/400, Batch 3401/4457, Loss: 1.1654326915740967\n",
      "Epoch 24/400, Batch 3501/4457, Loss: 1.1654324531555176\n",
      "Epoch 24/400, Batch 3601/4457, Loss: 1.1966749429702759\n",
      "Epoch 24/400, Batch 3701/4457, Loss: 1.1654382944107056\n",
      "Epoch 24/400, Batch 3801/4457, Loss: 1.1654233932495117\n",
      "Epoch 24/400, Batch 3901/4457, Loss: 1.2279257774353027\n",
      "Epoch 24/400, Batch 4001/4457, Loss: 1.1966726779937744\n",
      "Epoch 24/400, Batch 4101/4457, Loss: 1.1654233932495117\n",
      "Epoch 24/400, Batch 4201/4457, Loss: 1.1966766119003296\n",
      "Epoch 24/400, Batch 4301/4457, Loss: 1.1655220985412598\n",
      "Epoch 24/400, Batch 4401/4457, Loss: 1.196672797203064\n",
      "Epoch 24/400, Validation Loss: 1.2885950057516022\n",
      "Epoch 25/400, Batch 1/4457, Loss: 1.2279222011566162\n",
      "Epoch 25/400, Batch 101/4457, Loss: 1.1654287576675415\n",
      "Epoch 25/400, Batch 201/4457, Loss: 1.1654232740402222\n",
      "Epoch 25/400, Batch 301/4457, Loss: 1.1654233932495117\n",
      "Epoch 25/400, Batch 401/4457, Loss: 1.165850043296814\n",
      "Epoch 25/400, Batch 501/4457, Loss: 1.19667387008667\n",
      "Epoch 25/400, Batch 601/4457, Loss: 1.2279229164123535\n",
      "Epoch 25/400, Batch 701/4457, Loss: 1.1654292345046997\n",
      "Epoch 25/400, Batch 801/4457, Loss: 1.1966956853866577\n",
      "Epoch 25/400, Batch 901/4457, Loss: 1.1654231548309326\n",
      "Epoch 25/400, Batch 1001/4457, Loss: 1.2574095726013184\n",
      "Epoch 25/400, Batch 1101/4457, Loss: 1.1963632106781006\n",
      "Epoch 25/400, Batch 1201/4457, Loss: 1.1966726779937744\n",
      "Epoch 25/400, Batch 1301/4457, Loss: 1.1967718601226807\n",
      "Epoch 25/400, Batch 1401/4457, Loss: 1.19670569896698\n",
      "Epoch 25/400, Batch 1501/4457, Loss: 1.227766752243042\n",
      "Epoch 25/400, Batch 1601/4457, Loss: 1.1968873739242554\n",
      "Epoch 25/400, Batch 1701/4457, Loss: 1.1944465637207031\n",
      "Epoch 25/400, Batch 1801/4457, Loss: 1.1654225587844849\n",
      "Epoch 25/400, Batch 1901/4457, Loss: 1.1966723203659058\n",
      "Epoch 25/400, Batch 2001/4457, Loss: 1.16542387008667\n",
      "Epoch 25/400, Batch 2101/4457, Loss: 1.1966731548309326\n",
      "Epoch 25/400, Batch 2201/4457, Loss: 1.1966965198516846\n",
      "Epoch 25/400, Batch 2301/4457, Loss: 1.1967072486877441\n",
      "Epoch 25/400, Batch 2401/4457, Loss: 1.1966724395751953\n",
      "Epoch 25/400, Batch 2501/4457, Loss: 1.1966872215270996\n",
      "Epoch 25/400, Batch 2601/4457, Loss: 1.1966767311096191\n",
      "Epoch 25/400, Batch 2701/4457, Loss: 1.25917387008667\n",
      "Epoch 25/400, Batch 2801/4457, Loss: 1.1966758966445923\n",
      "Epoch 25/400, Batch 2901/4457, Loss: 1.1654224395751953\n",
      "Epoch 25/400, Batch 3001/4457, Loss: 1.1655491590499878\n",
      "Epoch 25/400, Batch 3101/4457, Loss: 1.1654222011566162\n",
      "Epoch 25/400, Batch 3201/4457, Loss: 1.1654225587844849\n",
      "Epoch 25/400, Batch 3301/4457, Loss: 1.1654289960861206\n",
      "Epoch 25/400, Batch 3401/4457, Loss: 1.228040337562561\n",
      "Epoch 25/400, Batch 3501/4457, Loss: 1.1654224395751953\n",
      "Epoch 25/400, Batch 3601/4457, Loss: 1.227923035621643\n",
      "Epoch 25/400, Batch 3701/4457, Loss: 1.1654222011566162\n",
      "Epoch 25/400, Batch 3801/4457, Loss: 1.1654223203659058\n",
      "Epoch 25/400, Batch 3901/4457, Loss: 1.2308731079101562\n",
      "Epoch 25/400, Batch 4001/4457, Loss: 1.1966726779937744\n",
      "Epoch 25/400, Batch 4101/4457, Loss: 1.1966767311096191\n",
      "Epoch 25/400, Batch 4201/4457, Loss: 1.1966753005981445\n",
      "Epoch 25/400, Batch 4301/4457, Loss: 1.1654222011566162\n",
      "Epoch 25/400, Batch 4401/4457, Loss: 1.196675419807434\n",
      "Epoch 25/400, Validation Loss: 1.28616849746969\n",
      "Epoch 26/400, Batch 1/4457, Loss: 1.1966722011566162\n",
      "Epoch 26/400, Batch 101/4457, Loss: 1.196669340133667\n",
      "Epoch 26/400, Batch 201/4457, Loss: 1.1655521392822266\n",
      "Epoch 26/400, Batch 301/4457, Loss: 1.1654225587844849\n",
      "Epoch 26/400, Batch 401/4457, Loss: 1.170295238494873\n",
      "Epoch 26/400, Batch 501/4457, Loss: 1.1966722011566162\n",
      "Epoch 26/400, Batch 601/4457, Loss: 1.1654222011566162\n",
      "Epoch 26/400, Batch 701/4457, Loss: 1.1654318571090698\n",
      "Epoch 26/400, Batch 801/4457, Loss: 1.2279462814331055\n",
      "Epoch 26/400, Batch 901/4457, Loss: 1.1966732740402222\n",
      "Epoch 26/400, Batch 1001/4457, Loss: 1.1654233932495117\n",
      "Epoch 26/400, Batch 1101/4457, Loss: 1.1654255390167236\n",
      "Epoch 26/400, Batch 1201/4457, Loss: 1.2279222011566162\n",
      "Epoch 26/400, Batch 1301/4457, Loss: 1.1654224395751953\n",
      "Epoch 26/400, Batch 1401/4457, Loss: 1.2279398441314697\n",
      "Epoch 26/400, Batch 1501/4457, Loss: 1.1654250621795654\n",
      "Epoch 26/400, Batch 1601/4457, Loss: 1.1966729164123535\n",
      "Epoch 26/400, Batch 1701/4457, Loss: 1.1966732740402222\n",
      "Epoch 26/400, Batch 1801/4457, Loss: 1.1966729164123535\n",
      "Epoch 26/400, Batch 1901/4457, Loss: 1.1654223203659058\n",
      "Epoch 26/400, Batch 2001/4457, Loss: 1.1654222011566162\n",
      "Epoch 26/400, Batch 2101/4457, Loss: 1.1654350757598877\n",
      "Epoch 26/400, Batch 2201/4457, Loss: 1.1967029571533203\n",
      "Epoch 26/400, Batch 2301/4457, Loss: 1.229975938796997\n",
      "Epoch 26/400, Batch 2401/4457, Loss: 1.1682995557785034\n",
      "Epoch 26/400, Batch 2501/4457, Loss: 1.1654224395751953\n",
      "Epoch 26/400, Batch 2601/4457, Loss: 1.1654223203659058\n",
      "Epoch 26/400, Batch 2701/4457, Loss: 1.1654233932495117\n",
      "Epoch 26/400, Batch 2801/4457, Loss: 1.1996954679489136\n",
      "Epoch 26/400, Batch 2901/4457, Loss: 1.1654973030090332\n",
      "Epoch 26/400, Batch 3001/4457, Loss: 1.1966729164123535\n",
      "Epoch 26/400, Batch 3101/4457, Loss: 1.1655656099319458\n",
      "Epoch 26/400, Batch 3201/4457, Loss: 1.2279224395751953\n",
      "Epoch 26/400, Batch 3301/4457, Loss: 1.196673035621643\n",
      "Epoch 26/400, Batch 3401/4457, Loss: 1.2072465419769287\n",
      "Epoch 26/400, Batch 3501/4457, Loss: 1.1654222011566162\n",
      "Epoch 26/400, Batch 3601/4457, Loss: 1.1656140089035034\n",
      "Epoch 26/400, Batch 3701/4457, Loss: 1.1966725587844849\n",
      "Epoch 26/400, Batch 3801/4457, Loss: 1.2282278537750244\n",
      "Epoch 26/400, Batch 3901/4457, Loss: 1.1966726779937744\n",
      "Epoch 26/400, Batch 4001/4457, Loss: 1.1654236316680908\n",
      "Epoch 26/400, Batch 4101/4457, Loss: 1.1654224395751953\n",
      "Epoch 26/400, Batch 4201/4457, Loss: 1.1654224395751953\n",
      "Epoch 26/400, Batch 4301/4457, Loss: 1.1654269695281982\n",
      "Epoch 26/400, Batch 4401/4457, Loss: 1.1966922283172607\n",
      "Epoch 26/400, Validation Loss: 1.2864218690092601\n",
      "Epoch 27/400, Batch 1/4457, Loss: 1.1654409170150757\n",
      "Epoch 27/400, Batch 101/4457, Loss: 1.1654223203659058\n",
      "Epoch 27/400, Batch 201/4457, Loss: 1.1654226779937744\n",
      "Epoch 27/400, Batch 301/4457, Loss: 1.248091459274292\n",
      "Epoch 27/400, Batch 401/4457, Loss: 1.1966791152954102\n",
      "Epoch 27/400, Batch 501/4457, Loss: 1.1967684030532837\n",
      "Epoch 27/400, Batch 601/4457, Loss: 1.1654224395751953\n",
      "Epoch 27/400, Batch 701/4457, Loss: 1.1654253005981445\n",
      "Epoch 27/400, Batch 801/4457, Loss: 1.1654222011566162\n",
      "Epoch 27/400, Batch 901/4457, Loss: 1.1966609954833984\n",
      "Epoch 27/400, Batch 1001/4457, Loss: 1.1654415130615234\n",
      "Epoch 27/400, Batch 1101/4457, Loss: 1.2132965326309204\n",
      "Epoch 27/400, Batch 1201/4457, Loss: 1.1654249429702759\n",
      "Epoch 27/400, Batch 1301/4457, Loss: 1.1654472351074219\n",
      "Epoch 27/400, Batch 1401/4457, Loss: 1.1975810527801514\n",
      "Epoch 27/400, Batch 1501/4457, Loss: 1.1654229164123535\n",
      "Epoch 27/400, Batch 1601/4457, Loss: 1.1969141960144043\n",
      "Epoch 27/400, Batch 1701/4457, Loss: 1.1654661893844604\n",
      "Epoch 27/400, Batch 1801/4457, Loss: 1.1966795921325684\n",
      "Epoch 27/400, Batch 1901/4457, Loss: 1.174588918685913\n",
      "Epoch 27/400, Batch 2001/4457, Loss: 1.1654236316680908\n",
      "Epoch 27/400, Batch 2101/4457, Loss: 1.165584683418274\n",
      "Epoch 27/400, Batch 2201/4457, Loss: 1.1654226779937744\n",
      "Epoch 27/400, Batch 2301/4457, Loss: 1.1654233932495117\n",
      "Epoch 27/400, Batch 2401/4457, Loss: 1.19533371925354\n",
      "Epoch 27/400, Batch 2501/4457, Loss: 1.1654255390167236\n",
      "Epoch 27/400, Batch 2601/4457, Loss: 1.165422797203064\n",
      "Epoch 27/400, Batch 2701/4457, Loss: 1.1966731548309326\n",
      "Epoch 27/400, Batch 2801/4457, Loss: 1.1654222011566162\n",
      "Epoch 27/400, Batch 2901/4457, Loss: 1.2279231548309326\n",
      "Epoch 27/400, Batch 3001/4457, Loss: 1.2279222011566162\n",
      "Epoch 27/400, Batch 3101/4457, Loss: 1.2279223203659058\n",
      "Epoch 27/400, Batch 3201/4457, Loss: 1.2904274463653564\n",
      "Epoch 27/400, Batch 3301/4457, Loss: 1.227482557296753\n",
      "Epoch 27/400, Batch 3401/4457, Loss: 1.1966722011566162\n",
      "Epoch 27/400, Batch 3501/4457, Loss: 1.1959116458892822\n",
      "Epoch 27/400, Batch 3601/4457, Loss: 1.1966679096221924\n",
      "Epoch 27/400, Batch 3701/4457, Loss: 1.1966001987457275\n",
      "Epoch 27/400, Batch 3801/4457, Loss: 1.1966729164123535\n",
      "Epoch 27/400, Batch 3901/4457, Loss: 1.194941520690918\n",
      "Epoch 27/400, Batch 4001/4457, Loss: 1.16542387008667\n",
      "Epoch 27/400, Batch 4101/4457, Loss: 1.1966726779937744\n",
      "Epoch 27/400, Batch 4201/4457, Loss: 1.1966723203659058\n",
      "Epoch 27/400, Batch 4301/4457, Loss: 1.1654224395751953\n",
      "Epoch 27/400, Batch 4401/4457, Loss: 1.1668034791946411\n",
      "Epoch 27/400, Validation Loss: 1.2895477753546503\n",
      "Epoch 28/400, Batch 1/4457, Loss: 1.1912647485733032\n",
      "Epoch 28/400, Batch 101/4457, Loss: 1.196673035621643\n",
      "Epoch 28/400, Batch 201/4457, Loss: 1.1654224395751953\n",
      "Epoch 28/400, Batch 301/4457, Loss: 1.1966733932495117\n",
      "Epoch 28/400, Batch 401/4457, Loss: 1.1654372215270996\n",
      "Epoch 28/400, Batch 501/4457, Loss: 1.2279245853424072\n",
      "Epoch 28/400, Batch 601/4457, Loss: 1.165422797203064\n",
      "Epoch 28/400, Batch 701/4457, Loss: 1.1654222011566162\n",
      "Epoch 28/400, Batch 801/4457, Loss: 1.16542649269104\n",
      "Epoch 28/400, Batch 901/4457, Loss: 1.165428638458252\n",
      "Epoch 28/400, Batch 1001/4457, Loss: 1.1654222011566162\n",
      "Epoch 28/400, Batch 1101/4457, Loss: 1.19667387008667\n",
      "Epoch 28/400, Batch 1201/4457, Loss: 1.1654223203659058\n",
      "Epoch 28/400, Batch 1301/4457, Loss: 1.2279260158538818\n",
      "Epoch 28/400, Batch 1401/4457, Loss: 1.1654225587844849\n",
      "Epoch 28/400, Batch 1501/4457, Loss: 1.2279226779937744\n",
      "Epoch 28/400, Batch 1601/4457, Loss: 1.1654224395751953\n",
      "Epoch 28/400, Batch 1701/4457, Loss: 1.2280595302581787\n",
      "Epoch 28/400, Batch 1801/4457, Loss: 1.1966722011566162\n",
      "Epoch 28/400, Batch 1901/4457, Loss: 1.1966723203659058\n",
      "Epoch 28/400, Batch 2001/4457, Loss: 1.1966722011566162\n",
      "Epoch 28/400, Batch 2101/4457, Loss: 1.1680002212524414\n",
      "Epoch 28/400, Batch 2201/4457, Loss: 1.1654222011566162\n",
      "Epoch 28/400, Batch 2301/4457, Loss: 1.2279621362686157\n",
      "Epoch 28/400, Batch 2401/4457, Loss: 1.1654224395751953\n",
      "Epoch 28/400, Batch 2501/4457, Loss: 1.2591724395751953\n",
      "Epoch 28/400, Batch 2601/4457, Loss: 1.2279223203659058\n",
      "Epoch 28/400, Batch 2701/4457, Loss: 1.1657216548919678\n",
      "Epoch 28/400, Batch 2801/4457, Loss: 1.1966723203659058\n",
      "Epoch 28/400, Batch 2901/4457, Loss: 1.1654231548309326\n",
      "Epoch 28/400, Batch 3001/4457, Loss: 1.1654223203659058\n",
      "Epoch 28/400, Batch 3101/4457, Loss: 1.1654232740402222\n",
      "Epoch 28/400, Batch 3201/4457, Loss: 1.1654223203659058\n",
      "Epoch 28/400, Batch 3301/4457, Loss: 1.1654937267303467\n",
      "Epoch 28/400, Batch 3401/4457, Loss: 1.1979568004608154\n",
      "Epoch 28/400, Batch 3501/4457, Loss: 1.184996485710144\n",
      "Epoch 28/400, Batch 3601/4457, Loss: 1.1654748916625977\n",
      "Epoch 28/400, Batch 3701/4457, Loss: 1.1966722011566162\n",
      "Epoch 28/400, Batch 3801/4457, Loss: 1.1719505786895752\n",
      "Epoch 28/400, Batch 3901/4457, Loss: 1.1654229164123535\n",
      "Epoch 28/400, Batch 4001/4457, Loss: 1.1654226779937744\n",
      "Epoch 28/400, Batch 4101/4457, Loss: 1.1654276847839355\n",
      "Epoch 28/400, Batch 4201/4457, Loss: 1.2279223203659058\n",
      "Epoch 28/400, Batch 4301/4457, Loss: 1.1654367446899414\n",
      "Epoch 28/400, Batch 4401/4457, Loss: 1.1654225587844849\n",
      "Epoch 28/400, Validation Loss: 1.2910082691482134\n",
      "Epoch 29/400, Batch 1/4457, Loss: 1.2583880424499512\n",
      "Epoch 29/400, Batch 101/4457, Loss: 1.1654225587844849\n",
      "Epoch 29/400, Batch 201/4457, Loss: 1.1654255390167236\n",
      "Epoch 29/400, Batch 301/4457, Loss: 1.2279226779937744\n",
      "Epoch 29/400, Batch 401/4457, Loss: 1.1654222011566162\n",
      "Epoch 29/400, Batch 501/4457, Loss: 1.1654245853424072\n",
      "Epoch 29/400, Batch 601/4457, Loss: 1.2591722011566162\n",
      "Epoch 29/400, Batch 701/4457, Loss: 1.1966723203659058\n",
      "Epoch 29/400, Batch 801/4457, Loss: 1.1654223203659058\n",
      "Epoch 29/400, Batch 901/4457, Loss: 1.1654222011566162\n",
      "Epoch 29/400, Batch 1001/4457, Loss: 1.1654386520385742\n",
      "Epoch 29/400, Batch 1101/4457, Loss: 1.165422797203064\n",
      "Epoch 29/400, Batch 1201/4457, Loss: 1.1654224395751953\n",
      "Epoch 29/400, Batch 1301/4457, Loss: 1.1654624938964844\n",
      "Epoch 29/400, Batch 1401/4457, Loss: 1.1654222011566162\n",
      "Epoch 29/400, Batch 1501/4457, Loss: 1.1966722011566162\n",
      "Epoch 29/400, Batch 1601/4457, Loss: 1.2279223203659058\n",
      "Epoch 29/400, Batch 1701/4457, Loss: 1.2261171340942383\n",
      "Epoch 29/400, Batch 1801/4457, Loss: 1.1971759796142578\n",
      "Epoch 29/400, Batch 1901/4457, Loss: 1.165424108505249\n",
      "Epoch 29/400, Batch 2001/4457, Loss: 1.2279226779937744\n",
      "Epoch 29/400, Batch 2101/4457, Loss: 1.1966725587844849\n",
      "Epoch 29/400, Batch 2201/4457, Loss: 1.165422797203064\n",
      "Epoch 29/400, Batch 2301/4457, Loss: 1.165460467338562\n",
      "Epoch 29/400, Batch 2401/4457, Loss: 1.2043344974517822\n",
      "Epoch 29/400, Batch 2501/4457, Loss: 1.165508508682251\n",
      "Epoch 29/400, Batch 2601/4457, Loss: 1.1966722011566162\n",
      "Epoch 29/400, Batch 2701/4457, Loss: 1.1654233932495117\n",
      "Epoch 29/400, Batch 2801/4457, Loss: 1.1655011177062988\n",
      "Epoch 29/400, Batch 2901/4457, Loss: 1.1967353820800781\n",
      "Epoch 29/400, Batch 3001/4457, Loss: 1.2591724395751953\n",
      "Epoch 29/400, Batch 3101/4457, Loss: 1.1654224395751953\n",
      "Epoch 29/400, Batch 3201/4457, Loss: 1.2904226779937744\n",
      "Epoch 29/400, Batch 3301/4457, Loss: 1.1966725587844849\n",
      "Epoch 29/400, Batch 3401/4457, Loss: 1.1966722011566162\n",
      "Epoch 29/400, Batch 3501/4457, Loss: 1.1966723203659058\n",
      "Epoch 29/400, Batch 3601/4457, Loss: 1.1966729164123535\n",
      "Epoch 29/400, Batch 3701/4457, Loss: 1.1966722011566162\n",
      "Epoch 29/400, Batch 3801/4457, Loss: 1.1654222011566162\n",
      "Epoch 29/400, Batch 3901/4457, Loss: 1.1654319763183594\n",
      "Epoch 29/400, Batch 4001/4457, Loss: 1.1654222011566162\n",
      "Epoch 29/400, Batch 4101/4457, Loss: 1.1654225587844849\n",
      "Epoch 29/400, Batch 4201/4457, Loss: 1.1654222011566162\n",
      "Epoch 29/400, Batch 4301/4457, Loss: 1.1654225587844849\n",
      "Epoch 29/400, Batch 4401/4457, Loss: 1.1654226779937744\n",
      "Epoch 29/400, Validation Loss: 1.2912635113748292\n",
      "Epoch 30/400, Batch 1/4457, Loss: 1.2279248237609863\n",
      "Epoch 30/400, Batch 101/4457, Loss: 1.1679179668426514\n",
      "Epoch 30/400, Batch 201/4457, Loss: 1.1654224395751953\n",
      "Epoch 30/400, Batch 301/4457, Loss: 1.1654229164123535\n",
      "Epoch 30/400, Batch 401/4457, Loss: 1.1654236316680908\n",
      "Epoch 30/400, Batch 501/4457, Loss: 1.1966729164123535\n",
      "Epoch 30/400, Batch 601/4457, Loss: 1.1966753005981445\n",
      "Epoch 30/400, Batch 701/4457, Loss: 1.1967713832855225\n",
      "Epoch 30/400, Batch 801/4457, Loss: 1.2279222011566162\n",
      "Epoch 30/400, Batch 901/4457, Loss: 1.2279736995697021\n",
      "Epoch 30/400, Batch 1001/4457, Loss: 1.1966736316680908\n",
      "Epoch 30/400, Batch 1101/4457, Loss: 1.2279222011566162\n",
      "Epoch 30/400, Batch 1201/4457, Loss: 1.1654229164123535\n",
      "Epoch 30/400, Batch 1301/4457, Loss: 1.1654232740402222\n",
      "Epoch 30/400, Batch 1401/4457, Loss: 1.1654226779937744\n",
      "Epoch 30/400, Batch 1501/4457, Loss: 1.1669251918792725\n",
      "Epoch 30/400, Batch 1601/4457, Loss: 1.1654789447784424\n",
      "Epoch 30/400, Batch 1701/4457, Loss: 1.1966791152954102\n",
      "Epoch 30/400, Batch 1801/4457, Loss: 1.2279224395751953\n",
      "Epoch 30/400, Batch 1901/4457, Loss: 1.16542387008667\n",
      "Epoch 30/400, Batch 2001/4457, Loss: 1.1966745853424072\n",
      "Epoch 30/400, Batch 2101/4457, Loss: 1.1966803073883057\n",
      "Epoch 30/400, Batch 2201/4457, Loss: 1.2279224395751953\n",
      "Epoch 30/400, Batch 2301/4457, Loss: 1.1966725587844849\n",
      "Epoch 30/400, Batch 2401/4457, Loss: 1.2005751132965088\n",
      "Epoch 30/400, Batch 2501/4457, Loss: 1.2279223203659058\n",
      "Epoch 30/400, Batch 2601/4457, Loss: 1.1968553066253662\n",
      "Epoch 30/400, Batch 2701/4457, Loss: 1.1654225587844849\n",
      "Epoch 30/400, Batch 2801/4457, Loss: 1.228175163269043\n",
      "Epoch 30/400, Batch 2901/4457, Loss: 1.1966724395751953\n",
      "Epoch 30/400, Batch 3001/4457, Loss: 1.1663432121276855\n",
      "Epoch 30/400, Batch 3101/4457, Loss: 1.1654235124588013\n",
      "Epoch 30/400, Batch 3201/4457, Loss: 1.1654224395751953\n",
      "Epoch 30/400, Batch 3301/4457, Loss: 1.199294090270996\n",
      "Epoch 30/400, Batch 3401/4457, Loss: 1.2375710010528564\n",
      "Epoch 30/400, Batch 3501/4457, Loss: 1.1654222011566162\n",
      "Epoch 30/400, Batch 3601/4457, Loss: 1.1968480348587036\n",
      "Epoch 30/400, Batch 3701/4457, Loss: 1.1654226779937744\n",
      "Epoch 30/400, Batch 3801/4457, Loss: 1.165586233139038\n",
      "Epoch 30/400, Batch 3901/4457, Loss: 1.1654222011566162\n",
      "Epoch 30/400, Batch 4001/4457, Loss: 1.1654224395751953\n",
      "Epoch 30/400, Batch 4101/4457, Loss: 1.1966736316680908\n",
      "Epoch 30/400, Batch 4201/4457, Loss: 1.1966726779937744\n",
      "Epoch 30/400, Batch 4301/4457, Loss: 1.1654243469238281\n",
      "Epoch 30/400, Batch 4401/4457, Loss: 1.1997543573379517\n",
      "Epoch 30/400, Validation Loss: 1.284364372137047\n",
      "Epoch 31/400, Batch 1/4457, Loss: 1.1654226779937744\n",
      "Epoch 31/400, Batch 101/4457, Loss: 1.1654223203659058\n",
      "Epoch 31/400, Batch 201/4457, Loss: 1.1654222011566162\n",
      "Epoch 31/400, Batch 301/4457, Loss: 1.1654229164123535\n",
      "Epoch 31/400, Batch 401/4457, Loss: 1.1654253005981445\n",
      "Epoch 31/400, Batch 501/4457, Loss: 1.1654222011566162\n",
      "Epoch 31/400, Batch 601/4457, Loss: 1.16542649269104\n",
      "Epoch 31/400, Batch 701/4457, Loss: 1.1654274463653564\n",
      "Epoch 31/400, Batch 801/4457, Loss: 1.2279222011566162\n",
      "Epoch 31/400, Batch 901/4457, Loss: 1.1654223203659058\n",
      "Epoch 31/400, Batch 1001/4457, Loss: 1.1966824531555176\n",
      "Epoch 31/400, Batch 1101/4457, Loss: 1.1655367612838745\n",
      "Epoch 31/400, Batch 1201/4457, Loss: 1.2279224395751953\n",
      "Epoch 31/400, Batch 1301/4457, Loss: 1.2286065816879272\n",
      "Epoch 31/400, Batch 1401/4457, Loss: 1.196673035621643\n",
      "Epoch 31/400, Batch 1501/4457, Loss: 1.16545832157135\n",
      "Epoch 31/400, Batch 1601/4457, Loss: 1.1660997867584229\n",
      "Epoch 31/400, Batch 1701/4457, Loss: 1.1654223203659058\n",
      "Epoch 31/400, Batch 1801/4457, Loss: 1.1654270887374878\n",
      "Epoch 31/400, Batch 1901/4457, Loss: 1.1966769695281982\n",
      "Epoch 31/400, Batch 2001/4457, Loss: 1.2279222011566162\n",
      "Epoch 31/400, Batch 2101/4457, Loss: 1.1654224395751953\n",
      "Epoch 31/400, Batch 2201/4457, Loss: 1.1966722011566162\n",
      "Epoch 31/400, Batch 2301/4457, Loss: 1.1654222011566162\n",
      "Epoch 31/400, Batch 2401/4457, Loss: 1.1655335426330566\n",
      "Epoch 31/400, Batch 2501/4457, Loss: 1.1966723203659058\n",
      "Epoch 31/400, Batch 2601/4457, Loss: 1.205979585647583\n",
      "Epoch 31/400, Batch 2701/4457, Loss: 1.1654223203659058\n",
      "Epoch 31/400, Batch 2801/4457, Loss: 1.1654243469238281\n",
      "Epoch 31/400, Batch 2901/4457, Loss: 1.1654222011566162\n",
      "Epoch 31/400, Batch 3001/4457, Loss: 1.1966723203659058\n",
      "Epoch 31/400, Batch 3101/4457, Loss: 1.1654226779937744\n",
      "Epoch 31/400, Batch 3201/4457, Loss: 1.2279225587844849\n",
      "Epoch 31/400, Batch 3301/4457, Loss: 1.1966724395751953\n",
      "Epoch 31/400, Batch 3401/4457, Loss: 1.2591722011566162\n",
      "Epoch 31/400, Batch 3501/4457, Loss: 1.1654272079467773\n",
      "Epoch 31/400, Batch 3601/4457, Loss: 1.1654223203659058\n",
      "Epoch 31/400, Batch 3701/4457, Loss: 1.1966726779937744\n",
      "Epoch 31/400, Batch 3801/4457, Loss: 1.1654222011566162\n",
      "Epoch 31/400, Batch 3901/4457, Loss: 1.1654257774353027\n",
      "Epoch 31/400, Batch 4001/4457, Loss: 1.1654253005981445\n",
      "Epoch 31/400, Batch 4101/4457, Loss: 1.1966722011566162\n",
      "Epoch 31/400, Batch 4201/4457, Loss: 1.1654223203659058\n",
      "Epoch 31/400, Batch 4301/4457, Loss: 1.2279226779937744\n",
      "Epoch 31/400, Batch 4401/4457, Loss: 1.1654223203659058\n",
      "Epoch 31/400, Validation Loss: 1.282532285721529\n",
      "Epoch 32/400, Batch 1/4457, Loss: 1.1654222011566162\n",
      "Epoch 32/400, Batch 101/4457, Loss: 1.1968055963516235\n",
      "Epoch 32/400, Batch 201/4457, Loss: 1.1654222011566162\n",
      "Epoch 32/400, Batch 301/4457, Loss: 1.1657605171203613\n",
      "Epoch 32/400, Batch 401/4457, Loss: 1.1966723203659058\n",
      "Epoch 32/400, Batch 501/4457, Loss: 1.1978833675384521\n",
      "Epoch 32/400, Batch 601/4457, Loss: 1.1966724395751953\n",
      "Epoch 32/400, Batch 701/4457, Loss: 1.2279224395751953\n",
      "Epoch 32/400, Batch 801/4457, Loss: 1.196689248085022\n",
      "Epoch 32/400, Batch 901/4457, Loss: 1.1654255390167236\n",
      "Epoch 32/400, Batch 1001/4457, Loss: 1.196677565574646\n",
      "Epoch 32/400, Batch 1101/4457, Loss: 1.1654224395751953\n",
      "Epoch 32/400, Batch 1201/4457, Loss: 1.2279222011566162\n",
      "Epoch 32/400, Batch 1301/4457, Loss: 1.1654229164123535\n",
      "Epoch 32/400, Batch 1401/4457, Loss: 1.1654223203659058\n",
      "Epoch 32/400, Batch 1501/4457, Loss: 1.1654229164123535\n",
      "Epoch 32/400, Batch 1601/4457, Loss: 1.1654224395751953\n",
      "Epoch 32/400, Batch 1701/4457, Loss: 1.1966729164123535\n",
      "Epoch 32/400, Batch 1801/4457, Loss: 1.1654226779937744\n",
      "Epoch 32/400, Batch 1901/4457, Loss: 1.1654229164123535\n",
      "Epoch 32/400, Batch 2001/4457, Loss: 1.1966819763183594\n",
      "Epoch 32/400, Batch 2101/4457, Loss: 1.196682333946228\n",
      "Epoch 32/400, Batch 2201/4457, Loss: 1.16542387008667\n",
      "Epoch 32/400, Batch 2301/4457, Loss: 1.1654224395751953\n",
      "Epoch 32/400, Batch 2401/4457, Loss: 1.1966724395751953\n",
      "Epoch 32/400, Batch 2501/4457, Loss: 1.195014238357544\n",
      "Epoch 32/400, Batch 2601/4457, Loss: 1.1966731548309326\n",
      "Epoch 32/400, Batch 2701/4457, Loss: 1.196672797203064\n",
      "Epoch 32/400, Batch 2801/4457, Loss: 1.1654250621795654\n",
      "Epoch 32/400, Batch 2901/4457, Loss: 1.1966743469238281\n",
      "Epoch 32/400, Batch 3001/4457, Loss: 1.1966890096664429\n",
      "Epoch 32/400, Batch 3101/4457, Loss: 1.1654245853424072\n",
      "Epoch 32/400, Batch 3201/4457, Loss: 1.1654222011566162\n",
      "Epoch 32/400, Batch 3301/4457, Loss: 1.1654224395751953\n",
      "Epoch 32/400, Batch 3401/4457, Loss: 1.1966726779937744\n",
      "Epoch 32/400, Batch 3501/4457, Loss: 1.1654272079467773\n",
      "Epoch 32/400, Batch 3601/4457, Loss: 1.1966784000396729\n",
      "Epoch 32/400, Batch 3701/4457, Loss: 1.1654226779937744\n",
      "Epoch 32/400, Batch 3801/4457, Loss: 1.1654229164123535\n",
      "Epoch 32/400, Batch 3901/4457, Loss: 1.2279225587844849\n",
      "Epoch 32/400, Batch 4001/4457, Loss: 1.165423035621643\n",
      "Epoch 32/400, Batch 4101/4457, Loss: 1.1966724395751953\n",
      "Epoch 32/400, Batch 4201/4457, Loss: 1.1654229164123535\n",
      "Epoch 32/400, Batch 4301/4457, Loss: 1.1654233932495117\n",
      "Epoch 32/400, Batch 4401/4457, Loss: 1.1654223203659058\n",
      "Epoch 32/400, Validation Loss: 1.280891891864557\n",
      "Epoch 33/400, Batch 1/4457, Loss: 1.1654223203659058\n",
      "Epoch 33/400, Batch 101/4457, Loss: 1.1654322147369385\n",
      "Epoch 33/400, Batch 201/4457, Loss: 1.1654226779937744\n",
      "Epoch 33/400, Batch 301/4457, Loss: 1.1654229164123535\n",
      "Epoch 33/400, Batch 401/4457, Loss: 1.2279223203659058\n",
      "Epoch 33/400, Batch 501/4457, Loss: 1.1966742277145386\n",
      "Epoch 33/400, Batch 601/4457, Loss: 1.2279236316680908\n",
      "Epoch 33/400, Batch 701/4457, Loss: 1.2279222011566162\n",
      "Epoch 33/400, Batch 801/4457, Loss: 1.1966724395751953\n",
      "Epoch 33/400, Batch 901/4457, Loss: 1.1654222011566162\n",
      "Epoch 33/400, Batch 1001/4457, Loss: 1.1654233932495117\n",
      "Epoch 33/400, Batch 1101/4457, Loss: 1.1966724395751953\n",
      "Epoch 33/400, Batch 1201/4457, Loss: 1.196837067604065\n",
      "Epoch 33/400, Batch 1301/4457, Loss: 1.1966723203659058\n",
      "Epoch 33/400, Batch 1401/4457, Loss: 1.1657445430755615\n",
      "Epoch 33/400, Batch 1501/4457, Loss: 1.1966722011566162\n",
      "Epoch 33/400, Batch 1601/4457, Loss: 1.1654303073883057\n",
      "Epoch 33/400, Batch 1701/4457, Loss: 1.1654226779937744\n",
      "Epoch 33/400, Batch 1801/4457, Loss: 1.2642431259155273\n",
      "Epoch 33/400, Batch 1901/4457, Loss: 1.1654226779937744\n",
      "Epoch 33/400, Batch 2001/4457, Loss: 1.1970397233963013\n",
      "Epoch 33/400, Batch 2101/4457, Loss: 1.165423035621643\n",
      "Epoch 33/400, Batch 2201/4457, Loss: 1.2279412746429443\n",
      "Epoch 33/400, Batch 2301/4457, Loss: 1.1966725587844849\n",
      "Epoch 33/400, Batch 2401/4457, Loss: 1.1654233932495117\n",
      "Epoch 33/400, Batch 2501/4457, Loss: 1.2279235124588013\n",
      "Epoch 33/400, Batch 2601/4457, Loss: 1.1974761486053467\n",
      "Epoch 33/400, Batch 2701/4457, Loss: 1.165422797203064\n",
      "Epoch 33/400, Batch 2801/4457, Loss: 1.1654231548309326\n",
      "Epoch 33/400, Batch 2901/4457, Loss: 1.1654226779937744\n",
      "Epoch 33/400, Batch 3001/4457, Loss: 1.1654226779937744\n",
      "Epoch 33/400, Batch 3101/4457, Loss: 1.1654231548309326\n",
      "Epoch 33/400, Batch 3201/4457, Loss: 1.1654245853424072\n",
      "Epoch 33/400, Batch 3301/4457, Loss: 1.1654586791992188\n",
      "Epoch 33/400, Batch 3401/4457, Loss: 1.1654380559921265\n",
      "Epoch 33/400, Batch 3501/4457, Loss: 1.1966725587844849\n",
      "Epoch 33/400, Batch 3601/4457, Loss: 1.196690320968628\n",
      "Epoch 33/400, Batch 3701/4457, Loss: 1.1654268503189087\n",
      "Epoch 33/400, Batch 3801/4457, Loss: 1.1966726779937744\n",
      "Epoch 33/400, Batch 3901/4457, Loss: 1.165433406829834\n",
      "Epoch 33/400, Batch 4001/4457, Loss: 1.2524752616882324\n",
      "Epoch 33/400, Batch 4101/4457, Loss: 1.1654257774353027\n",
      "Epoch 33/400, Batch 4201/4457, Loss: 1.1654224395751953\n",
      "Epoch 33/400, Batch 4301/4457, Loss: 1.1654226779937744\n",
      "Epoch 33/400, Batch 4401/4457, Loss: 1.227922797203064\n",
      "Epoch 33/400, Validation Loss: 1.291798196851261\n",
      "Epoch 34/400, Batch 1/4457, Loss: 1.16542387008667\n",
      "Epoch 34/400, Batch 101/4457, Loss: 1.1654595136642456\n",
      "Epoch 34/400, Batch 201/4457, Loss: 1.1966724395751953\n",
      "Epoch 34/400, Batch 301/4457, Loss: 1.1966722011566162\n",
      "Epoch 34/400, Batch 401/4457, Loss: 1.1966732740402222\n",
      "Epoch 34/400, Batch 501/4457, Loss: 1.2591726779937744\n",
      "Epoch 34/400, Batch 601/4457, Loss: 1.16544771194458\n",
      "Epoch 34/400, Batch 701/4457, Loss: 1.1654229164123535\n",
      "Epoch 34/400, Batch 801/4457, Loss: 1.165423035621643\n",
      "Epoch 34/400, Batch 901/4457, Loss: 1.1966724395751953\n",
      "Epoch 34/400, Batch 1001/4457, Loss: 1.1654245853424072\n",
      "Epoch 34/400, Batch 1101/4457, Loss: 1.1966722011566162\n",
      "Epoch 34/400, Batch 1201/4457, Loss: 1.1966745853424072\n",
      "Epoch 34/400, Batch 1301/4457, Loss: 1.227924108505249\n",
      "Epoch 34/400, Batch 1401/4457, Loss: 1.1654224395751953\n",
      "Epoch 34/400, Batch 1501/4457, Loss: 1.1965463161468506\n",
      "Epoch 34/400, Batch 1601/4457, Loss: 1.2279245853424072\n",
      "Epoch 34/400, Batch 1701/4457, Loss: 1.1665515899658203\n",
      "Epoch 34/400, Batch 1801/4457, Loss: 1.1966722011566162\n",
      "Epoch 34/400, Batch 1901/4457, Loss: 1.1966859102249146\n",
      "Epoch 34/400, Batch 2001/4457, Loss: 1.1966724395751953\n",
      "Epoch 34/400, Batch 2101/4457, Loss: 1.1654233932495117\n",
      "Epoch 34/400, Batch 2201/4457, Loss: 1.196678876876831\n",
      "Epoch 34/400, Batch 2301/4457, Loss: 1.1654222011566162\n",
      "Epoch 34/400, Batch 2401/4457, Loss: 1.1966722011566162\n",
      "Epoch 34/400, Batch 2501/4457, Loss: 1.1966722011566162\n",
      "Epoch 34/400, Batch 2601/4457, Loss: 1.1654222011566162\n",
      "Epoch 34/400, Batch 2701/4457, Loss: 1.1966722011566162\n",
      "Epoch 34/400, Batch 2801/4457, Loss: 1.1966723203659058\n",
      "Epoch 34/400, Batch 2901/4457, Loss: 1.1966733932495117\n",
      "Epoch 34/400, Batch 3001/4457, Loss: 1.1654222011566162\n",
      "Epoch 34/400, Batch 3101/4457, Loss: 1.1966723203659058\n",
      "Epoch 34/400, Batch 3201/4457, Loss: 1.1654225587844849\n",
      "Epoch 34/400, Batch 3301/4457, Loss: 1.1654223203659058\n",
      "Epoch 34/400, Batch 3401/4457, Loss: 1.1654223203659058\n",
      "Epoch 34/400, Batch 3501/4457, Loss: 1.1966722011566162\n",
      "Epoch 34/400, Batch 3601/4457, Loss: 1.196674108505249\n",
      "Epoch 34/400, Batch 3701/4457, Loss: 1.196675181388855\n",
      "Epoch 34/400, Batch 3801/4457, Loss: 1.1654222011566162\n",
      "Epoch 34/400, Batch 3901/4457, Loss: 1.2904218435287476\n",
      "Epoch 34/400, Batch 4001/4457, Loss: 1.1966725587844849\n",
      "Epoch 34/400, Batch 4101/4457, Loss: 1.165438175201416\n",
      "Epoch 34/400, Batch 4201/4457, Loss: 1.1654222011566162\n",
      "Epoch 34/400, Batch 4301/4457, Loss: 1.1966731548309326\n",
      "Epoch 34/400, Batch 4401/4457, Loss: 1.1654255390167236\n",
      "Epoch 34/400, Validation Loss: 1.2854948001248496\n",
      "Epoch 35/400, Batch 1/4457, Loss: 1.2286638021469116\n",
      "Epoch 35/400, Batch 101/4457, Loss: 1.2279222011566162\n",
      "Epoch 35/400, Batch 201/4457, Loss: 1.1657058000564575\n",
      "Epoch 35/400, Batch 301/4457, Loss: 1.1654224395751953\n",
      "Epoch 35/400, Batch 401/4457, Loss: 1.2279250621795654\n",
      "Epoch 35/400, Batch 501/4457, Loss: 1.2279224395751953\n",
      "Epoch 35/400, Batch 601/4457, Loss: 1.2279222011566162\n",
      "Epoch 35/400, Batch 701/4457, Loss: 1.1966722011566162\n",
      "Epoch 35/400, Batch 801/4457, Loss: 1.2592039108276367\n",
      "Epoch 35/400, Batch 901/4457, Loss: 1.19668447971344\n",
      "Epoch 35/400, Batch 1001/4457, Loss: 1.259172797203064\n",
      "Epoch 35/400, Batch 1101/4457, Loss: 1.2279223203659058\n",
      "Epoch 35/400, Batch 1201/4457, Loss: 1.1855603456497192\n",
      "Epoch 35/400, Batch 1301/4457, Loss: 1.1966725587844849\n",
      "Epoch 35/400, Batch 1401/4457, Loss: 1.1656544208526611\n",
      "Epoch 35/400, Batch 1501/4457, Loss: 1.1966722011566162\n",
      "Epoch 35/400, Batch 1601/4457, Loss: 1.1654222011566162\n",
      "Epoch 35/400, Batch 1701/4457, Loss: 1.1654269695281982\n",
      "Epoch 35/400, Batch 1801/4457, Loss: 1.1654224395751953\n",
      "Epoch 35/400, Batch 1901/4457, Loss: 1.1966798305511475\n",
      "Epoch 35/400, Batch 2001/4457, Loss: 1.1966722011566162\n",
      "Epoch 35/400, Batch 2101/4457, Loss: 1.16542387008667\n",
      "Epoch 35/400, Batch 2201/4457, Loss: 1.1654222011566162\n",
      "Epoch 35/400, Batch 2301/4457, Loss: 1.1967512369155884\n",
      "Epoch 35/400, Batch 2401/4457, Loss: 1.1966724395751953\n",
      "Epoch 35/400, Batch 2501/4457, Loss: 1.1654223203659058\n",
      "Epoch 35/400, Batch 2601/4457, Loss: 1.176716685295105\n",
      "Epoch 35/400, Batch 2701/4457, Loss: 1.227946162223816\n",
      "Epoch 35/400, Batch 2801/4457, Loss: 1.1654298305511475\n",
      "Epoch 35/400, Batch 2901/4457, Loss: 1.1966729164123535\n",
      "Epoch 35/400, Batch 3001/4457, Loss: 1.1654222011566162\n",
      "Epoch 35/400, Batch 3101/4457, Loss: 1.2279222011566162\n",
      "Epoch 35/400, Batch 3201/4457, Loss: 1.1654222011566162\n",
      "Epoch 35/400, Batch 3301/4457, Loss: 1.16542387008667\n",
      "Epoch 35/400, Batch 3401/4457, Loss: 1.1654233932495117\n",
      "Epoch 35/400, Batch 3501/4457, Loss: 1.1654223203659058\n",
      "Epoch 35/400, Batch 3601/4457, Loss: 1.196671962738037\n",
      "Epoch 35/400, Batch 3701/4457, Loss: 1.1654272079467773\n",
      "Epoch 35/400, Batch 3801/4457, Loss: 1.1966724395751953\n",
      "Epoch 35/400, Batch 3901/4457, Loss: 1.1654224395751953\n",
      "Epoch 35/400, Batch 4001/4457, Loss: 1.1654245853424072\n",
      "Epoch 35/400, Batch 4101/4457, Loss: 1.1655209064483643\n",
      "Epoch 35/400, Batch 4201/4457, Loss: 1.1966811418533325\n",
      "Epoch 35/400, Batch 4301/4457, Loss: 1.2279232740402222\n",
      "Epoch 35/400, Batch 4401/4457, Loss: 1.1966731548309326\n",
      "Epoch 35/400, Validation Loss: 1.2863569845046317\n",
      "Epoch 36/400, Batch 1/4457, Loss: 1.1966722011566162\n",
      "Epoch 36/400, Batch 101/4457, Loss: 1.1654231548309326\n",
      "Epoch 36/400, Batch 201/4457, Loss: 1.1654303073883057\n",
      "Epoch 36/400, Batch 301/4457, Loss: 1.1654224395751953\n",
      "Epoch 36/400, Batch 401/4457, Loss: 1.1966724395751953\n",
      "Epoch 36/400, Batch 501/4457, Loss: 1.1654223203659058\n",
      "Epoch 36/400, Batch 601/4457, Loss: 1.1966722011566162\n",
      "Epoch 36/400, Batch 701/4457, Loss: 1.1725869178771973\n",
      "Epoch 36/400, Batch 801/4457, Loss: 1.1654236316680908\n",
      "Epoch 36/400, Batch 901/4457, Loss: 1.165475845336914\n",
      "Epoch 36/400, Batch 1001/4457, Loss: 1.1654232740402222\n",
      "Epoch 36/400, Batch 1101/4457, Loss: 1.1654798984527588\n",
      "Epoch 36/400, Batch 1201/4457, Loss: 1.16542649269104\n",
      "Epoch 36/400, Batch 1301/4457, Loss: 1.1654225587844849\n",
      "Epoch 36/400, Batch 1401/4457, Loss: 1.1966779232025146\n",
      "Epoch 36/400, Batch 1501/4457, Loss: 1.2292908430099487\n",
      "Epoch 36/400, Batch 1601/4457, Loss: 1.1655042171478271\n",
      "Epoch 36/400, Batch 1701/4457, Loss: 1.1654222011566162\n",
      "Epoch 36/400, Batch 1801/4457, Loss: 1.1654225587844849\n",
      "Epoch 36/400, Batch 1901/4457, Loss: 1.1654222011566162\n",
      "Epoch 36/400, Batch 2001/4457, Loss: 1.1964073181152344\n",
      "Epoch 36/400, Batch 2101/4457, Loss: 1.1654222011566162\n",
      "Epoch 36/400, Batch 2201/4457, Loss: 1.1966818571090698\n",
      "Epoch 36/400, Batch 2301/4457, Loss: 1.1966724395751953\n",
      "Epoch 36/400, Batch 2401/4457, Loss: 1.1968426704406738\n",
      "Epoch 36/400, Batch 2501/4457, Loss: 1.1654224395751953\n",
      "Epoch 36/400, Batch 2601/4457, Loss: 1.1966722011566162\n",
      "Epoch 36/400, Batch 2701/4457, Loss: 1.1654248237609863\n",
      "Epoch 36/400, Batch 2801/4457, Loss: 1.1966726779937744\n",
      "Epoch 36/400, Batch 2901/4457, Loss: 1.1966958045959473\n",
      "Epoch 36/400, Batch 3001/4457, Loss: 1.2209385633468628\n",
      "Epoch 36/400, Batch 3101/4457, Loss: 1.1654222011566162\n",
      "Epoch 36/400, Batch 3201/4457, Loss: 1.1966801881790161\n",
      "Epoch 36/400, Batch 3301/4457, Loss: 1.1654222011566162\n",
      "Epoch 36/400, Batch 3401/4457, Loss: 1.165483832359314\n",
      "Epoch 36/400, Batch 3501/4457, Loss: 1.1654222011566162\n",
      "Epoch 36/400, Batch 3601/4457, Loss: 1.20113205909729\n",
      "Epoch 36/400, Batch 3701/4457, Loss: 1.1654222011566162\n",
      "Epoch 36/400, Batch 3801/4457, Loss: 1.2279224395751953\n",
      "Epoch 36/400, Batch 3901/4457, Loss: 1.2279222011566162\n",
      "Epoch 36/400, Batch 4001/4457, Loss: 1.1966722011566162\n",
      "Epoch 36/400, Batch 4101/4457, Loss: 1.1966723203659058\n",
      "Epoch 36/400, Batch 4201/4457, Loss: 1.1966723203659058\n",
      "Epoch 36/400, Batch 4301/4457, Loss: 1.1966724395751953\n",
      "Epoch 36/400, Batch 4401/4457, Loss: 1.196673035621643\n",
      "Epoch 36/400, Validation Loss: 1.283871069432251\n",
      "Epoch 37/400, Batch 1/4457, Loss: 1.1654244661331177\n",
      "Epoch 37/400, Batch 101/4457, Loss: 1.1966722011566162\n",
      "Epoch 37/400, Batch 201/4457, Loss: 1.1966722011566162\n",
      "Epoch 37/400, Batch 301/4457, Loss: 1.1654224395751953\n",
      "Epoch 37/400, Batch 401/4457, Loss: 1.1654239892959595\n",
      "Epoch 37/400, Batch 501/4457, Loss: 1.165905237197876\n",
      "Epoch 37/400, Batch 601/4457, Loss: 1.196673035621643\n",
      "Epoch 37/400, Batch 701/4457, Loss: 1.1654231548309326\n",
      "Epoch 37/400, Batch 801/4457, Loss: 1.1966731548309326\n",
      "Epoch 37/400, Batch 901/4457, Loss: 1.1657122373580933\n",
      "Epoch 37/400, Batch 1001/4457, Loss: 1.1966723203659058\n",
      "Epoch 37/400, Batch 1101/4457, Loss: 1.1654224395751953\n",
      "Epoch 37/400, Batch 1201/4457, Loss: 1.1957371234893799\n",
      "Epoch 37/400, Batch 1301/4457, Loss: 1.2279233932495117\n",
      "Epoch 37/400, Batch 1401/4457, Loss: 1.1966723203659058\n",
      "Epoch 37/400, Batch 1501/4457, Loss: 1.1966725587844849\n",
      "Epoch 37/400, Batch 1601/4457, Loss: 1.2279224395751953\n",
      "Epoch 37/400, Batch 1701/4457, Loss: 1.1654223203659058\n",
      "Epoch 37/400, Batch 1801/4457, Loss: 1.1654233932495117\n",
      "Epoch 37/400, Batch 1901/4457, Loss: 1.1654225587844849\n",
      "Epoch 37/400, Batch 2001/4457, Loss: 1.1654222011566162\n",
      "Epoch 37/400, Batch 2101/4457, Loss: 1.2279222011566162\n",
      "Epoch 37/400, Batch 2201/4457, Loss: 1.1654222011566162\n",
      "Epoch 37/400, Batch 2301/4457, Loss: 1.1655791997909546\n",
      "Epoch 37/400, Batch 2401/4457, Loss: 1.1654222011566162\n",
      "Epoch 37/400, Batch 2501/4457, Loss: 1.1966725587844849\n",
      "Epoch 37/400, Batch 2601/4457, Loss: 1.1654236316680908\n",
      "Epoch 37/400, Batch 2701/4457, Loss: 1.1966722011566162\n",
      "Epoch 37/400, Batch 2801/4457, Loss: 1.1654226779937744\n",
      "Epoch 37/400, Batch 2901/4457, Loss: 1.1966722011566162\n",
      "Epoch 37/400, Batch 3001/4457, Loss: 1.16542387008667\n",
      "Epoch 37/400, Batch 3101/4457, Loss: 1.2507414817810059\n",
      "Epoch 37/400, Batch 3201/4457, Loss: 1.19696044921875\n",
      "Epoch 37/400, Batch 3301/4457, Loss: 1.1654222011566162\n",
      "Epoch 37/400, Batch 3401/4457, Loss: 1.220947027206421\n",
      "Epoch 37/400, Batch 3501/4457, Loss: 1.1654222011566162\n",
      "Epoch 37/400, Batch 3601/4457, Loss: 1.1964435577392578\n",
      "Epoch 37/400, Batch 3701/4457, Loss: 1.1654236316680908\n",
      "Epoch 37/400, Batch 3801/4457, Loss: 1.1966724395751953\n",
      "Epoch 37/400, Batch 3901/4457, Loss: 1.1966722011566162\n",
      "Epoch 37/400, Batch 4001/4457, Loss: 1.1654222011566162\n",
      "Epoch 37/400, Batch 4101/4457, Loss: 1.1654222011566162\n",
      "Epoch 37/400, Batch 4201/4457, Loss: 1.1654226779937744\n",
      "Epoch 37/400, Batch 4301/4457, Loss: 1.1654222011566162\n",
      "Epoch 37/400, Batch 4401/4457, Loss: 1.1654222011566162\n",
      "Epoch 37/400, Validation Loss: 1.2855481818791419\n",
      "Epoch 38/400, Batch 1/4457, Loss: 1.1967113018035889\n",
      "Epoch 38/400, Batch 101/4457, Loss: 1.1654436588287354\n",
      "Epoch 38/400, Batch 201/4457, Loss: 1.1654222011566162\n",
      "Epoch 38/400, Batch 301/4457, Loss: 1.1967408657073975\n",
      "Epoch 38/400, Batch 401/4457, Loss: 1.1966731548309326\n",
      "Epoch 38/400, Batch 501/4457, Loss: 1.1655042171478271\n",
      "Epoch 38/400, Batch 601/4457, Loss: 1.2591724395751953\n",
      "Epoch 38/400, Batch 701/4457, Loss: 1.165422797203064\n",
      "Epoch 38/400, Batch 801/4457, Loss: 1.1654231548309326\n",
      "Epoch 38/400, Batch 901/4457, Loss: 1.1966934204101562\n",
      "Epoch 38/400, Batch 1001/4457, Loss: 1.1654224395751953\n",
      "Epoch 38/400, Batch 1101/4457, Loss: 1.2221167087554932\n",
      "Epoch 38/400, Batch 1201/4457, Loss: 1.165422797203064\n",
      "Epoch 38/400, Batch 1301/4457, Loss: 1.1654233932495117\n",
      "Epoch 38/400, Batch 1401/4457, Loss: 1.1966739892959595\n",
      "Epoch 38/400, Batch 1501/4457, Loss: 1.1654231548309326\n",
      "Epoch 38/400, Batch 1601/4457, Loss: 1.1656014919281006\n",
      "Epoch 38/400, Batch 1701/4457, Loss: 1.1966737508773804\n",
      "Epoch 38/400, Batch 1801/4457, Loss: 1.2279231548309326\n",
      "Epoch 38/400, Batch 1901/4457, Loss: 1.1686633825302124\n",
      "Epoch 38/400, Batch 2001/4457, Loss: 1.1654229164123535\n",
      "Epoch 38/400, Batch 2101/4457, Loss: 1.165425419807434\n",
      "Epoch 38/400, Batch 2201/4457, Loss: 1.1655229330062866\n",
      "Epoch 38/400, Batch 2301/4457, Loss: 1.2270584106445312\n",
      "Epoch 38/400, Batch 2401/4457, Loss: 1.1654237508773804\n",
      "Epoch 38/400, Batch 2501/4457, Loss: 1.1654223203659058\n",
      "Epoch 38/400, Batch 2601/4457, Loss: 1.1940782070159912\n",
      "Epoch 38/400, Batch 2701/4457, Loss: 1.1654226779937744\n",
      "Epoch 38/400, Batch 2801/4457, Loss: 1.1654226779937744\n",
      "Epoch 38/400, Batch 2901/4457, Loss: 1.1966723203659058\n",
      "Epoch 38/400, Batch 3001/4457, Loss: 1.1654528379440308\n",
      "Epoch 38/400, Batch 3101/4457, Loss: 1.1654226779937744\n",
      "Epoch 38/400, Batch 3201/4457, Loss: 1.165425181388855\n",
      "Epoch 38/400, Batch 3301/4457, Loss: 1.1654231548309326\n",
      "Epoch 38/400, Batch 3401/4457, Loss: 1.1654222011566162\n",
      "Epoch 38/400, Batch 3501/4457, Loss: 1.288898229598999\n",
      "Epoch 38/400, Batch 3601/4457, Loss: 1.2279222011566162\n",
      "Epoch 38/400, Batch 3701/4457, Loss: 1.2279300689697266\n",
      "Epoch 38/400, Batch 3801/4457, Loss: 1.1966750621795654\n",
      "Epoch 38/400, Batch 3901/4457, Loss: 1.165763020515442\n",
      "Epoch 38/400, Batch 4001/4457, Loss: 1.1968142986297607\n",
      "Epoch 38/400, Batch 4101/4457, Loss: 1.1966736316680908\n",
      "Epoch 38/400, Batch 4201/4457, Loss: 1.1966724395751953\n",
      "Epoch 38/400, Batch 4301/4457, Loss: 1.1654222011566162\n",
      "Epoch 38/400, Batch 4401/4457, Loss: 1.1966722011566162\n",
      "Epoch 38/400, Validation Loss: 1.284274714570197\n",
      "Epoch 39/400, Batch 1/4457, Loss: 1.2279231548309326\n",
      "Epoch 39/400, Batch 101/4457, Loss: 1.1969316005706787\n",
      "Epoch 39/400, Batch 201/4457, Loss: 1.1654225587844849\n",
      "Epoch 39/400, Batch 301/4457, Loss: 1.1654253005981445\n",
      "Epoch 39/400, Batch 401/4457, Loss: 1.1966722011566162\n",
      "Epoch 39/400, Batch 501/4457, Loss: 1.1976861953735352\n",
      "Epoch 39/400, Batch 601/4457, Loss: 1.1966722011566162\n",
      "Epoch 39/400, Batch 701/4457, Loss: 1.1654225587844849\n",
      "Epoch 39/400, Batch 801/4457, Loss: 1.1654268503189087\n",
      "Epoch 39/400, Batch 901/4457, Loss: 1.1966893672943115\n",
      "Epoch 39/400, Batch 1001/4457, Loss: 1.1654243469238281\n",
      "Epoch 39/400, Batch 1101/4457, Loss: 1.1654468774795532\n",
      "Epoch 39/400, Batch 1201/4457, Loss: 1.1966822147369385\n",
      "Epoch 39/400, Batch 1301/4457, Loss: 1.1966724395751953\n",
      "Epoch 39/400, Batch 1401/4457, Loss: 1.1654224395751953\n",
      "Epoch 39/400, Batch 1501/4457, Loss: 1.1654229164123535\n",
      "Epoch 39/400, Batch 1601/4457, Loss: 1.1654223203659058\n",
      "Epoch 39/400, Batch 1701/4457, Loss: 1.1966724395751953\n",
      "Epoch 39/400, Batch 1801/4457, Loss: 1.2279223203659058\n",
      "Epoch 39/400, Batch 1901/4457, Loss: 1.1654222011566162\n",
      "Epoch 39/400, Batch 2001/4457, Loss: 1.1661553382873535\n",
      "Epoch 39/400, Batch 2101/4457, Loss: 1.1654224395751953\n",
      "Epoch 39/400, Batch 2201/4457, Loss: 1.1654225587844849\n",
      "Epoch 39/400, Batch 2301/4457, Loss: 1.1966724395751953\n",
      "Epoch 39/400, Batch 2401/4457, Loss: 1.1966723203659058\n",
      "Epoch 39/400, Batch 2501/4457, Loss: 1.1655731201171875\n",
      "Epoch 39/400, Batch 2601/4457, Loss: 1.1654226779937744\n",
      "Epoch 39/400, Batch 2701/4457, Loss: 1.196702003479004\n",
      "Epoch 39/400, Batch 2801/4457, Loss: 1.1654224395751953\n",
      "Epoch 39/400, Batch 2901/4457, Loss: 1.1654222011566162\n",
      "Epoch 39/400, Batch 3001/4457, Loss: 1.1966722011566162\n",
      "Epoch 39/400, Batch 3101/4457, Loss: 1.1654223203659058\n",
      "Epoch 39/400, Batch 3201/4457, Loss: 1.1654232740402222\n",
      "Epoch 39/400, Batch 3301/4457, Loss: 1.2279223203659058\n",
      "Epoch 39/400, Batch 3401/4457, Loss: 1.1654224395751953\n",
      "Epoch 39/400, Batch 3501/4457, Loss: 1.1966722011566162\n",
      "Epoch 39/400, Batch 3601/4457, Loss: 1.1968421936035156\n",
      "Epoch 39/400, Batch 3701/4457, Loss: 1.1966733932495117\n",
      "Epoch 39/400, Batch 3801/4457, Loss: 1.1654298305511475\n",
      "Epoch 39/400, Batch 3901/4457, Loss: 1.165422797203064\n",
      "Epoch 39/400, Batch 4001/4457, Loss: 1.1967328786849976\n",
      "Epoch 39/400, Batch 4101/4457, Loss: 1.1654224395751953\n",
      "Epoch 39/400, Batch 4201/4457, Loss: 1.1966865062713623\n",
      "Epoch 39/400, Batch 4301/4457, Loss: 1.1966722011566162\n",
      "Epoch 39/400, Batch 4401/4457, Loss: 1.1654253005981445\n",
      "Epoch 39/400, Validation Loss: 1.282504937241948\n",
      "Epoch 40/400, Batch 1/4457, Loss: 1.1654225587844849\n",
      "Epoch 40/400, Batch 101/4457, Loss: 1.1654534339904785\n",
      "Epoch 40/400, Batch 201/4457, Loss: 1.1654222011566162\n",
      "Epoch 40/400, Batch 301/4457, Loss: 1.1654224395751953\n",
      "Epoch 40/400, Batch 401/4457, Loss: 1.1961193084716797\n",
      "Epoch 40/400, Batch 501/4457, Loss: 1.1654249429702759\n",
      "Epoch 40/400, Batch 601/4457, Loss: 1.1966259479522705\n",
      "Epoch 40/400, Batch 701/4457, Loss: 1.1656932830810547\n",
      "Epoch 40/400, Batch 801/4457, Loss: 1.1656770706176758\n",
      "Epoch 40/400, Batch 901/4457, Loss: 1.1966722011566162\n",
      "Epoch 40/400, Batch 1001/4457, Loss: 1.1654226779937744\n",
      "Epoch 40/400, Batch 1101/4457, Loss: 1.1966722011566162\n",
      "Epoch 40/400, Batch 1201/4457, Loss: 1.1654224395751953\n",
      "Epoch 40/400, Batch 1301/4457, Loss: 1.2333617210388184\n",
      "Epoch 40/400, Batch 1401/4457, Loss: 1.1967333555221558\n",
      "Epoch 40/400, Batch 1501/4457, Loss: 1.1659647226333618\n",
      "Epoch 40/400, Batch 1601/4457, Loss: 1.2279222011566162\n",
      "Epoch 40/400, Batch 1701/4457, Loss: 1.1654233932495117\n",
      "Epoch 40/400, Batch 1801/4457, Loss: 1.1654222011566162\n",
      "Epoch 40/400, Batch 1901/4457, Loss: 1.1654223203659058\n",
      "Epoch 40/400, Batch 2001/4457, Loss: 1.2279224395751953\n",
      "Epoch 40/400, Batch 2101/4457, Loss: 1.1654407978057861\n",
      "Epoch 40/400, Batch 2201/4457, Loss: 1.1654424667358398\n",
      "Epoch 40/400, Batch 2301/4457, Loss: 1.1654481887817383\n",
      "Epoch 40/400, Batch 2401/4457, Loss: 1.1654223203659058\n",
      "Epoch 40/400, Batch 2501/4457, Loss: 1.1966724395751953\n",
      "Epoch 40/400, Batch 2601/4457, Loss: 1.2279222011566162\n",
      "Epoch 40/400, Batch 2701/4457, Loss: 1.202968716621399\n",
      "Epoch 40/400, Batch 2801/4457, Loss: 1.1966722011566162\n",
      "Epoch 40/400, Batch 2901/4457, Loss: 1.1966722011566162\n",
      "Epoch 40/400, Batch 3001/4457, Loss: 1.1654223203659058\n",
      "Epoch 40/400, Batch 3101/4457, Loss: 1.2591723203659058\n",
      "Epoch 40/400, Batch 3201/4457, Loss: 1.1966722011566162\n",
      "Epoch 40/400, Batch 3301/4457, Loss: 1.1654223203659058\n",
      "Epoch 40/400, Batch 3401/4457, Loss: 1.1966723203659058\n",
      "Epoch 40/400, Batch 3501/4457, Loss: 1.1654222011566162\n",
      "Epoch 40/400, Batch 3601/4457, Loss: 1.1654224395751953\n",
      "Epoch 40/400, Batch 3701/4457, Loss: 1.1654224395751953\n",
      "Epoch 40/400, Batch 3801/4457, Loss: 1.1964216232299805\n",
      "Epoch 40/400, Batch 3901/4457, Loss: 1.1655597686767578\n",
      "Epoch 40/400, Batch 4001/4457, Loss: 1.1654222011566162\n",
      "Epoch 40/400, Batch 4101/4457, Loss: 1.245006799697876\n",
      "Epoch 40/400, Batch 4201/4457, Loss: 1.1654280424118042\n",
      "Epoch 40/400, Batch 4301/4457, Loss: 1.1654493808746338\n",
      "Epoch 40/400, Batch 4401/4457, Loss: 1.1654222011566162\n",
      "Epoch 40/400, Validation Loss: 1.2844894912744325\n",
      "Epoch 41/400, Batch 1/4457, Loss: 1.1966723203659058\n",
      "Epoch 41/400, Batch 101/4457, Loss: 1.1654224395751953\n",
      "Epoch 41/400, Batch 201/4457, Loss: 1.1659905910491943\n",
      "Epoch 41/400, Batch 301/4457, Loss: 1.1685739755630493\n",
      "Epoch 41/400, Batch 401/4457, Loss: 1.1966726779937744\n",
      "Epoch 41/400, Batch 501/4457, Loss: 1.1654222011566162\n",
      "Epoch 41/400, Batch 601/4457, Loss: 1.1966724395751953\n",
      "Epoch 41/400, Batch 701/4457, Loss: 1.1654696464538574\n",
      "Epoch 41/400, Batch 801/4457, Loss: 1.1654222011566162\n",
      "Epoch 41/400, Batch 901/4457, Loss: 1.1654223203659058\n",
      "Epoch 41/400, Batch 1001/4457, Loss: 1.1654229164123535\n",
      "Epoch 41/400, Batch 1101/4457, Loss: 1.1654222011566162\n",
      "Epoch 41/400, Batch 1201/4457, Loss: 1.1654222011566162\n",
      "Epoch 41/400, Batch 1301/4457, Loss: 1.1966722011566162\n",
      "Epoch 41/400, Batch 1401/4457, Loss: 1.1966904401779175\n",
      "Epoch 41/400, Batch 1501/4457, Loss: 1.1966817378997803\n",
      "Epoch 41/400, Batch 1601/4457, Loss: 1.1966722011566162\n",
      "Epoch 41/400, Batch 1701/4457, Loss: 1.1966989040374756\n",
      "Epoch 41/400, Batch 1801/4457, Loss: 1.1654232740402222\n",
      "Epoch 41/400, Batch 1901/4457, Loss: 1.1654224395751953\n",
      "Epoch 41/400, Batch 2001/4457, Loss: 1.1654229164123535\n",
      "Epoch 41/400, Batch 2101/4457, Loss: 1.2591720819473267\n",
      "Epoch 41/400, Batch 2201/4457, Loss: 1.1966722011566162\n",
      "Epoch 41/400, Batch 2301/4457, Loss: 1.1659420728683472\n",
      "Epoch 41/400, Batch 2401/4457, Loss: 1.1654222011566162\n",
      "Epoch 41/400, Batch 2501/4457, Loss: 1.2591822147369385\n",
      "Epoch 41/400, Batch 2601/4457, Loss: 1.1966725587844849\n",
      "Epoch 41/400, Batch 2701/4457, Loss: 1.2279222011566162\n",
      "Epoch 41/400, Batch 2801/4457, Loss: 1.1966735124588013\n",
      "Epoch 41/400, Batch 2901/4457, Loss: 1.1654222011566162\n",
      "Epoch 41/400, Batch 3001/4457, Loss: 1.1966722011566162\n",
      "Epoch 41/400, Batch 3101/4457, Loss: 1.1654224395751953\n",
      "Epoch 41/400, Batch 3201/4457, Loss: 1.1654293537139893\n",
      "Epoch 41/400, Batch 3301/4457, Loss: 1.1966722011566162\n",
      "Epoch 41/400, Batch 3401/4457, Loss: 1.2279222011566162\n",
      "Epoch 41/400, Batch 3501/4457, Loss: 1.1654222011566162\n",
      "Epoch 41/400, Batch 3601/4457, Loss: 1.1966722011566162\n",
      "Epoch 41/400, Batch 3701/4457, Loss: 1.1966736316680908\n",
      "Epoch 41/400, Batch 3801/4457, Loss: 1.1654222011566162\n",
      "Epoch 41/400, Batch 3901/4457, Loss: 1.1654222011566162\n",
      "Epoch 41/400, Batch 4001/4457, Loss: 1.1654223203659058\n",
      "Epoch 41/400, Batch 4101/4457, Loss: 1.1966723203659058\n",
      "Epoch 41/400, Batch 4201/4457, Loss: 1.1654222011566162\n",
      "Epoch 41/400, Batch 4301/4457, Loss: 1.1654222011566162\n",
      "Epoch 41/400, Batch 4401/4457, Loss: 1.1736781597137451\n",
      "Epoch 41/400, Validation Loss: 1.281626284122467\n",
      "Epoch 42/400, Batch 1/4457, Loss: 1.1654223203659058\n",
      "Epoch 42/400, Batch 101/4457, Loss: 1.2237470149993896\n",
      "Epoch 42/400, Batch 201/4457, Loss: 1.1666362285614014\n",
      "Epoch 42/400, Batch 301/4457, Loss: 1.196673035621643\n",
      "Epoch 42/400, Batch 401/4457, Loss: 1.1654222011566162\n",
      "Epoch 42/400, Batch 501/4457, Loss: 1.1966723203659058\n",
      "Epoch 42/400, Batch 601/4457, Loss: 1.1654222011566162\n",
      "Epoch 42/400, Batch 701/4457, Loss: 1.2279225587844849\n",
      "Epoch 42/400, Batch 801/4457, Loss: 1.1966722011566162\n",
      "Epoch 42/400, Batch 901/4457, Loss: 1.1654263734817505\n",
      "Epoch 42/400, Batch 1001/4457, Loss: 1.1654229164123535\n",
      "Epoch 42/400, Batch 1101/4457, Loss: 1.1654245853424072\n",
      "Epoch 42/400, Batch 1201/4457, Loss: 1.1657805442810059\n",
      "Epoch 42/400, Batch 1301/4457, Loss: 1.1966723203659058\n",
      "Epoch 42/400, Batch 1401/4457, Loss: 1.196671962738037\n",
      "Epoch 42/400, Batch 1501/4457, Loss: 1.165474772453308\n",
      "Epoch 42/400, Batch 1601/4457, Loss: 1.1654224395751953\n",
      "Epoch 42/400, Batch 1701/4457, Loss: 1.2591724395751953\n",
      "Epoch 42/400, Batch 1801/4457, Loss: 1.1654222011566162\n",
      "Epoch 42/400, Batch 1901/4457, Loss: 1.2279224395751953\n",
      "Epoch 42/400, Batch 2001/4457, Loss: 1.1654224395751953\n",
      "Epoch 42/400, Batch 2101/4457, Loss: 1.1966040134429932\n",
      "Epoch 42/400, Batch 2201/4457, Loss: 1.1966724395751953\n",
      "Epoch 42/400, Batch 2301/4457, Loss: 1.2280848026275635\n",
      "Epoch 42/400, Batch 2401/4457, Loss: 1.1654239892959595\n",
      "Epoch 42/400, Batch 2501/4457, Loss: 1.1654224395751953\n",
      "Epoch 42/400, Batch 2601/4457, Loss: 1.1654353141784668\n",
      "Epoch 42/400, Batch 2701/4457, Loss: 1.1966686248779297\n",
      "Epoch 42/400, Batch 2801/4457, Loss: 1.165459394454956\n",
      "Epoch 42/400, Batch 2901/4457, Loss: 1.1966743469238281\n",
      "Epoch 42/400, Batch 3001/4457, Loss: 1.1654224395751953\n",
      "Epoch 42/400, Batch 3101/4457, Loss: 1.1654223203659058\n",
      "Epoch 42/400, Batch 3201/4457, Loss: 1.1966657638549805\n",
      "Epoch 42/400, Batch 3301/4457, Loss: 1.1654223203659058\n",
      "Epoch 42/400, Batch 3401/4457, Loss: 1.1966893672943115\n",
      "Epoch 42/400, Batch 3501/4457, Loss: 1.2060315608978271\n",
      "Epoch 42/400, Batch 3601/4457, Loss: 1.1966723203659058\n",
      "Epoch 42/400, Batch 3701/4457, Loss: 1.1654245853424072\n",
      "Epoch 42/400, Batch 3801/4457, Loss: 1.1656986474990845\n",
      "Epoch 42/400, Batch 3901/4457, Loss: 1.1961477994918823\n",
      "Epoch 42/400, Batch 4001/4457, Loss: 1.1966723203659058\n",
      "Epoch 42/400, Batch 4101/4457, Loss: 1.1966726779937744\n",
      "Epoch 42/400, Batch 4201/4457, Loss: 1.2279229164123535\n",
      "Epoch 42/400, Batch 4301/4457, Loss: 1.196679949760437\n",
      "Epoch 42/400, Batch 4401/4457, Loss: 1.2279231548309326\n",
      "Epoch 42/400, Validation Loss: 1.2922414983548816\n",
      "Epoch 43/400, Batch 1/4457, Loss: 1.2277500629425049\n",
      "Epoch 43/400, Batch 101/4457, Loss: 1.1654248237609863\n",
      "Epoch 43/400, Batch 201/4457, Loss: 1.1654223203659058\n",
      "Epoch 43/400, Batch 301/4457, Loss: 1.1658623218536377\n",
      "Epoch 43/400, Batch 401/4457, Loss: 1.1654236316680908\n",
      "Epoch 43/400, Batch 501/4457, Loss: 1.1966723203659058\n",
      "Epoch 43/400, Batch 601/4457, Loss: 1.1654224395751953\n",
      "Epoch 43/400, Batch 701/4457, Loss: 1.196672797203064\n",
      "Epoch 43/400, Batch 801/4457, Loss: 1.1654224395751953\n",
      "Epoch 43/400, Batch 901/4457, Loss: 1.227922797203064\n",
      "Epoch 43/400, Batch 1001/4457, Loss: 1.1966724395751953\n",
      "Epoch 43/400, Batch 1101/4457, Loss: 1.2277007102966309\n",
      "Epoch 43/400, Batch 1201/4457, Loss: 1.1654224395751953\n",
      "Epoch 43/400, Batch 1301/4457, Loss: 1.1966724395751953\n",
      "Epoch 43/400, Batch 1401/4457, Loss: 1.1654231548309326\n",
      "Epoch 43/400, Batch 1501/4457, Loss: 1.1966726779937744\n",
      "Epoch 43/400, Batch 1601/4457, Loss: 1.1654223203659058\n",
      "Epoch 43/400, Batch 1701/4457, Loss: 1.1654231548309326\n",
      "Epoch 43/400, Batch 1801/4457, Loss: 1.1654223203659058\n",
      "Epoch 43/400, Batch 1901/4457, Loss: 1.1654225587844849\n",
      "Epoch 43/400, Batch 2001/4457, Loss: 1.1654269695281982\n",
      "Epoch 43/400, Batch 2101/4457, Loss: 1.1654224395751953\n",
      "Epoch 43/400, Batch 2201/4457, Loss: 1.1966729164123535\n",
      "Epoch 43/400, Batch 2301/4457, Loss: 1.2591743469238281\n",
      "Epoch 43/400, Batch 2401/4457, Loss: 1.1654223203659058\n",
      "Epoch 43/400, Batch 2501/4457, Loss: 1.1654260158538818\n",
      "Epoch 43/400, Batch 2601/4457, Loss: 1.196672797203064\n",
      "Epoch 43/400, Batch 2701/4457, Loss: 1.1966724395751953\n",
      "Epoch 43/400, Batch 2801/4457, Loss: 1.2279222011566162\n",
      "Epoch 43/400, Batch 2901/4457, Loss: 1.1654359102249146\n",
      "Epoch 43/400, Batch 3001/4457, Loss: 1.1935601234436035\n",
      "Epoch 43/400, Batch 3101/4457, Loss: 1.1654927730560303\n",
      "Epoch 43/400, Batch 3201/4457, Loss: 1.1966724395751953\n",
      "Epoch 43/400, Batch 3301/4457, Loss: 1.1654301881790161\n",
      "Epoch 43/400, Batch 3401/4457, Loss: 1.224808931350708\n",
      "Epoch 43/400, Batch 3501/4457, Loss: 1.1654224395751953\n",
      "Epoch 43/400, Batch 3601/4457, Loss: 1.2062511444091797\n",
      "Epoch 43/400, Batch 3701/4457, Loss: 1.1966722011566162\n",
      "Epoch 43/400, Batch 3801/4457, Loss: 1.2279223203659058\n",
      "Epoch 43/400, Batch 3901/4457, Loss: 1.165764331817627\n",
      "Epoch 43/400, Batch 4001/4457, Loss: 1.1654222011566162\n",
      "Epoch 43/400, Batch 4101/4457, Loss: 1.1654223203659058\n",
      "Epoch 43/400, Batch 4201/4457, Loss: 1.1654481887817383\n",
      "Epoch 43/400, Batch 4301/4457, Loss: 1.1654226779937744\n",
      "Epoch 43/400, Batch 4401/4457, Loss: 1.1966722011566162\n",
      "Epoch 43/400, Validation Loss: 1.2810468161626467\n",
      "Epoch 44/400, Batch 1/4457, Loss: 1.1966725587844849\n",
      "Epoch 44/400, Batch 101/4457, Loss: 1.165422797203064\n",
      "Epoch 44/400, Batch 201/4457, Loss: 1.1654233932495117\n",
      "Epoch 44/400, Batch 301/4457, Loss: 1.165424108505249\n",
      "Epoch 44/400, Batch 401/4457, Loss: 1.1966724395751953\n",
      "Epoch 44/400, Batch 501/4457, Loss: 1.1654229164123535\n",
      "Epoch 44/400, Batch 601/4457, Loss: 1.1654226779937744\n",
      "Epoch 44/400, Batch 701/4457, Loss: 1.2591724395751953\n",
      "Epoch 44/400, Batch 801/4457, Loss: 1.1654224395751953\n",
      "Epoch 44/400, Batch 901/4457, Loss: 1.1966722011566162\n",
      "Epoch 44/400, Batch 1001/4457, Loss: 1.2279460430145264\n",
      "Epoch 44/400, Batch 1101/4457, Loss: 1.1654224395751953\n",
      "Epoch 44/400, Batch 1201/4457, Loss: 1.1966725587844849\n",
      "Epoch 44/400, Batch 1301/4457, Loss: 1.2279222011566162\n",
      "Epoch 44/400, Batch 1401/4457, Loss: 1.1966733932495117\n",
      "Epoch 44/400, Batch 1501/4457, Loss: 1.1654231548309326\n",
      "Epoch 44/400, Batch 1601/4457, Loss: 1.2293729782104492\n",
      "Epoch 44/400, Batch 1701/4457, Loss: 1.2279222011566162\n",
      "Epoch 44/400, Batch 1801/4457, Loss: 1.1654272079467773\n",
      "Epoch 44/400, Batch 1901/4457, Loss: 1.165423035621643\n",
      "Epoch 44/400, Batch 2001/4457, Loss: 1.1966722011566162\n",
      "Epoch 44/400, Batch 2101/4457, Loss: 1.1966711282730103\n",
      "Epoch 44/400, Batch 2201/4457, Loss: 1.1966723203659058\n",
      "Epoch 44/400, Batch 2301/4457, Loss: 1.196673035621643\n",
      "Epoch 44/400, Batch 2401/4457, Loss: 1.196845293045044\n",
      "Epoch 44/400, Batch 2501/4457, Loss: 1.2591735124588013\n",
      "Epoch 44/400, Batch 2601/4457, Loss: 1.165423035621643\n",
      "Epoch 44/400, Batch 2701/4457, Loss: 1.1966736316680908\n",
      "Epoch 44/400, Batch 2801/4457, Loss: 1.1654231548309326\n",
      "Epoch 44/400, Batch 2901/4457, Loss: 1.1966705322265625\n",
      "Epoch 44/400, Batch 3001/4457, Loss: 1.1654255390167236\n",
      "Epoch 44/400, Batch 3101/4457, Loss: 1.2279224395751953\n",
      "Epoch 44/400, Batch 3201/4457, Loss: 1.1654263734817505\n",
      "Epoch 44/400, Batch 3301/4457, Loss: 1.1654231548309326\n",
      "Epoch 44/400, Batch 3401/4457, Loss: 1.1966726779937744\n",
      "Epoch 44/400, Batch 3501/4457, Loss: 1.1654242277145386\n",
      "Epoch 44/400, Batch 3601/4457, Loss: 1.2279248237609863\n",
      "Epoch 44/400, Batch 3701/4457, Loss: 1.1654245853424072\n",
      "Epoch 44/400, Batch 3801/4457, Loss: 1.1971492767333984\n",
      "Epoch 44/400, Batch 3901/4457, Loss: 1.2279257774353027\n",
      "Epoch 44/400, Batch 4001/4457, Loss: 1.1654232740402222\n",
      "Epoch 44/400, Batch 4101/4457, Loss: 1.1654229164123535\n",
      "Epoch 44/400, Batch 4201/4457, Loss: 1.1969194412231445\n",
      "Epoch 44/400, Batch 4301/4457, Loss: 1.1966722011566162\n",
      "Epoch 44/400, Batch 4401/4457, Loss: 1.1654257774353027\n",
      "Epoch 44/400, Validation Loss: 1.2923967049471916\n",
      "Epoch 45/400, Batch 1/4457, Loss: 1.1659525632858276\n",
      "Epoch 45/400, Batch 101/4457, Loss: 1.1966753005981445\n",
      "Epoch 45/400, Batch 201/4457, Loss: 1.165512204170227\n",
      "Epoch 45/400, Batch 301/4457, Loss: 1.1654250621795654\n",
      "Epoch 45/400, Batch 401/4457, Loss: 1.1654267311096191\n",
      "Epoch 45/400, Batch 501/4457, Loss: 1.1966729164123535\n",
      "Epoch 45/400, Batch 601/4457, Loss: 1.16542387008667\n",
      "Epoch 45/400, Batch 701/4457, Loss: 1.1967607736587524\n",
      "Epoch 45/400, Batch 801/4457, Loss: 1.1654248237609863\n",
      "Epoch 45/400, Batch 901/4457, Loss: 1.1654235124588013\n",
      "Epoch 45/400, Batch 1001/4457, Loss: 1.1966723203659058\n",
      "Epoch 45/400, Batch 1101/4457, Loss: 1.1656404733657837\n",
      "Epoch 45/400, Batch 1201/4457, Loss: 1.1654980182647705\n",
      "Epoch 45/400, Batch 1301/4457, Loss: 1.227917194366455\n",
      "Epoch 45/400, Batch 1401/4457, Loss: 1.1654223203659058\n",
      "Epoch 45/400, Batch 1501/4457, Loss: 1.1654460430145264\n",
      "Epoch 45/400, Batch 1601/4457, Loss: 1.1966753005981445\n",
      "Epoch 45/400, Batch 1701/4457, Loss: 1.2279216051101685\n",
      "Epoch 45/400, Batch 1801/4457, Loss: 1.1654224395751953\n",
      "Epoch 45/400, Batch 1901/4457, Loss: 1.16542387008667\n",
      "Epoch 45/400, Batch 2001/4457, Loss: 1.1654239892959595\n",
      "Epoch 45/400, Batch 2101/4457, Loss: 1.1654226779937744\n",
      "Epoch 45/400, Batch 2201/4457, Loss: 1.1654284000396729\n",
      "Epoch 45/400, Batch 2301/4457, Loss: 1.1654303073883057\n",
      "Epoch 45/400, Batch 2401/4457, Loss: 1.196674108505249\n",
      "Epoch 45/400, Batch 2501/4457, Loss: 1.1654226779937744\n",
      "Epoch 45/400, Batch 2601/4457, Loss: 1.1654222011566162\n",
      "Epoch 45/400, Batch 2701/4457, Loss: 1.1654224395751953\n",
      "Epoch 45/400, Batch 2801/4457, Loss: 1.1654222011566162\n",
      "Epoch 45/400, Batch 2901/4457, Loss: 1.1654223203659058\n",
      "Epoch 45/400, Batch 3001/4457, Loss: 1.1980316638946533\n",
      "Epoch 45/400, Batch 3101/4457, Loss: 1.1654222011566162\n",
      "Epoch 45/400, Batch 3201/4457, Loss: 1.1966724395751953\n",
      "Epoch 45/400, Batch 3301/4457, Loss: 1.2279224395751953\n",
      "Epoch 45/400, Batch 3401/4457, Loss: 1.1966724395751953\n",
      "Epoch 45/400, Batch 3501/4457, Loss: 1.165433406829834\n",
      "Epoch 45/400, Batch 3601/4457, Loss: 1.1966724395751953\n",
      "Epoch 45/400, Batch 3701/4457, Loss: 1.22792649269104\n",
      "Epoch 45/400, Batch 3801/4457, Loss: 1.2279222011566162\n",
      "Epoch 45/400, Batch 3901/4457, Loss: 1.1654877662658691\n",
      "Epoch 45/400, Batch 4001/4457, Loss: 1.2279226779937744\n",
      "Epoch 45/400, Batch 4101/4457, Loss: 1.1654229164123535\n",
      "Epoch 45/400, Batch 4201/4457, Loss: 1.1966722011566162\n",
      "Epoch 45/400, Batch 4301/4457, Loss: 1.1966724395751953\n",
      "Epoch 45/400, Batch 4401/4457, Loss: 1.1654382944107056\n",
      "Epoch 45/400, Validation Loss: 1.2810537463852338\n",
      "Epoch 46/400, Batch 1/4457, Loss: 1.1655205488204956\n",
      "Epoch 46/400, Batch 101/4457, Loss: 1.1966785192489624\n",
      "Epoch 46/400, Batch 201/4457, Loss: 1.2279225587844849\n",
      "Epoch 46/400, Batch 301/4457, Loss: 1.188519835472107\n",
      "Epoch 46/400, Batch 401/4457, Loss: 1.196686863899231\n",
      "Epoch 46/400, Batch 501/4457, Loss: 1.1654223203659058\n",
      "Epoch 46/400, Batch 601/4457, Loss: 1.1677918434143066\n",
      "Epoch 46/400, Batch 701/4457, Loss: 1.2010350227355957\n",
      "Epoch 46/400, Batch 801/4457, Loss: 1.1972289085388184\n",
      "Epoch 46/400, Batch 901/4457, Loss: 1.2014750242233276\n",
      "Epoch 46/400, Batch 1001/4457, Loss: 1.1974329948425293\n",
      "Epoch 46/400, Batch 1101/4457, Loss: 1.1654239892959595\n",
      "Epoch 46/400, Batch 1201/4457, Loss: 1.1654222011566162\n",
      "Epoch 46/400, Batch 1301/4457, Loss: 1.1706475019454956\n",
      "Epoch 46/400, Batch 1401/4457, Loss: 1.1654224395751953\n",
      "Epoch 46/400, Batch 1501/4457, Loss: 1.1654226779937744\n",
      "Epoch 46/400, Batch 1601/4457, Loss: 1.1966722011566162\n",
      "Epoch 46/400, Batch 1701/4457, Loss: 1.165440559387207\n",
      "Epoch 46/400, Batch 1801/4457, Loss: 1.2279465198516846\n",
      "Epoch 46/400, Batch 1901/4457, Loss: 1.1654634475708008\n",
      "Epoch 46/400, Batch 2001/4457, Loss: 1.1654231548309326\n",
      "Epoch 46/400, Batch 2101/4457, Loss: 1.196675181388855\n",
      "Epoch 46/400, Batch 2201/4457, Loss: 1.1654386520385742\n",
      "Epoch 46/400, Batch 2301/4457, Loss: 1.1656794548034668\n",
      "Epoch 46/400, Batch 2401/4457, Loss: 1.165460467338562\n",
      "Epoch 46/400, Batch 2501/4457, Loss: 1.1966779232025146\n",
      "Epoch 46/400, Batch 2601/4457, Loss: 1.165425419807434\n",
      "Epoch 46/400, Batch 2701/4457, Loss: 1.1966536045074463\n",
      "Epoch 46/400, Batch 2801/4457, Loss: 1.174980878829956\n",
      "Epoch 46/400, Batch 2901/4457, Loss: 1.2192745208740234\n",
      "Epoch 46/400, Batch 3001/4457, Loss: 1.1966725587844849\n",
      "Epoch 46/400, Batch 3101/4457, Loss: 1.1966722011566162\n",
      "Epoch 46/400, Batch 3201/4457, Loss: 1.1966636180877686\n",
      "Epoch 46/400, Batch 3301/4457, Loss: 1.1720536947250366\n",
      "Epoch 46/400, Batch 3401/4457, Loss: 1.1966739892959595\n",
      "Epoch 46/400, Batch 3501/4457, Loss: 1.1654224395751953\n",
      "Epoch 46/400, Batch 3601/4457, Loss: 1.165449857711792\n",
      "Epoch 46/400, Batch 3701/4457, Loss: 1.1654231548309326\n",
      "Epoch 46/400, Batch 3801/4457, Loss: 1.1654253005981445\n",
      "Epoch 46/400, Batch 3901/4457, Loss: 1.1966696977615356\n",
      "Epoch 46/400, Batch 4001/4457, Loss: 1.1967027187347412\n",
      "Epoch 46/400, Batch 4101/4457, Loss: 1.165435552597046\n",
      "Epoch 46/400, Batch 4201/4457, Loss: 1.259039282798767\n",
      "Epoch 46/400, Batch 4301/4457, Loss: 1.2278999090194702\n",
      "Epoch 46/400, Batch 4401/4457, Loss: 1.2260500192642212\n",
      "Epoch 46/400, Validation Loss: 1.2827802988744916\n",
      "Epoch 47/400, Batch 1/4457, Loss: 1.1972146034240723\n",
      "Epoch 47/400, Batch 101/4457, Loss: 1.1654738187789917\n",
      "Epoch 47/400, Batch 201/4457, Loss: 1.1965956687927246\n",
      "Epoch 47/400, Batch 301/4457, Loss: 1.1742802858352661\n",
      "Epoch 47/400, Batch 401/4457, Loss: 1.1860244274139404\n",
      "Epoch 47/400, Batch 501/4457, Loss: 1.165582537651062\n",
      "Epoch 47/400, Batch 601/4457, Loss: 1.227001428604126\n",
      "Epoch 47/400, Batch 701/4457, Loss: 1.196683406829834\n",
      "Epoch 47/400, Batch 801/4457, Loss: 1.198773741722107\n",
      "Epoch 47/400, Batch 901/4457, Loss: 1.2278075218200684\n",
      "Epoch 47/400, Batch 1001/4457, Loss: 1.2279002666473389\n",
      "Epoch 47/400, Batch 1101/4457, Loss: 1.1948848962783813\n",
      "Epoch 47/400, Batch 1201/4457, Loss: 1.1654225587844849\n",
      "Epoch 47/400, Batch 1301/4457, Loss: 1.1972031593322754\n",
      "Epoch 47/400, Batch 1401/4457, Loss: 1.1955759525299072\n",
      "Epoch 47/400, Batch 1501/4457, Loss: 1.165459394454956\n",
      "Epoch 47/400, Batch 1601/4457, Loss: 1.1654224395751953\n",
      "Epoch 47/400, Batch 1701/4457, Loss: 1.1654224395751953\n",
      "Epoch 47/400, Batch 1801/4457, Loss: 1.1966722011566162\n",
      "Epoch 47/400, Batch 1901/4457, Loss: 1.1654222011566162\n",
      "Epoch 47/400, Batch 2001/4457, Loss: 1.165423035621643\n",
      "Epoch 47/400, Batch 2101/4457, Loss: 1.1966742277145386\n",
      "Epoch 47/400, Batch 2201/4457, Loss: 1.1966724395751953\n",
      "Epoch 47/400, Batch 2301/4457, Loss: 1.1654229164123535\n",
      "Epoch 47/400, Batch 2401/4457, Loss: 1.1654222011566162\n",
      "Epoch 47/400, Batch 2501/4457, Loss: 1.1654222011566162\n",
      "Epoch 47/400, Batch 2601/4457, Loss: 1.1718707084655762\n",
      "Epoch 47/400, Batch 2701/4457, Loss: 1.1654243469238281\n",
      "Epoch 47/400, Batch 2801/4457, Loss: 1.196671724319458\n",
      "Epoch 47/400, Batch 2901/4457, Loss: 1.1654491424560547\n",
      "Epoch 47/400, Batch 3001/4457, Loss: 1.2279245853424072\n",
      "Epoch 47/400, Batch 3101/4457, Loss: 1.1968567371368408\n",
      "Epoch 47/400, Batch 3201/4457, Loss: 1.1654223203659058\n",
      "Epoch 47/400, Batch 3301/4457, Loss: 1.1654237508773804\n",
      "Epoch 47/400, Batch 3401/4457, Loss: 1.1966724395751953\n",
      "Epoch 47/400, Batch 3501/4457, Loss: 1.1966722011566162\n",
      "Epoch 47/400, Batch 3601/4457, Loss: 1.1966722011566162\n",
      "Epoch 47/400, Batch 3701/4457, Loss: 1.1737269163131714\n",
      "Epoch 47/400, Batch 3801/4457, Loss: 1.1966726779937744\n",
      "Epoch 47/400, Batch 3901/4457, Loss: 1.1966723203659058\n",
      "Epoch 47/400, Batch 4001/4457, Loss: 1.1966853141784668\n",
      "Epoch 47/400, Batch 4101/4457, Loss: 1.1966723203659058\n",
      "Epoch 47/400, Batch 4201/4457, Loss: 1.1966725587844849\n",
      "Epoch 47/400, Batch 4301/4457, Loss: 1.197082757949829\n",
      "Epoch 47/400, Batch 4401/4457, Loss: 1.1966791152954102\n",
      "Epoch 47/400, Validation Loss: 1.2801143271582467\n",
      "Epoch 48/400, Batch 1/4457, Loss: 1.1966723203659058\n",
      "Epoch 48/400, Batch 101/4457, Loss: 1.2461719512939453\n",
      "Epoch 48/400, Batch 201/4457, Loss: 1.195756196975708\n",
      "Epoch 48/400, Batch 301/4457, Loss: 1.1654225587844849\n",
      "Epoch 48/400, Batch 401/4457, Loss: 1.1654231548309326\n",
      "Epoch 48/400, Batch 501/4457, Loss: 1.1952462196350098\n",
      "Epoch 48/400, Batch 601/4457, Loss: 1.1966763734817505\n",
      "Epoch 48/400, Batch 701/4457, Loss: 1.195860743522644\n",
      "Epoch 48/400, Batch 801/4457, Loss: 1.1967060565948486\n",
      "Epoch 48/400, Batch 901/4457, Loss: 1.2141624689102173\n",
      "Epoch 48/400, Batch 1001/4457, Loss: 1.165442705154419\n",
      "Epoch 48/400, Batch 1101/4457, Loss: 1.1654226779937744\n",
      "Epoch 48/400, Batch 1201/4457, Loss: 1.1966729164123535\n",
      "Epoch 48/400, Batch 1301/4457, Loss: 1.253104567527771\n",
      "Epoch 48/400, Batch 1401/4457, Loss: 1.1654244661331177\n",
      "Epoch 48/400, Batch 1501/4457, Loss: 1.1654350757598877\n",
      "Epoch 48/400, Batch 1601/4457, Loss: 1.1654224395751953\n",
      "Epoch 48/400, Batch 1701/4457, Loss: 1.1660394668579102\n",
      "Epoch 48/400, Batch 1801/4457, Loss: 1.1655890941619873\n",
      "Epoch 48/400, Batch 1901/4457, Loss: 1.1654258966445923\n",
      "Epoch 48/400, Batch 2001/4457, Loss: 1.1654222011566162\n",
      "Epoch 48/400, Batch 2101/4457, Loss: 1.1654505729675293\n",
      "Epoch 48/400, Batch 2201/4457, Loss: 1.1654237508773804\n",
      "Epoch 48/400, Batch 2301/4457, Loss: 1.1654331684112549\n",
      "Epoch 48/400, Batch 2401/4457, Loss: 1.165504813194275\n",
      "Epoch 48/400, Batch 2501/4457, Loss: 1.1966758966445923\n",
      "Epoch 48/400, Batch 2601/4457, Loss: 1.1657748222351074\n",
      "Epoch 48/400, Batch 2701/4457, Loss: 1.1749451160430908\n",
      "Epoch 48/400, Batch 2801/4457, Loss: 1.1966946125030518\n",
      "Epoch 48/400, Batch 2901/4457, Loss: 1.1654226779937744\n",
      "Epoch 48/400, Batch 3001/4457, Loss: 1.2241045236587524\n",
      "Epoch 48/400, Batch 3101/4457, Loss: 1.1654239892959595\n",
      "Epoch 48/400, Batch 3201/4457, Loss: 1.196595311164856\n",
      "Epoch 48/400, Batch 3301/4457, Loss: 1.196677803993225\n",
      "Epoch 48/400, Batch 3401/4457, Loss: 1.1825342178344727\n",
      "Epoch 48/400, Batch 3501/4457, Loss: 1.227890968322754\n",
      "Epoch 48/400, Batch 3601/4457, Loss: 1.1966720819473267\n",
      "Epoch 48/400, Batch 3701/4457, Loss: 1.1655423641204834\n",
      "Epoch 48/400, Batch 3801/4457, Loss: 1.1662483215332031\n",
      "Epoch 48/400, Batch 3901/4457, Loss: 1.1657500267028809\n",
      "Epoch 48/400, Batch 4001/4457, Loss: 1.1654455661773682\n",
      "Epoch 48/400, Batch 4101/4457, Loss: 1.196672797203064\n",
      "Epoch 48/400, Batch 4201/4457, Loss: 1.1966722011566162\n",
      "Epoch 48/400, Batch 4301/4457, Loss: 1.2220258712768555\n",
      "Epoch 48/400, Batch 4401/4457, Loss: 1.1654263734817505\n",
      "Epoch 48/400, Validation Loss: 1.2802517702655187\n",
      "Epoch 49/400, Batch 1/4457, Loss: 1.1654726266860962\n",
      "Epoch 49/400, Batch 101/4457, Loss: 1.1654223203659058\n",
      "Epoch 49/400, Batch 201/4457, Loss: 1.197086215019226\n",
      "Epoch 49/400, Batch 301/4457, Loss: 1.1654222011566162\n",
      "Epoch 49/400, Batch 401/4457, Loss: 1.1673638820648193\n",
      "Epoch 49/400, Batch 501/4457, Loss: 1.165422797203064\n",
      "Epoch 49/400, Batch 601/4457, Loss: 1.1654374599456787\n",
      "Epoch 49/400, Batch 701/4457, Loss: 1.1654233932495117\n",
      "Epoch 49/400, Batch 801/4457, Loss: 1.176133394241333\n",
      "Epoch 49/400, Batch 901/4457, Loss: 1.1654223203659058\n",
      "Epoch 49/400, Batch 1001/4457, Loss: 1.165513515472412\n",
      "Epoch 49/400, Batch 1101/4457, Loss: 1.2184890508651733\n",
      "Epoch 49/400, Batch 1201/4457, Loss: 1.1654231548309326\n",
      "Epoch 49/400, Batch 1301/4457, Loss: 1.219639778137207\n",
      "Epoch 49/400, Batch 1401/4457, Loss: 1.1654534339904785\n",
      "Epoch 49/400, Batch 1501/4457, Loss: 1.1966367959976196\n",
      "Epoch 49/400, Batch 1601/4457, Loss: 1.1654233932495117\n",
      "Epoch 49/400, Batch 1701/4457, Loss: 1.1654233932495117\n",
      "Epoch 49/400, Batch 1801/4457, Loss: 1.165422797203064\n",
      "Epoch 49/400, Batch 1901/4457, Loss: 1.1654298305511475\n",
      "Epoch 49/400, Batch 2001/4457, Loss: 1.1950671672821045\n",
      "Epoch 49/400, Batch 2101/4457, Loss: 1.165428876876831\n",
      "Epoch 49/400, Batch 2201/4457, Loss: 1.1963329315185547\n",
      "Epoch 49/400, Batch 2301/4457, Loss: 1.2280340194702148\n",
      "Epoch 49/400, Batch 2401/4457, Loss: 1.2279224395751953\n",
      "Epoch 49/400, Batch 2501/4457, Loss: 1.1966931819915771\n",
      "Epoch 49/400, Batch 2601/4457, Loss: 1.1654472351074219\n",
      "Epoch 49/400, Batch 2701/4457, Loss: 1.1654224395751953\n",
      "Epoch 49/400, Batch 2801/4457, Loss: 1.1655327081680298\n",
      "Epoch 49/400, Batch 2901/4457, Loss: 1.1654235124588013\n",
      "Epoch 49/400, Batch 3001/4457, Loss: 1.1654226779937744\n",
      "Epoch 49/400, Batch 3101/4457, Loss: 1.165446400642395\n",
      "Epoch 49/400, Batch 3201/4457, Loss: 1.16542387008667\n",
      "Epoch 49/400, Batch 3301/4457, Loss: 1.165915846824646\n",
      "Epoch 49/400, Batch 3401/4457, Loss: 1.1654300689697266\n",
      "Epoch 49/400, Batch 3501/4457, Loss: 1.1654231548309326\n",
      "Epoch 49/400, Batch 3601/4457, Loss: 1.1963841915130615\n",
      "Epoch 49/400, Batch 3701/4457, Loss: 1.1654225587844849\n",
      "Epoch 49/400, Batch 3801/4457, Loss: 1.1966726779937744\n",
      "Epoch 49/400, Batch 3901/4457, Loss: 1.1654293537139893\n",
      "Epoch 49/400, Batch 4001/4457, Loss: 1.1654222011566162\n",
      "Epoch 49/400, Batch 4101/4457, Loss: 1.1658580303192139\n",
      "Epoch 49/400, Batch 4201/4457, Loss: 1.1654282808303833\n",
      "Epoch 49/400, Batch 4301/4457, Loss: 1.1654226779937744\n",
      "Epoch 49/400, Batch 4401/4457, Loss: 1.1966813802719116\n",
      "Epoch 49/400, Validation Loss: 1.2813564781395217\n",
      "Epoch 50/400, Batch 1/4457, Loss: 1.1656527519226074\n",
      "Epoch 50/400, Batch 101/4457, Loss: 1.1966509819030762\n",
      "Epoch 50/400, Batch 201/4457, Loss: 1.1654224395751953\n",
      "Epoch 50/400, Batch 301/4457, Loss: 1.1654620170593262\n",
      "Epoch 50/400, Batch 401/4457, Loss: 1.16672682762146\n",
      "Epoch 50/400, Batch 501/4457, Loss: 1.167710781097412\n",
      "Epoch 50/400, Batch 601/4457, Loss: 1.1966723203659058\n",
      "Epoch 50/400, Batch 701/4457, Loss: 1.1654961109161377\n",
      "Epoch 50/400, Batch 801/4457, Loss: 1.1980539560317993\n",
      "Epoch 50/400, Batch 901/4457, Loss: 1.165422797203064\n",
      "Epoch 50/400, Batch 1001/4457, Loss: 1.1654223203659058\n",
      "Epoch 50/400, Batch 1101/4457, Loss: 1.1654242277145386\n",
      "Epoch 50/400, Batch 1201/4457, Loss: 1.196671962738037\n",
      "Epoch 50/400, Batch 1301/4457, Loss: 1.1966724395751953\n",
      "Epoch 50/400, Batch 1401/4457, Loss: 1.1654233932495117\n",
      "Epoch 50/400, Batch 1501/4457, Loss: 1.1654349565505981\n",
      "Epoch 50/400, Batch 1601/4457, Loss: 1.1654224395751953\n",
      "Epoch 50/400, Batch 1701/4457, Loss: 1.1654222011566162\n",
      "Epoch 50/400, Batch 1801/4457, Loss: 1.1672790050506592\n",
      "Epoch 50/400, Batch 1901/4457, Loss: 1.196687936782837\n",
      "Epoch 50/400, Batch 2001/4457, Loss: 1.1654226779937744\n",
      "Epoch 50/400, Batch 2101/4457, Loss: 1.196813702583313\n",
      "Epoch 50/400, Batch 2201/4457, Loss: 1.1966733932495117\n",
      "Epoch 50/400, Batch 2301/4457, Loss: 1.1711128950119019\n",
      "Epoch 50/400, Batch 2401/4457, Loss: 1.165425419807434\n",
      "Epoch 50/400, Batch 2501/4457, Loss: 1.165422797203064\n",
      "Epoch 50/400, Batch 2601/4457, Loss: 1.1966724395751953\n",
      "Epoch 50/400, Batch 2701/4457, Loss: 1.1968014240264893\n",
      "Epoch 50/400, Batch 2801/4457, Loss: 1.1654701232910156\n",
      "Epoch 50/400, Batch 2901/4457, Loss: 1.1655685901641846\n",
      "Epoch 50/400, Batch 3001/4457, Loss: 1.165442705154419\n",
      "Epoch 50/400, Batch 3101/4457, Loss: 1.165422797203064\n",
      "Epoch 50/400, Batch 3201/4457, Loss: 1.1654257774353027\n",
      "Epoch 50/400, Batch 3301/4457, Loss: 1.1654224395751953\n",
      "Epoch 50/400, Batch 3401/4457, Loss: 1.196685791015625\n",
      "Epoch 50/400, Batch 3501/4457, Loss: 1.1966590881347656\n",
      "Epoch 50/400, Batch 3601/4457, Loss: 1.19667387008667\n",
      "Epoch 50/400, Batch 3701/4457, Loss: 1.1654258966445923\n",
      "Epoch 50/400, Batch 3801/4457, Loss: 1.1660523414611816\n",
      "Epoch 50/400, Batch 3901/4457, Loss: 1.1654300689697266\n",
      "Epoch 50/400, Batch 4001/4457, Loss: 1.1654736995697021\n",
      "Epoch 50/400, Batch 4101/4457, Loss: 1.196676254272461\n",
      "Epoch 50/400, Batch 4201/4457, Loss: 1.1657572984695435\n",
      "Epoch 50/400, Batch 4301/4457, Loss: 1.1966724395751953\n",
      "Epoch 50/400, Batch 4401/4457, Loss: 1.1656320095062256\n",
      "Epoch 50/400, Validation Loss: 1.2824613802016727\n",
      "Epoch 51/400, Batch 1/4457, Loss: 1.1654331684112549\n",
      "Epoch 51/400, Batch 101/4457, Loss: 1.1654958724975586\n",
      "Epoch 51/400, Batch 201/4457, Loss: 1.165457844734192\n",
      "Epoch 51/400, Batch 301/4457, Loss: 1.1664904356002808\n",
      "Epoch 51/400, Batch 401/4457, Loss: 1.1966756582260132\n",
      "Epoch 51/400, Batch 501/4457, Loss: 1.1654398441314697\n",
      "Epoch 51/400, Batch 601/4457, Loss: 1.1654369831085205\n",
      "Epoch 51/400, Batch 701/4457, Loss: 1.1950633525848389\n",
      "Epoch 51/400, Batch 801/4457, Loss: 1.1654224395751953\n",
      "Epoch 51/400, Batch 901/4457, Loss: 1.1966729164123535\n",
      "Epoch 51/400, Batch 1001/4457, Loss: 1.1771962642669678\n",
      "Epoch 51/400, Batch 1101/4457, Loss: 1.1963615417480469\n",
      "Epoch 51/400, Batch 1201/4457, Loss: 1.1654224395751953\n",
      "Epoch 51/400, Batch 1301/4457, Loss: 1.1654224395751953\n",
      "Epoch 51/400, Batch 1401/4457, Loss: 1.1654223203659058\n",
      "Epoch 51/400, Batch 1501/4457, Loss: 1.1657270193099976\n",
      "Epoch 51/400, Batch 1601/4457, Loss: 1.1654224395751953\n",
      "Epoch 51/400, Batch 1701/4457, Loss: 1.1654243469238281\n",
      "Epoch 51/400, Batch 1801/4457, Loss: 1.1654222011566162\n",
      "Epoch 51/400, Batch 1901/4457, Loss: 1.1654274463653564\n",
      "Epoch 51/400, Batch 2001/4457, Loss: 1.1966726779937744\n",
      "Epoch 51/400, Batch 2101/4457, Loss: 1.1655311584472656\n",
      "Epoch 51/400, Batch 2201/4457, Loss: 1.165581226348877\n",
      "Epoch 51/400, Batch 2301/4457, Loss: 1.1739823818206787\n",
      "Epoch 51/400, Batch 2401/4457, Loss: 1.1654232740402222\n",
      "Epoch 51/400, Batch 2501/4457, Loss: 1.1966760158538818\n",
      "Epoch 51/400, Batch 2601/4457, Loss: 1.1654223203659058\n",
      "Epoch 51/400, Batch 2701/4457, Loss: 1.1682088375091553\n",
      "Epoch 51/400, Batch 2801/4457, Loss: 1.1654587984085083\n",
      "Epoch 51/400, Batch 2901/4457, Loss: 1.16542387008667\n",
      "Epoch 51/400, Batch 3001/4457, Loss: 1.1654243469238281\n",
      "Epoch 51/400, Batch 3101/4457, Loss: 1.2278759479522705\n",
      "Epoch 51/400, Batch 3201/4457, Loss: 1.1654224395751953\n",
      "Epoch 51/400, Batch 3301/4457, Loss: 1.1654222011566162\n",
      "Epoch 51/400, Batch 3401/4457, Loss: 1.1966856718063354\n",
      "Epoch 51/400, Batch 3501/4457, Loss: 1.1654224395751953\n",
      "Epoch 51/400, Batch 3601/4457, Loss: 1.1654224395751953\n",
      "Epoch 51/400, Batch 3701/4457, Loss: 1.1973427534103394\n",
      "Epoch 51/400, Batch 3801/4457, Loss: 1.165423035621643\n",
      "Epoch 51/400, Batch 3901/4457, Loss: 1.165427803993225\n",
      "Epoch 51/400, Batch 4001/4457, Loss: 1.1966769695281982\n",
      "Epoch 51/400, Batch 4101/4457, Loss: 1.1934021711349487\n",
      "Epoch 51/400, Batch 4201/4457, Loss: 1.1654281616210938\n",
      "Epoch 51/400, Batch 4301/4457, Loss: 1.1654226779937744\n",
      "Epoch 51/400, Batch 4401/4457, Loss: 1.1966688632965088\n",
      "Epoch 51/400, Validation Loss: 1.2827800143332708\n",
      "Epoch 52/400, Batch 1/4457, Loss: 1.165423035621643\n",
      "Epoch 52/400, Batch 101/4457, Loss: 1.1654250621795654\n",
      "Epoch 52/400, Batch 201/4457, Loss: 1.1654248237609863\n",
      "Epoch 52/400, Batch 301/4457, Loss: 1.165435552597046\n",
      "Epoch 52/400, Batch 401/4457, Loss: 1.1654237508773804\n",
      "Epoch 52/400, Batch 501/4457, Loss: 1.1654243469238281\n",
      "Epoch 52/400, Batch 601/4457, Loss: 1.2614119052886963\n",
      "Epoch 52/400, Batch 701/4457, Loss: 1.1654272079467773\n",
      "Epoch 52/400, Batch 801/4457, Loss: 1.1962348222732544\n",
      "Epoch 52/400, Batch 901/4457, Loss: 1.16542387008667\n",
      "Epoch 52/400, Batch 1001/4457, Loss: 1.1655226945877075\n",
      "Epoch 52/400, Batch 1101/4457, Loss: 1.1654250621795654\n",
      "Epoch 52/400, Batch 1201/4457, Loss: 1.1654276847839355\n",
      "Epoch 52/400, Batch 1301/4457, Loss: 1.1654468774795532\n",
      "Epoch 52/400, Batch 1401/4457, Loss: 1.1654224395751953\n",
      "Epoch 52/400, Batch 1501/4457, Loss: 1.2279667854309082\n",
      "Epoch 52/400, Batch 1601/4457, Loss: 1.1966729164123535\n",
      "Epoch 52/400, Batch 1701/4457, Loss: 1.1966733932495117\n",
      "Epoch 52/400, Batch 1801/4457, Loss: 1.1654223203659058\n",
      "Epoch 52/400, Batch 1901/4457, Loss: 1.1654224395751953\n",
      "Epoch 52/400, Batch 2001/4457, Loss: 1.1654222011566162\n",
      "Epoch 52/400, Batch 2101/4457, Loss: 1.165434718132019\n",
      "Epoch 52/400, Batch 2201/4457, Loss: 1.1654226779937744\n",
      "Epoch 52/400, Batch 2301/4457, Loss: 1.1654280424118042\n",
      "Epoch 52/400, Batch 2401/4457, Loss: 1.1654223203659058\n",
      "Epoch 52/400, Batch 2501/4457, Loss: 1.1966181993484497\n",
      "Epoch 52/400, Batch 2601/4457, Loss: 1.1654226779937744\n",
      "Epoch 52/400, Batch 2701/4457, Loss: 1.196877360343933\n",
      "Epoch 52/400, Batch 2801/4457, Loss: 1.165872573852539\n",
      "Epoch 52/400, Batch 2901/4457, Loss: 1.1967488527297974\n",
      "Epoch 52/400, Batch 3001/4457, Loss: 1.1654239892959595\n",
      "Epoch 52/400, Batch 3101/4457, Loss: 1.1966726779937744\n",
      "Epoch 52/400, Batch 3201/4457, Loss: 1.1654223203659058\n",
      "Epoch 52/400, Batch 3301/4457, Loss: 1.1654245853424072\n",
      "Epoch 52/400, Batch 3401/4457, Loss: 1.165828824043274\n",
      "Epoch 52/400, Batch 3501/4457, Loss: 1.196686029434204\n",
      "Epoch 52/400, Batch 3601/4457, Loss: 1.165426254272461\n",
      "Epoch 52/400, Batch 3701/4457, Loss: 1.2279212474822998\n",
      "Epoch 52/400, Batch 3801/4457, Loss: 1.1654376983642578\n",
      "Epoch 52/400, Batch 3901/4457, Loss: 1.1654222011566162\n",
      "Epoch 52/400, Batch 4001/4457, Loss: 1.1654223203659058\n",
      "Epoch 52/400, Batch 4101/4457, Loss: 1.1902695894241333\n",
      "Epoch 52/400, Batch 4201/4457, Loss: 1.1654289960861206\n",
      "Epoch 52/400, Batch 4301/4457, Loss: 1.1654348373413086\n",
      "Epoch 52/400, Batch 4401/4457, Loss: 1.1966722011566162\n",
      "Epoch 52/400, Validation Loss: 1.2882184866401885\n",
      "Epoch 53/400, Batch 1/4457, Loss: 1.1654232740402222\n",
      "Epoch 53/400, Batch 101/4457, Loss: 1.1654679775238037\n",
      "Epoch 53/400, Batch 201/4457, Loss: 1.1654324531555176\n",
      "Epoch 53/400, Batch 301/4457, Loss: 1.1654223203659058\n",
      "Epoch 53/400, Batch 401/4457, Loss: 1.1654229164123535\n",
      "Epoch 53/400, Batch 501/4457, Loss: 1.1654285192489624\n",
      "Epoch 53/400, Batch 601/4457, Loss: 1.1654399633407593\n",
      "Epoch 53/400, Batch 701/4457, Loss: 1.1654362678527832\n",
      "Epoch 53/400, Batch 801/4457, Loss: 1.165433645248413\n",
      "Epoch 53/400, Batch 901/4457, Loss: 1.1945505142211914\n",
      "Epoch 53/400, Batch 1001/4457, Loss: 1.165456771850586\n",
      "Epoch 53/400, Batch 1101/4457, Loss: 1.196669340133667\n",
      "Epoch 53/400, Batch 1201/4457, Loss: 1.1654223203659058\n",
      "Epoch 53/400, Batch 1301/4457, Loss: 1.165513515472412\n",
      "Epoch 53/400, Batch 1401/4457, Loss: 1.1765098571777344\n",
      "Epoch 53/400, Batch 1501/4457, Loss: 1.1902074813842773\n",
      "Epoch 53/400, Batch 1601/4457, Loss: 1.16542649269104\n",
      "Epoch 53/400, Batch 1701/4457, Loss: 1.1654307842254639\n",
      "Epoch 53/400, Batch 1801/4457, Loss: 1.1654243469238281\n",
      "Epoch 53/400, Batch 1901/4457, Loss: 1.1654222011566162\n",
      "Epoch 53/400, Batch 2001/4457, Loss: 1.1654223203659058\n",
      "Epoch 53/400, Batch 2101/4457, Loss: 1.1661399602890015\n",
      "Epoch 53/400, Batch 2201/4457, Loss: 1.1966733932495117\n",
      "Epoch 53/400, Batch 2301/4457, Loss: 1.1654942035675049\n",
      "Epoch 53/400, Batch 2401/4457, Loss: 1.19672691822052\n",
      "Epoch 53/400, Batch 2501/4457, Loss: 1.1654222011566162\n",
      "Epoch 53/400, Batch 2601/4457, Loss: 1.1966919898986816\n",
      "Epoch 53/400, Batch 2701/4457, Loss: 1.16542387008667\n",
      "Epoch 53/400, Batch 2801/4457, Loss: 1.1654305458068848\n",
      "Epoch 53/400, Batch 2901/4457, Loss: 1.1654331684112549\n",
      "Epoch 53/400, Batch 3001/4457, Loss: 1.165426254272461\n",
      "Epoch 53/400, Batch 3101/4457, Loss: 1.2272756099700928\n",
      "Epoch 53/400, Batch 3201/4457, Loss: 1.1654225587844849\n",
      "Epoch 53/400, Batch 3301/4457, Loss: 1.1654222011566162\n",
      "Epoch 53/400, Batch 3401/4457, Loss: 1.1966722011566162\n",
      "Epoch 53/400, Batch 3501/4457, Loss: 1.1654222011566162\n",
      "Epoch 53/400, Batch 3601/4457, Loss: 1.1654222011566162\n",
      "Epoch 53/400, Batch 3701/4457, Loss: 1.1654250621795654\n",
      "Epoch 53/400, Batch 3801/4457, Loss: 1.1654222011566162\n",
      "Epoch 53/400, Batch 3901/4457, Loss: 1.1966724395751953\n",
      "Epoch 53/400, Batch 4001/4457, Loss: 1.227691650390625\n",
      "Epoch 53/400, Batch 4101/4457, Loss: 1.1680527925491333\n",
      "Epoch 53/400, Batch 4201/4457, Loss: 1.1846352815628052\n",
      "Epoch 53/400, Batch 4301/4457, Loss: 1.16542649269104\n",
      "Epoch 53/400, Batch 4401/4457, Loss: 1.1654223203659058\n",
      "Epoch 53/400, Validation Loss: 1.2840448264328261\n",
      "Epoch 54/400, Batch 1/4457, Loss: 1.1661489009857178\n",
      "Epoch 54/400, Batch 101/4457, Loss: 1.1798007488250732\n",
      "Epoch 54/400, Batch 201/4457, Loss: 1.1655304431915283\n",
      "Epoch 54/400, Batch 301/4457, Loss: 1.1654222011566162\n",
      "Epoch 54/400, Batch 401/4457, Loss: 1.1654222011566162\n",
      "Epoch 54/400, Batch 501/4457, Loss: 1.1654243469238281\n",
      "Epoch 54/400, Batch 601/4457, Loss: 1.1654222011566162\n",
      "Epoch 54/400, Batch 701/4457, Loss: 1.1654225587844849\n",
      "Epoch 54/400, Batch 801/4457, Loss: 1.1654225587844849\n",
      "Epoch 54/400, Batch 901/4457, Loss: 1.1654222011566162\n",
      "Epoch 54/400, Batch 1001/4457, Loss: 1.165453553199768\n",
      "Epoch 54/400, Batch 1101/4457, Loss: 1.1966729164123535\n",
      "Epoch 54/400, Batch 1201/4457, Loss: 1.1654248237609863\n",
      "Epoch 54/400, Batch 1301/4457, Loss: 1.1654613018035889\n",
      "Epoch 54/400, Batch 1401/4457, Loss: 1.1654298305511475\n",
      "Epoch 54/400, Batch 1501/4457, Loss: 1.1966722011566162\n",
      "Epoch 54/400, Batch 1601/4457, Loss: 1.1654222011566162\n",
      "Epoch 54/400, Batch 1701/4457, Loss: 1.1654243469238281\n",
      "Epoch 54/400, Batch 1801/4457, Loss: 1.1966723203659058\n",
      "Epoch 54/400, Batch 1901/4457, Loss: 1.165423035621643\n",
      "Epoch 54/400, Batch 2001/4457, Loss: 1.1654224395751953\n",
      "Epoch 54/400, Batch 2101/4457, Loss: 1.1654222011566162\n",
      "Epoch 54/400, Batch 2201/4457, Loss: 1.1654223203659058\n",
      "Epoch 54/400, Batch 2301/4457, Loss: 1.183318853378296\n",
      "Epoch 54/400, Batch 2401/4457, Loss: 1.1654224395751953\n",
      "Epoch 54/400, Batch 2501/4457, Loss: 1.165425181388855\n",
      "Epoch 54/400, Batch 2601/4457, Loss: 1.1966588497161865\n",
      "Epoch 54/400, Batch 2701/4457, Loss: 1.1966780424118042\n",
      "Epoch 54/400, Batch 2801/4457, Loss: 1.1966726779937744\n",
      "Epoch 54/400, Batch 2901/4457, Loss: 1.1709885597229004\n",
      "Epoch 54/400, Batch 3001/4457, Loss: 1.1654233932495117\n",
      "Epoch 54/400, Batch 3101/4457, Loss: 1.1654248237609863\n",
      "Epoch 54/400, Batch 3201/4457, Loss: 1.1654229164123535\n",
      "Epoch 54/400, Batch 3301/4457, Loss: 1.1654222011566162\n",
      "Epoch 54/400, Batch 3401/4457, Loss: 1.1948225498199463\n",
      "Epoch 54/400, Batch 3501/4457, Loss: 1.1654263734817505\n",
      "Epoch 54/400, Batch 3601/4457, Loss: 1.1732492446899414\n",
      "Epoch 54/400, Batch 3701/4457, Loss: 1.1654274463653564\n",
      "Epoch 54/400, Batch 3801/4457, Loss: 1.1654236316680908\n",
      "Epoch 54/400, Batch 3901/4457, Loss: 1.1654223203659058\n",
      "Epoch 54/400, Batch 4001/4457, Loss: 1.1654224395751953\n",
      "Epoch 54/400, Batch 4101/4457, Loss: 1.1654224395751953\n",
      "Epoch 54/400, Batch 4201/4457, Loss: 1.1654232740402222\n",
      "Epoch 54/400, Batch 4301/4457, Loss: 1.165475606918335\n",
      "Epoch 54/400, Batch 4401/4457, Loss: 1.1654224395751953\n",
      "Epoch 54/400, Validation Loss: 1.2860309705138206\n",
      "Epoch 55/400, Batch 1/4457, Loss: 1.1966722011566162\n",
      "Epoch 55/400, Batch 101/4457, Loss: 1.1654222011566162\n",
      "Epoch 55/400, Batch 201/4457, Loss: 1.1966030597686768\n",
      "Epoch 55/400, Batch 301/4457, Loss: 1.1654233932495117\n",
      "Epoch 55/400, Batch 401/4457, Loss: 1.196681022644043\n",
      "Epoch 55/400, Batch 501/4457, Loss: 1.1654243469238281\n",
      "Epoch 55/400, Batch 601/4457, Loss: 1.1655890941619873\n",
      "Epoch 55/400, Batch 701/4457, Loss: 1.1654284000396729\n",
      "Epoch 55/400, Batch 801/4457, Loss: 1.1654331684112549\n",
      "Epoch 55/400, Batch 901/4457, Loss: 1.1654222011566162\n",
      "Epoch 55/400, Batch 1001/4457, Loss: 1.1654422283172607\n",
      "Epoch 55/400, Batch 1101/4457, Loss: 1.1654223203659058\n",
      "Epoch 55/400, Batch 1201/4457, Loss: 1.1967923641204834\n",
      "Epoch 55/400, Batch 1301/4457, Loss: 1.1654337644577026\n",
      "Epoch 55/400, Batch 1401/4457, Loss: 1.1654223203659058\n",
      "Epoch 55/400, Batch 1501/4457, Loss: 1.1654229164123535\n",
      "Epoch 55/400, Batch 1601/4457, Loss: 1.1654226779937744\n",
      "Epoch 55/400, Batch 1701/4457, Loss: 1.1967298984527588\n",
      "Epoch 55/400, Batch 1801/4457, Loss: 1.1672146320343018\n",
      "Epoch 55/400, Batch 1901/4457, Loss: 1.165433406829834\n",
      "Epoch 55/400, Batch 2001/4457, Loss: 1.165423035621643\n",
      "Epoch 55/400, Batch 2101/4457, Loss: 1.1658430099487305\n",
      "Epoch 55/400, Batch 2201/4457, Loss: 1.1966731548309326\n",
      "Epoch 55/400, Batch 2301/4457, Loss: 1.16542387008667\n",
      "Epoch 55/400, Batch 2401/4457, Loss: 1.1966769695281982\n",
      "Epoch 55/400, Batch 2501/4457, Loss: 1.1654305458068848\n",
      "Epoch 55/400, Batch 2601/4457, Loss: 1.1940085887908936\n",
      "Epoch 55/400, Batch 2701/4457, Loss: 1.2280051708221436\n",
      "Epoch 55/400, Batch 2801/4457, Loss: 1.1654222011566162\n",
      "Epoch 55/400, Batch 2901/4457, Loss: 1.169521450996399\n",
      "Epoch 55/400, Batch 3001/4457, Loss: 1.1654229164123535\n",
      "Epoch 55/400, Batch 3101/4457, Loss: 1.172145128250122\n",
      "Epoch 55/400, Batch 3201/4457, Loss: 1.1654223203659058\n",
      "Epoch 55/400, Batch 3301/4457, Loss: 1.196673035621643\n",
      "Epoch 55/400, Batch 3401/4457, Loss: 1.1654222011566162\n",
      "Epoch 55/400, Batch 3501/4457, Loss: 1.176868200302124\n",
      "Epoch 55/400, Batch 3601/4457, Loss: 1.1654268503189087\n",
      "Epoch 55/400, Batch 3701/4457, Loss: 1.16542649269104\n",
      "Epoch 55/400, Batch 3801/4457, Loss: 1.1654605865478516\n",
      "Epoch 55/400, Batch 3901/4457, Loss: 1.1654284000396729\n",
      "Epoch 55/400, Batch 4001/4457, Loss: 1.1654223203659058\n",
      "Epoch 55/400, Batch 4101/4457, Loss: 1.165431022644043\n",
      "Epoch 55/400, Batch 4201/4457, Loss: 1.1966737508773804\n",
      "Epoch 55/400, Batch 4301/4457, Loss: 1.1654223203659058\n",
      "Epoch 55/400, Batch 4401/4457, Loss: 1.1654222011566162\n",
      "Epoch 55/400, Validation Loss: 1.2758205942218266\n",
      "Epoch 56/400, Batch 1/4457, Loss: 1.165480375289917\n",
      "Epoch 56/400, Batch 101/4457, Loss: 1.1654571294784546\n",
      "Epoch 56/400, Batch 201/4457, Loss: 1.1654222011566162\n",
      "Epoch 56/400, Batch 301/4457, Loss: 1.1966688632965088\n",
      "Epoch 56/400, Batch 401/4457, Loss: 1.1654530763626099\n",
      "Epoch 56/400, Batch 501/4457, Loss: 1.1654223203659058\n",
      "Epoch 56/400, Batch 601/4457, Loss: 1.1654324531555176\n",
      "Epoch 56/400, Batch 701/4457, Loss: 1.1654222011566162\n",
      "Epoch 56/400, Batch 801/4457, Loss: 1.1654224395751953\n",
      "Epoch 56/400, Batch 901/4457, Loss: 1.1654295921325684\n",
      "Epoch 56/400, Batch 1001/4457, Loss: 1.1654225587844849\n",
      "Epoch 56/400, Batch 1101/4457, Loss: 1.1966722011566162\n",
      "Epoch 56/400, Batch 1201/4457, Loss: 1.1654250621795654\n",
      "Epoch 56/400, Batch 1301/4457, Loss: 1.1654223203659058\n",
      "Epoch 56/400, Batch 1401/4457, Loss: 1.165426254272461\n",
      "Epoch 56/400, Batch 1501/4457, Loss: 1.19667387008667\n",
      "Epoch 56/400, Batch 1601/4457, Loss: 1.166265845298767\n",
      "Epoch 56/400, Batch 1701/4457, Loss: 1.1966724395751953\n",
      "Epoch 56/400, Batch 1801/4457, Loss: 1.165424108505249\n",
      "Epoch 56/400, Batch 1901/4457, Loss: 1.1654250621795654\n",
      "Epoch 56/400, Batch 2001/4457, Loss: 1.1654269695281982\n",
      "Epoch 56/400, Batch 2101/4457, Loss: 1.1655464172363281\n",
      "Epoch 56/400, Batch 2201/4457, Loss: 1.1663146018981934\n",
      "Epoch 56/400, Batch 2301/4457, Loss: 1.1654224395751953\n",
      "Epoch 56/400, Batch 2401/4457, Loss: 1.165435552597046\n",
      "Epoch 56/400, Batch 2501/4457, Loss: 1.1654473543167114\n",
      "Epoch 56/400, Batch 2601/4457, Loss: 1.19667387008667\n",
      "Epoch 56/400, Batch 2701/4457, Loss: 1.1654224395751953\n",
      "Epoch 56/400, Batch 2801/4457, Loss: 1.1657652854919434\n",
      "Epoch 56/400, Batch 2901/4457, Loss: 1.1654223203659058\n",
      "Epoch 56/400, Batch 3001/4457, Loss: 1.1654223203659058\n",
      "Epoch 56/400, Batch 3101/4457, Loss: 1.1654223203659058\n",
      "Epoch 56/400, Batch 3201/4457, Loss: 1.1654222011566162\n",
      "Epoch 56/400, Batch 3301/4457, Loss: 1.1655137538909912\n",
      "Epoch 56/400, Batch 3401/4457, Loss: 1.166283130645752\n",
      "Epoch 56/400, Batch 3501/4457, Loss: 1.1931586265563965\n",
      "Epoch 56/400, Batch 3601/4457, Loss: 1.1654226779937744\n",
      "Epoch 56/400, Batch 3701/4457, Loss: 1.1688141822814941\n",
      "Epoch 56/400, Batch 3801/4457, Loss: 1.165424108505249\n",
      "Epoch 56/400, Batch 3901/4457, Loss: 1.1654225587844849\n",
      "Epoch 56/400, Batch 4001/4457, Loss: 1.1654222011566162\n",
      "Epoch 56/400, Batch 4101/4457, Loss: 1.1966712474822998\n",
      "Epoch 56/400, Batch 4201/4457, Loss: 1.1654250621795654\n",
      "Epoch 56/400, Batch 4301/4457, Loss: 1.1964133977890015\n",
      "Epoch 56/400, Batch 4401/4457, Loss: 1.2253663539886475\n",
      "Epoch 56/400, Validation Loss: 1.27848505642679\n",
      "Epoch 57/400, Batch 1/4457, Loss: 1.1966793537139893\n",
      "Epoch 57/400, Batch 101/4457, Loss: 1.1654224395751953\n",
      "Epoch 57/400, Batch 201/4457, Loss: 1.1966636180877686\n",
      "Epoch 57/400, Batch 301/4457, Loss: 1.165477991104126\n",
      "Epoch 57/400, Batch 401/4457, Loss: 1.1654222011566162\n",
      "Epoch 57/400, Batch 501/4457, Loss: 1.2279220819473267\n",
      "Epoch 57/400, Batch 601/4457, Loss: 1.1654224395751953\n",
      "Epoch 57/400, Batch 701/4457, Loss: 1.1966692209243774\n",
      "Epoch 57/400, Batch 801/4457, Loss: 1.1654244661331177\n",
      "Epoch 57/400, Batch 901/4457, Loss: 1.1673187017440796\n",
      "Epoch 57/400, Batch 1001/4457, Loss: 1.1654222011566162\n",
      "Epoch 57/400, Batch 1101/4457, Loss: 1.196678876876831\n",
      "Epoch 57/400, Batch 1201/4457, Loss: 1.1654224395751953\n",
      "Epoch 57/400, Batch 1301/4457, Loss: 1.1654222011566162\n",
      "Epoch 57/400, Batch 1401/4457, Loss: 1.165427803993225\n",
      "Epoch 57/400, Batch 1501/4457, Loss: 1.165426254272461\n",
      "Epoch 57/400, Batch 1601/4457, Loss: 1.1654237508773804\n",
      "Epoch 57/400, Batch 1701/4457, Loss: 1.1659529209136963\n",
      "Epoch 57/400, Batch 1801/4457, Loss: 1.1772243976593018\n",
      "Epoch 57/400, Batch 1901/4457, Loss: 1.1654223203659058\n",
      "Epoch 57/400, Batch 2001/4457, Loss: 1.196671962738037\n",
      "Epoch 57/400, Batch 2101/4457, Loss: 1.1654222011566162\n",
      "Epoch 57/400, Batch 2201/4457, Loss: 1.1966722011566162\n",
      "Epoch 57/400, Batch 2301/4457, Loss: 1.1654226779937744\n",
      "Epoch 57/400, Batch 2401/4457, Loss: 1.1654815673828125\n",
      "Epoch 57/400, Batch 2501/4457, Loss: 1.1654282808303833\n",
      "Epoch 57/400, Batch 2601/4457, Loss: 1.1966723203659058\n",
      "Epoch 57/400, Batch 2701/4457, Loss: 1.1966745853424072\n",
      "Epoch 57/400, Batch 2801/4457, Loss: 1.1654226779937744\n",
      "Epoch 57/400, Batch 2901/4457, Loss: 1.1970044374465942\n",
      "Epoch 57/400, Batch 3001/4457, Loss: 1.1654224395751953\n",
      "Epoch 57/400, Batch 3101/4457, Loss: 1.1654223203659058\n",
      "Epoch 57/400, Batch 3201/4457, Loss: 1.1654222011566162\n",
      "Epoch 57/400, Batch 3301/4457, Loss: 1.1654226779937744\n",
      "Epoch 57/400, Batch 3401/4457, Loss: 1.165426254272461\n",
      "Epoch 57/400, Batch 3501/4457, Loss: 1.1654232740402222\n",
      "Epoch 57/400, Batch 3601/4457, Loss: 1.1654222011566162\n",
      "Epoch 57/400, Batch 3701/4457, Loss: 1.1654229164123535\n",
      "Epoch 57/400, Batch 3801/4457, Loss: 1.1654222011566162\n",
      "Epoch 57/400, Batch 3901/4457, Loss: 1.1654222011566162\n",
      "Epoch 57/400, Batch 4001/4457, Loss: 1.2836387157440186\n",
      "Epoch 57/400, Batch 4101/4457, Loss: 1.1654224395751953\n",
      "Epoch 57/400, Batch 4201/4457, Loss: 1.1654229164123535\n",
      "Epoch 57/400, Batch 4301/4457, Loss: 1.196658730506897\n",
      "Epoch 57/400, Batch 4401/4457, Loss: 1.1654226779937744\n",
      "Epoch 57/400, Validation Loss: 1.2796075119385644\n",
      "Epoch 58/400, Batch 1/4457, Loss: 1.1654236316680908\n",
      "Epoch 58/400, Batch 101/4457, Loss: 1.1654222011566162\n",
      "Epoch 58/400, Batch 201/4457, Loss: 1.1654289960861206\n",
      "Epoch 58/400, Batch 301/4457, Loss: 1.165423035621643\n",
      "Epoch 58/400, Batch 401/4457, Loss: 1.193457841873169\n",
      "Epoch 58/400, Batch 501/4457, Loss: 1.1664974689483643\n",
      "Epoch 58/400, Batch 601/4457, Loss: 1.219299077987671\n",
      "Epoch 58/400, Batch 701/4457, Loss: 1.1967871189117432\n",
      "Epoch 58/400, Batch 801/4457, Loss: 1.1654223203659058\n",
      "Epoch 58/400, Batch 901/4457, Loss: 1.1966724395751953\n",
      "Epoch 58/400, Batch 1001/4457, Loss: 1.1654233932495117\n",
      "Epoch 58/400, Batch 1101/4457, Loss: 1.1966723203659058\n",
      "Epoch 58/400, Batch 1201/4457, Loss: 1.1654303073883057\n",
      "Epoch 58/400, Batch 1301/4457, Loss: 1.16542649269104\n",
      "Epoch 58/400, Batch 1401/4457, Loss: 1.166245698928833\n",
      "Epoch 58/400, Batch 1501/4457, Loss: 1.205583095550537\n",
      "Epoch 58/400, Batch 1601/4457, Loss: 1.1688053607940674\n",
      "Epoch 58/400, Batch 1701/4457, Loss: 1.1654267311096191\n",
      "Epoch 58/400, Batch 1801/4457, Loss: 1.1967291831970215\n",
      "Epoch 58/400, Batch 1901/4457, Loss: 1.1654222011566162\n",
      "Epoch 58/400, Batch 2001/4457, Loss: 1.165433645248413\n",
      "Epoch 58/400, Batch 2101/4457, Loss: 1.1654222011566162\n",
      "Epoch 58/400, Batch 2201/4457, Loss: 1.1654236316680908\n",
      "Epoch 58/400, Batch 2301/4457, Loss: 1.1966724395751953\n",
      "Epoch 58/400, Batch 2401/4457, Loss: 1.1654222011566162\n",
      "Epoch 58/400, Batch 2501/4457, Loss: 1.1654224395751953\n",
      "Epoch 58/400, Batch 2601/4457, Loss: 1.1654224395751953\n",
      "Epoch 58/400, Batch 2701/4457, Loss: 1.1654226779937744\n",
      "Epoch 58/400, Batch 2801/4457, Loss: 1.1654223203659058\n",
      "Epoch 58/400, Batch 2901/4457, Loss: 1.1654247045516968\n",
      "Epoch 58/400, Batch 3001/4457, Loss: 1.1944708824157715\n",
      "Epoch 58/400, Batch 3101/4457, Loss: 1.1654222011566162\n",
      "Epoch 58/400, Batch 3201/4457, Loss: 1.1654404401779175\n",
      "Epoch 58/400, Batch 3301/4457, Loss: 1.165426254272461\n",
      "Epoch 58/400, Batch 3401/4457, Loss: 1.1654253005981445\n",
      "Epoch 58/400, Batch 3501/4457, Loss: 1.1654530763626099\n",
      "Epoch 58/400, Batch 3601/4457, Loss: 1.1654233932495117\n",
      "Epoch 58/400, Batch 3701/4457, Loss: 1.1654226779937744\n",
      "Epoch 58/400, Batch 3801/4457, Loss: 1.1654245853424072\n",
      "Epoch 58/400, Batch 3901/4457, Loss: 1.1654224395751953\n",
      "Epoch 58/400, Batch 4001/4457, Loss: 1.165823221206665\n",
      "Epoch 58/400, Batch 4101/4457, Loss: 1.1654402017593384\n",
      "Epoch 58/400, Batch 4201/4457, Loss: 1.1654226779937744\n",
      "Epoch 58/400, Batch 4301/4457, Loss: 1.1654224395751953\n",
      "Epoch 58/400, Batch 4401/4457, Loss: 1.1654242277145386\n",
      "Epoch 58/400, Validation Loss: 1.2808477235218836\n",
      "Epoch 59/400, Batch 1/4457, Loss: 1.1654224395751953\n",
      "Epoch 59/400, Batch 101/4457, Loss: 1.1655163764953613\n",
      "Epoch 59/400, Batch 201/4457, Loss: 1.1654287576675415\n",
      "Epoch 59/400, Batch 301/4457, Loss: 1.1654223203659058\n",
      "Epoch 59/400, Batch 401/4457, Loss: 1.1654224395751953\n",
      "Epoch 59/400, Batch 501/4457, Loss: 1.1966525316238403\n",
      "Epoch 59/400, Batch 601/4457, Loss: 1.1659390926361084\n",
      "Epoch 59/400, Batch 701/4457, Loss: 1.1654253005981445\n",
      "Epoch 59/400, Batch 801/4457, Loss: 1.2062958478927612\n",
      "Epoch 59/400, Batch 901/4457, Loss: 1.2279222011566162\n",
      "Epoch 59/400, Batch 1001/4457, Loss: 1.1654229164123535\n",
      "Epoch 59/400, Batch 1101/4457, Loss: 1.1655936241149902\n",
      "Epoch 59/400, Batch 1201/4457, Loss: 1.1654231548309326\n",
      "Epoch 59/400, Batch 1301/4457, Loss: 1.165422797203064\n",
      "Epoch 59/400, Batch 1401/4457, Loss: 1.1654222011566162\n",
      "Epoch 59/400, Batch 1501/4457, Loss: 1.1654818058013916\n",
      "Epoch 59/400, Batch 1601/4457, Loss: 1.1654226779937744\n",
      "Epoch 59/400, Batch 1701/4457, Loss: 1.1654231548309326\n",
      "Epoch 59/400, Batch 1801/4457, Loss: 1.1654222011566162\n",
      "Epoch 59/400, Batch 1901/4457, Loss: 1.1654222011566162\n",
      "Epoch 59/400, Batch 2001/4457, Loss: 1.1655194759368896\n",
      "Epoch 59/400, Batch 2101/4457, Loss: 1.1654224395751953\n",
      "Epoch 59/400, Batch 2201/4457, Loss: 1.1654224395751953\n",
      "Epoch 59/400, Batch 2301/4457, Loss: 1.1654765605926514\n",
      "Epoch 59/400, Batch 2401/4457, Loss: 1.1654773950576782\n",
      "Epoch 59/400, Batch 2501/4457, Loss: 1.1654224395751953\n",
      "Epoch 59/400, Batch 2601/4457, Loss: 1.1654579639434814\n",
      "Epoch 59/400, Batch 2701/4457, Loss: 1.16542387008667\n",
      "Epoch 59/400, Batch 2801/4457, Loss: 1.1654398441314697\n",
      "Epoch 59/400, Batch 2901/4457, Loss: 1.1654222011566162\n",
      "Epoch 59/400, Batch 3001/4457, Loss: 1.1654242277145386\n",
      "Epoch 59/400, Batch 3101/4457, Loss: 1.1654222011566162\n",
      "Epoch 59/400, Batch 3201/4457, Loss: 1.1655123233795166\n",
      "Epoch 59/400, Batch 3301/4457, Loss: 1.1966674327850342\n",
      "Epoch 59/400, Batch 3401/4457, Loss: 1.16542649269104\n",
      "Epoch 59/400, Batch 3501/4457, Loss: 1.1655163764953613\n",
      "Epoch 59/400, Batch 3601/4457, Loss: 1.1654386520385742\n",
      "Epoch 59/400, Batch 3701/4457, Loss: 1.1654229164123535\n",
      "Epoch 59/400, Batch 3801/4457, Loss: 1.1684647798538208\n",
      "Epoch 59/400, Batch 3901/4457, Loss: 1.1654222011566162\n",
      "Epoch 59/400, Batch 4001/4457, Loss: 1.165459394454956\n",
      "Epoch 59/400, Batch 4101/4457, Loss: 1.1654235124588013\n",
      "Epoch 59/400, Batch 4201/4457, Loss: 1.1655625104904175\n",
      "Epoch 59/400, Batch 4301/4457, Loss: 1.1654229164123535\n",
      "Epoch 59/400, Batch 4401/4457, Loss: 1.1966722011566162\n",
      "Epoch 59/400, Validation Loss: 1.283572043927889\n",
      "Epoch 60/400, Batch 1/4457, Loss: 1.196671485900879\n",
      "Epoch 60/400, Batch 101/4457, Loss: 1.1654224395751953\n",
      "Epoch 60/400, Batch 201/4457, Loss: 1.1654222011566162\n",
      "Epoch 60/400, Batch 301/4457, Loss: 1.1654222011566162\n",
      "Epoch 60/400, Batch 401/4457, Loss: 1.196659803390503\n",
      "Epoch 60/400, Batch 501/4457, Loss: 1.1654226779937744\n",
      "Epoch 60/400, Batch 601/4457, Loss: 1.1654224395751953\n",
      "Epoch 60/400, Batch 701/4457, Loss: 1.1654257774353027\n",
      "Epoch 60/400, Batch 801/4457, Loss: 1.165921926498413\n",
      "Epoch 60/400, Batch 901/4457, Loss: 1.1654223203659058\n",
      "Epoch 60/400, Batch 1001/4457, Loss: 1.1654239892959595\n",
      "Epoch 60/400, Batch 1101/4457, Loss: 1.1654236316680908\n",
      "Epoch 60/400, Batch 1201/4457, Loss: 1.1966726779937744\n",
      "Epoch 60/400, Batch 1301/4457, Loss: 1.1654223203659058\n",
      "Epoch 60/400, Batch 1401/4457, Loss: 1.1654223203659058\n",
      "Epoch 60/400, Batch 1501/4457, Loss: 1.1654224395751953\n",
      "Epoch 60/400, Batch 1601/4457, Loss: 1.1654229164123535\n",
      "Epoch 60/400, Batch 1701/4457, Loss: 1.1654250621795654\n",
      "Epoch 60/400, Batch 1801/4457, Loss: 1.1654256582260132\n",
      "Epoch 60/400, Batch 1901/4457, Loss: 1.1654354333877563\n",
      "Epoch 60/400, Batch 2001/4457, Loss: 1.1654222011566162\n",
      "Epoch 60/400, Batch 2101/4457, Loss: 1.1654236316680908\n",
      "Epoch 60/400, Batch 2201/4457, Loss: 1.182389259338379\n",
      "Epoch 60/400, Batch 2301/4457, Loss: 1.1654223203659058\n",
      "Epoch 60/400, Batch 2401/4457, Loss: 1.1887500286102295\n",
      "Epoch 60/400, Batch 2501/4457, Loss: 1.1654223203659058\n",
      "Epoch 60/400, Batch 2601/4457, Loss: 1.1654225587844849\n",
      "Epoch 60/400, Batch 2701/4457, Loss: 1.1966722011566162\n",
      "Epoch 60/400, Batch 2801/4457, Loss: 1.1950891017913818\n",
      "Epoch 60/400, Batch 2901/4457, Loss: 1.165446162223816\n",
      "Epoch 60/400, Batch 3001/4457, Loss: 1.223737359046936\n",
      "Epoch 60/400, Batch 3101/4457, Loss: 1.1654222011566162\n",
      "Epoch 60/400, Batch 3201/4457, Loss: 1.1654255390167236\n",
      "Epoch 60/400, Batch 3301/4457, Loss: 1.1654224395751953\n",
      "Epoch 60/400, Batch 3401/4457, Loss: 1.1654235124588013\n",
      "Epoch 60/400, Batch 3501/4457, Loss: 1.1654235124588013\n",
      "Epoch 60/400, Batch 3601/4457, Loss: 1.1985743045806885\n",
      "Epoch 60/400, Batch 3701/4457, Loss: 1.1654272079467773\n",
      "Epoch 60/400, Batch 3801/4457, Loss: 1.1654222011566162\n",
      "Epoch 60/400, Batch 3901/4457, Loss: 1.1654255390167236\n",
      "Epoch 60/400, Batch 4001/4457, Loss: 1.1655112504959106\n",
      "Epoch 60/400, Batch 4101/4457, Loss: 1.1654248237609863\n",
      "Epoch 60/400, Batch 4201/4457, Loss: 1.1654272079467773\n",
      "Epoch 60/400, Batch 4301/4457, Loss: 1.165639042854309\n",
      "Epoch 60/400, Batch 4401/4457, Loss: 1.1654229164123535\n",
      "Epoch 60/400, Validation Loss: 1.2885332495447188\n",
      "Epoch 61/400, Batch 1/4457, Loss: 1.1654231548309326\n",
      "Epoch 61/400, Batch 101/4457, Loss: 1.1654300689697266\n",
      "Epoch 61/400, Batch 201/4457, Loss: 1.1654223203659058\n",
      "Epoch 61/400, Batch 301/4457, Loss: 1.1654222011566162\n",
      "Epoch 61/400, Batch 401/4457, Loss: 1.1654250621795654\n",
      "Epoch 61/400, Batch 501/4457, Loss: 1.1654223203659058\n",
      "Epoch 61/400, Batch 601/4457, Loss: 1.1654222011566162\n",
      "Epoch 61/400, Batch 701/4457, Loss: 1.1654222011566162\n",
      "Epoch 61/400, Batch 801/4457, Loss: 1.2279226779937744\n",
      "Epoch 61/400, Batch 901/4457, Loss: 1.2279210090637207\n",
      "Epoch 61/400, Batch 1001/4457, Loss: 1.1654224395751953\n",
      "Epoch 61/400, Batch 1101/4457, Loss: 1.1654226779937744\n",
      "Epoch 61/400, Batch 1201/4457, Loss: 1.1654256582260132\n",
      "Epoch 61/400, Batch 1301/4457, Loss: 1.1654343605041504\n",
      "Epoch 61/400, Batch 1401/4457, Loss: 1.1654999256134033\n",
      "Epoch 61/400, Batch 1501/4457, Loss: 1.1654244661331177\n",
      "Epoch 61/400, Batch 1601/4457, Loss: 1.1654226779937744\n",
      "Epoch 61/400, Batch 1701/4457, Loss: 1.165452003479004\n",
      "Epoch 61/400, Batch 1801/4457, Loss: 1.1654223203659058\n",
      "Epoch 61/400, Batch 1901/4457, Loss: 1.2279198169708252\n",
      "Epoch 61/400, Batch 2001/4457, Loss: 1.1654223203659058\n",
      "Epoch 61/400, Batch 2101/4457, Loss: 1.168128252029419\n",
      "Epoch 61/400, Batch 2201/4457, Loss: 1.1654224395751953\n",
      "Epoch 61/400, Batch 2301/4457, Loss: 1.1654446125030518\n",
      "Epoch 61/400, Batch 2401/4457, Loss: 1.165441870689392\n",
      "Epoch 61/400, Batch 2501/4457, Loss: 1.1654422283172607\n",
      "Epoch 61/400, Batch 2601/4457, Loss: 1.165422797203064\n",
      "Epoch 61/400, Batch 2701/4457, Loss: 1.1966722011566162\n",
      "Epoch 61/400, Batch 2801/4457, Loss: 1.1654257774353027\n",
      "Epoch 61/400, Batch 2901/4457, Loss: 1.165422797203064\n",
      "Epoch 61/400, Batch 3001/4457, Loss: 1.196665644645691\n",
      "Epoch 61/400, Batch 3101/4457, Loss: 1.1966722011566162\n",
      "Epoch 61/400, Batch 3201/4457, Loss: 1.1654222011566162\n",
      "Epoch 61/400, Batch 3301/4457, Loss: 1.1654350757598877\n",
      "Epoch 61/400, Batch 3401/4457, Loss: 1.227921962738037\n",
      "Epoch 61/400, Batch 3501/4457, Loss: 1.1654223203659058\n",
      "Epoch 61/400, Batch 3601/4457, Loss: 1.1654248237609863\n",
      "Epoch 61/400, Batch 3701/4457, Loss: 1.1654605865478516\n",
      "Epoch 61/400, Batch 3801/4457, Loss: 1.1654225587844849\n",
      "Epoch 61/400, Batch 3901/4457, Loss: 1.1654231548309326\n",
      "Epoch 61/400, Batch 4001/4457, Loss: 1.1654223203659058\n",
      "Epoch 61/400, Batch 4101/4457, Loss: 1.1654222011566162\n",
      "Epoch 61/400, Batch 4201/4457, Loss: 1.1674540042877197\n",
      "Epoch 61/400, Batch 4301/4457, Loss: 1.168033242225647\n",
      "Epoch 61/400, Batch 4401/4457, Loss: 1.1966725587844849\n",
      "Epoch 61/400, Validation Loss: 1.284356523246046\n",
      "Epoch 62/400, Batch 1/4457, Loss: 1.1654597520828247\n",
      "Epoch 62/400, Batch 101/4457, Loss: 1.1654224395751953\n",
      "Epoch 62/400, Batch 201/4457, Loss: 1.1654250621795654\n",
      "Epoch 62/400, Batch 301/4457, Loss: 1.2587435245513916\n",
      "Epoch 62/400, Batch 401/4457, Loss: 1.1654231548309326\n",
      "Epoch 62/400, Batch 501/4457, Loss: 1.1966723203659058\n",
      "Epoch 62/400, Batch 601/4457, Loss: 1.1654222011566162\n",
      "Epoch 62/400, Batch 701/4457, Loss: 1.1654226779937744\n",
      "Epoch 62/400, Batch 801/4457, Loss: 1.1966702938079834\n",
      "Epoch 62/400, Batch 901/4457, Loss: 1.165423035621643\n",
      "Epoch 62/400, Batch 1001/4457, Loss: 1.1655552387237549\n",
      "Epoch 62/400, Batch 1101/4457, Loss: 1.1704682111740112\n",
      "Epoch 62/400, Batch 1201/4457, Loss: 1.1654223203659058\n",
      "Epoch 62/400, Batch 1301/4457, Loss: 1.196533203125\n",
      "Epoch 62/400, Batch 1401/4457, Loss: 1.1654560565948486\n",
      "Epoch 62/400, Batch 1501/4457, Loss: 1.1654733419418335\n",
      "Epoch 62/400, Batch 1601/4457, Loss: 1.1654245853424072\n",
      "Epoch 62/400, Batch 1701/4457, Loss: 1.1654222011566162\n",
      "Epoch 62/400, Batch 1801/4457, Loss: 1.1654222011566162\n",
      "Epoch 62/400, Batch 1901/4457, Loss: 1.1657965183258057\n",
      "Epoch 62/400, Batch 2001/4457, Loss: 1.1966723203659058\n",
      "Epoch 62/400, Batch 2101/4457, Loss: 1.1686307191848755\n",
      "Epoch 62/400, Batch 2201/4457, Loss: 1.1654229164123535\n",
      "Epoch 62/400, Batch 2301/4457, Loss: 1.16552734375\n",
      "Epoch 62/400, Batch 2401/4457, Loss: 1.1966724395751953\n",
      "Epoch 62/400, Batch 2501/4457, Loss: 1.1966733932495117\n",
      "Epoch 62/400, Batch 2601/4457, Loss: 1.1654224395751953\n",
      "Epoch 62/400, Batch 2701/4457, Loss: 1.1654223203659058\n",
      "Epoch 62/400, Batch 2801/4457, Loss: 1.165454626083374\n",
      "Epoch 62/400, Batch 2901/4457, Loss: 1.1966733932495117\n",
      "Epoch 62/400, Batch 3001/4457, Loss: 1.1654224395751953\n",
      "Epoch 62/400, Batch 3101/4457, Loss: 1.1668219566345215\n",
      "Epoch 62/400, Batch 3201/4457, Loss: 1.2279163599014282\n",
      "Epoch 62/400, Batch 3301/4457, Loss: 1.1966722011566162\n",
      "Epoch 62/400, Batch 3401/4457, Loss: 1.1768337488174438\n",
      "Epoch 62/400, Batch 3501/4457, Loss: 1.1654226779937744\n",
      "Epoch 62/400, Batch 3601/4457, Loss: 1.19667387008667\n",
      "Epoch 62/400, Batch 3701/4457, Loss: 1.1654610633850098\n",
      "Epoch 62/400, Batch 3801/4457, Loss: 1.196674108505249\n",
      "Epoch 62/400, Batch 3901/4457, Loss: 1.1654260158538818\n",
      "Epoch 62/400, Batch 4001/4457, Loss: 1.1966705322265625\n",
      "Epoch 62/400, Batch 4101/4457, Loss: 1.2279165983200073\n",
      "Epoch 62/400, Batch 4201/4457, Loss: 1.1654272079467773\n",
      "Epoch 62/400, Batch 4301/4457, Loss: 1.1660610437393188\n",
      "Epoch 62/400, Batch 4401/4457, Loss: 1.1964550018310547\n",
      "Epoch 62/400, Validation Loss: 1.2855318593127387\n",
      "Epoch 63/400, Batch 1/4457, Loss: 1.1654224395751953\n",
      "Epoch 63/400, Batch 101/4457, Loss: 1.1654231548309326\n",
      "Epoch 63/400, Batch 201/4457, Loss: 1.165422797203064\n",
      "Epoch 63/400, Batch 301/4457, Loss: 1.1654229164123535\n",
      "Epoch 63/400, Batch 401/4457, Loss: 1.1654222011566162\n",
      "Epoch 63/400, Batch 501/4457, Loss: 1.1654223203659058\n",
      "Epoch 63/400, Batch 601/4457, Loss: 1.1654224395751953\n",
      "Epoch 63/400, Batch 701/4457, Loss: 1.1820485591888428\n",
      "Epoch 63/400, Batch 801/4457, Loss: 1.1654226779937744\n",
      "Epoch 63/400, Batch 901/4457, Loss: 1.1654225587844849\n",
      "Epoch 63/400, Batch 1001/4457, Loss: 1.207548975944519\n",
      "Epoch 63/400, Batch 1101/4457, Loss: 1.22739839553833\n",
      "Epoch 63/400, Batch 1201/4457, Loss: 1.16542387008667\n",
      "Epoch 63/400, Batch 1301/4457, Loss: 1.1654242277145386\n",
      "Epoch 63/400, Batch 1401/4457, Loss: 1.1654231548309326\n",
      "Epoch 63/400, Batch 1501/4457, Loss: 1.1836292743682861\n",
      "Epoch 63/400, Batch 1601/4457, Loss: 1.1658155918121338\n",
      "Epoch 63/400, Batch 1701/4457, Loss: 1.2574238777160645\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     45\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 46\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     49\u001b[0m epoch_train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\ashiq hussain teeli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ashiq hussain teeli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the LSTM model with a softmax layer\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Use the output from the last time step\n",
    "        out = self.softmax(out)  # Apply softmax\n",
    "        return out\n",
    "    \n",
    "    \n",
    "# Set the input size, hidden size, number of layers, and output size\n",
    "input_size = train_features.shape[1]\n",
    "hidden_size = 128  # Increase the hidden size\n",
    "num_layers = 2\n",
    "output_size = len(label_encoder.classes_)\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 400\n",
    "\n",
    "# Lists to store training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_losses = []\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_losses.append(loss.item())\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Calculate and store the average training loss for the epoch\n",
    "    avg_train_loss = np.mean(epoch_train_losses)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_val_losses = []\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            epoch_val_losses.append(val_loss.item())\n",
    "\n",
    "        # Calculate and store the average validation loss for the epoch\n",
    "        avg_val_loss = np.mean(epoch_val_losses)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "# Save the trained model and label encoder\n",
    "torch.save(model.state_dict(), './model.pth')\n",
    "torch.save(label_encoder.classes_, '.encoder.pth')\n",
    "\n",
    "\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses Over Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Collect predictions and labels for confusion matrix\n",
    "        all_predictions.extend(predicted.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Confusion Matrix for the test set\n",
    "conf_matrix_test = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Plot confusion matrix for the test set\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_test, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title(\"Confusion Matrix - Test Set\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
